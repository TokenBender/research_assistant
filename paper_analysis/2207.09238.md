## Formal Algorithms for Transformers: A Citation-Centric Analysis

**1. Introduction:**

- **Title:** Formal Algorithms for Transformers
- **Authors:** Mary Phuong and Marcus Hutter
- **Publication Date:** 19 July 2022
- **Objective:** The paper aims to provide a self-contained, mathematically precise, and pseudocode-based overview of transformer architectures and algorithms, including training and inference procedures.
- **Total References:** 22

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:** 
    - Highlights the success of transformers in NLP and other domains.
    - Notes the lack of published pseudocode for transformer variants despite their popularity.
    - Contrasts this with other fields like reinforcement learning where formal algorithms are common. [MKS+13, SBB18, EMK+21]
    - **Relevance:** These citations establish the context of the paper by highlighting a gap in the existing literature on transformers, which the paper aims to address.
- **Claim:** "Descriptions are usually graphical, verbal, partial, or incremental. Despite their popularity, it seems no pseudocode has ever been published for any variant."
    - **Citation:** None explicitly provided for this claim.
    - **Relevance:** This statement motivates the need for the paper by pointing out a deficiency in the current literature.
- **Claim:** "Contrast this to other fields of computer science, even to “cousin” discipline reinforcement learning [MKS+13, SBB18, EMK+21]."
    - **Citation:**
        - [MKS+13] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning, December 2013.
        - [SBB18] Richard S. Sutton, Andrew G. Barto, and Francis Bach. Reinforcement Learning: An Introduction. MIT Press, Cambridge, Massachusetts, second edition edition edition, November 2018.
        - [EMK+21] Yonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Provable RL with Exogenous Distractors via Multistep Inverse Dynamics, March 2021.
    - **Relevance:** These citations exemplify fields where formal algorithms are prevalent, contrasting with the lack thereof in the transformer literature.

**2.2 Motivation:**

- **Key Points:**
    - Argues for the value of formal algorithms in deep learning research.
    - Discusses the lack of scientific precision and detail in many deep learning publications. [RBC+21]
    - Explains the limitations of relying solely on open-source code and advocates for well-crafted pseudocode.
    - Provides examples of good neural network pseudocode and mathematical explanations. [MPCB14, BFT17, JGH18, SGBK+21]
- **Claim:** "Many describe only informally how they change a previous model, Some 100+ page papers contain only a few lines of prose informally describing the model [RBC+21]."
    - **Citation:** [RBC+21] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv:2112.11446, 2021.
    - **Relevance:** This citation supports the authors' claim about the lack of detailed descriptions in some prominent deep learning papers.
- **Claim:** "For a rare text-book example of pseudocode for a non-trivial neural network architecture, see Algorithm S2 of [SGBK+21]."
    - **Citation:** [SGBK+21] Eren Sezener, Agnieszka Grabska-Barwińska, Dimitar Kostadinov, Maxime Beau, Sanjukta Krishnagopal, David Budden, Marcus Hutter, Joel Veness, Matthew Botvinick, Claudia Clopath, Michael Häusser, and Peter E. Latham. A rapid and efficient learning rule for biological neural circuits. Technical report, DeepMind, London, UK, 2021.
    - **Relevance:** This citation provides a positive example of a paper that includes detailed pseudocode for a complex neural network architecture, further emphasizing the need for such practices in the transformer literature.

**2.3 Transformers and Typical Tasks:**

- **Key Points:**
    - Defines transformers as neural network models excelling in natural language processing and sequential data modeling.
    - Describes two common transformer tasks: sequence modeling and sequence-to-sequence prediction.
    - Introduces notation for vocabulary, tokens, and sequences.
    - Explains the concept of chunking for handling long sequences exceeding the transformer's context length.
- **Claim:** "Transformers are neural network models that excel at natural language processing, or more generally at modelling sequential data."
    - **Citation:** [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.
    - **Relevance:** This is the seminal paper introducing the transformer architecture and highlighting its effectiveness in NLP tasks.

**2.4 Tokenization: How Text is Represented:**

- **Key Points:**
    - Explains the concept of tokenization in natural language processing.
    - Describes different tokenization methods: character-level, word-level, and subword tokenization.
    - Introduces Byte Pair Encoding as a successful subword tokenization method. [Gag94, SHB16, RWC+19]
    - Explains the final vocabulary and text representation using token IDs and special tokens.
- **Claim:** "One of the simplest and most successful ones is Byte Pair Encoding [Gag94, SHB16] used in GPT-2 [RWC+19]."
    - **Citation:**
        - [Gag94] Philip Gage. A new algorithm for data compression. Dr. Dobbs / C Users Journal, 12(2):23-38, 1994.
        - [SHB16] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In 54th Annual Meeting of the Association for Computational Linguistics, pages 1715–1725. Association for Computational Linguistics (ACL), 2016.
        - [RWC+19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019.
    - **Relevance:** These citations provide the origin and development of Byte Pair Encoding as a tokenization method and its application in the prominent GPT-2 language model.

**2.5 Architectural Components:**

- **Key Points:**
    - Describes the core building blocks of transformer architectures.
    - Explains token embedding (Algorithm 1).
    - Explains positional embedding, including learned and hard-coded approaches. [Ala18, Ker21, VSP+17]
    - Explains the attention mechanism, its variants (bidirectional, unidirectional, cross-attention, multi-head), and its role in utilizing contextual information. [Ala18, Ala19]
- **Claim:** "Not all transformers make use of learned positional embeddings, some use a hard-coded mapping Wp : N → Rde instead [Ker21]."
    - **Citation:** [Ker21] Jonathan Kernes. Mas-ter Positional Encoding: Part I. https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3, March 2021.
    - **Relevance:** This citation provides an example of a transformer variant that utilizes hard-coded positional embeddings instead of the more common learned approach.
- **Claim:** "The original Transformer [VSP+17] uses [...]" (followed by the formula for sinusoidal positional embeddings).
    - **Citation:** [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.
    - **Relevance:** This citation refers to the original transformer paper, which introduced the specific sinusoidal positional embedding formulation used in many subsequent transformer models.

**2.6 Transformer Architectures:**

- **Key Points:**
    - Presents prominent transformer architectures, including:
        - Encoder-Decoder Transformer (EDT) [VSP+17] (Algorithm 8)
        - Encoder-only transformer (BERT) [DCLT19] (Algorithm 9)
        - Decoder-only transformers (GPT-2, GPT-3, Gopher) [RWC+19, BMR+20, RBC+21] (Algorithm 10)
        - Multi-domain decoder-only transformer (Gato) [RZP+22]
    - Highlights the differences between these architectures, particularly in terms of attention masking and other components.
- **Claim:** "Encoder-decoder / sequence-to-sequence transformer [VSP+17]."
    - **Citation:** [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.
    - **Relevance:** This citation refers to the original transformer paper, which introduced the encoder-decoder architecture for sequence-to-sequence tasks.

**2.7 Transformer Training and Inference:**

- **Key Points:**
    - Provides pseudocode for various training and inference algorithms for different transformer architectures:
        - EDTraining (Algorithm 11)
        - ETraining (Algorithm 12)
        - DTraining (Algorithm 13)
        - DInference (Algorithm 14)
        - EDInference (Algorithm 15)
    - Explains the use of Stochastic Gradient Descent (SGD) for training. [BPRS18, Rud16, KB15]
- **Claim:** "The described training Algorithms 11 to 13 use Stochastic Gradient Descent (SGD) [...] to minimize the log loss (aka cross entropy) as the update rule."
    - **Citation:**
        - [BPRS18] Atilim Gunes Baydin, Barak A. Pearl-mutter, Alexey Andreyevich Radul, and Jef-frey Mark Siskind. Automatic Differentiation in Machine Learning: A Survey. Journal of Ma-chine Learning Research, 18(153):1–43, 2018.
        - [Rud16] Sebastian Ruder. An overview of gradient descent optimization algo-rithms. https://ruder.io/optimizing-gradient-descent/, January 2016.
        - [KB15] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
    - **Relevance:** These citations provide background on the SGD optimization algorithm, its application in deep learning, and the Adam optimizer, a popular variant of SGD.

**2.8 Practical Considerations:**

- **Key Points:**
    - Discusses various "tricks" and techniques used to improve the performance of transformers in practice.
    - These include data preprocessing techniques [FGW+21, Lem21], architectural modifications, training improvements [Sut15], regularization methods [MBM20, TZ22], inference techniques [LAD+22], and other miscellaneous approaches.
- **Claim:** "While the vanilla transformers provided here may work in practice, a variety of “tricks” have been developed over the years to improve the performance of deep neural networks in general and transformers in particular [LWLQ21]."
    - **Citation:** [LWLQ21] Tianyang Lin, Yuxin Wang, Xi-angyang Liu, and Xipeng Qiu. A Survey of Transformers, June 2021.
    - **Relevance:** This citation provides a comprehensive survey of transformer architectures and techniques, including many of the "tricks" mentioned in this section.

**3. Key Insights and Supporting Literature:**

- **Key Insight 1:** Formal algorithms and pseudocode are crucial for a deeper understanding and analysis of transformer architectures.
    - **Supporting Citations:** [MKS+13, SBB18, EMK+21, RBC+21, SGBK+21]
    - **Explanation:** These citations highlight the importance of formal algorithms in other fields and contrast them with the lack of such practices in the transformer literature, motivating the need for the paper's contribution.
- **Key Insight 2:** The attention mechanism is the core component enabling transformers to effectively utilize contextual information in sequential data.
    - **Supporting Citations:** [Ala18, Ala19, VSP+17]
    - **Explanation:** These citations explain the attention mechanism and its role in capturing relationships between tokens in a sequence, which is fundamental to the success of transformers.
- **Key Insight 3:** Different transformer architectures, such as encoder-decoder, encoder-only, and decoder-only, are tailored for specific tasks and applications.
    - **Supporting Citations:** [VSP+17, DCLT19, RWC+19, BMR+20, RBC+21, RZP+22]
    - **Explanation:** These citations introduce and describe various transformer architectures and their suitability for different tasks like sequence-to-sequence prediction, masked language modeling, and next token prediction.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:** The paper does not involve any novel experiments. It focuses on providing a formal description of existing transformer architectures and algorithms.
- **Cited Works for Methodology:** The authors do not cite specific works as a direct basis for their methodology of presenting formal algorithms. However, they draw inspiration from the practice of formal algorithm presentation in other fields like reinforcement learning, as evidenced by citations like [MKS+13, SBB18, EMK+21].
- **Novel Aspects:** The paper's novelty lies in its comprehensive and precise presentation of transformer algorithms in pseudocode, which was previously lacking in the literature. The authors do not cite specific works to justify this novel approach, but they argue for its importance based on the need for clarity and rigor in deep learning research.

**5. Results in Context:**

- **Main Results:** The paper does not present experimental results in the traditional sense. Its main "results" are the formal algorithms and pseudocode descriptions of transformer architectures and training/inference procedures.
- **Comparison with Existing Literature:** The authors do not explicitly compare their formal algorithms with existing implementations or descriptions. However, they implicitly validate their pseudocode by referencing the original papers introducing each transformer architecture (e.g., [VSP+17] for the encoder-decoder transformer).
- **Confirmation, Contradiction, or Extension:** The paper's formal algorithms aim to accurately reflect the architectures and algorithms described in the cited works. They do not contradict or extend existing literature but rather provide a more precise and accessible representation of it.

**6. Discussion and Related Work:**

- **Situating the Work:** The authors situate their work as a necessary step towards improving the rigor and clarity of deep learning research, particularly in the field of transformers. They argue that formal algorithms and pseudocode are essential for a deeper understanding and analysis of these complex models.
- **Key Papers Cited:** The authors cite several key papers introducing different transformer architectures and training methods, including [VSP+17, DCLT19, RWC+19, BMR+20, RBC+21, RZP+22]. They also cite works discussing the importance of formal algorithms in other fields like reinforcement learning [MKS+13, SBB18, EMK+21].
- **Highlighting Novelty and Importance:** The authors highlight the novelty of their work by emphasizing the lack of published pseudocode for transformer variants despite their widespread use. They argue that their formal algorithms provide a valuable resource for researchers and practitioners seeking to understand, implement, and analyze transformers more effectively.

**7. Future Work and Open Questions:**

- **Areas for Further Research:** The authors do not explicitly outline specific areas for future research. However, their work implicitly suggests that future research could benefit from adopting a more formal approach to describing and analyzing deep learning models.
- **Citations Supporting Future Work:** The authors do not cite specific works to support suggestions for future work.

**8. Critical Analysis of Citation Usage:**

- **Effectiveness of Citation Usage:** The authors effectively use citations to support their arguments about the lack of formal algorithms in the transformer literature and the importance of their contribution. They cite relevant papers introducing different transformer architectures, training methods, and related concepts.
- **Areas for Additional Citations:** The authors could have potentially included citations to works that attempt to formally analyze specific aspects of transformers, even if they do not provide complete pseudocode. This would have further strengthened their argument about the need for more formal approaches in the field.
- **Potential Biases:** The authors do not exhibit any noticeable biases in their selection of cited works. They cite papers from various reputable venues and research groups.

**9. Final Summary:**

- **Contribution to the Field:** The paper makes a significant contribution to the field of deep learning by providing a comprehensive and precise set of formal algorithms and pseudocode for various transformer architectures. This resource can be valuable for researchers, practitioners, and educators seeking a deeper understanding of these powerful models.
- **Influential and Frequently Cited Works:** The most influential and frequently cited work throughout the paper is the original transformer paper [VSP+17], which introduced the fundamental architecture and concepts upon which subsequent transformer variants are built.
- **Assessment of Literature Integration:** The paper effectively integrates existing literature to support its claims and findings. The authors clearly demonstrate the need for their work by highlighting the gap in the current literature and referencing relevant papers introducing different transformer architectures and related concepts. The citations are used appropriately and contribute to a strong factual basis for the paper's arguments.


This citation-centric analysis provides a detailed overview of the paper's factual basis, its relationship to existing literature, and its contribution to the field of deep learning. By tracing the origins of key ideas and examining the authors' use of citations, readers can gain a deeper understanding of the paper's significance and its place within the broader research context.