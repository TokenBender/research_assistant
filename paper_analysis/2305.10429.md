## DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining - A Citation-Centric Analysis

This analysis provides a comprehensive overview of the paper "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining" by Xie et al., published in November 2023, with a strong emphasis on the citations used to support its claims and findings.

**1. Introduction:**

- **Title:** DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining
- **Authors:** Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V. Le, Tengyu Ma, and Adams Wei Yu
- **Publication Date:** November 21, 2023
- **Objective:** The research aims to develop a method for automatically determining the optimal mixture proportions of different data domains for language model pretraining, without relying on downstream task performance.
- **Total References:** 58

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:** Language models (LMs) are typically trained on data from multiple domains. The composition of this data significantly impacts LM performance. Existing methods for determining domain weights rely on intuition or downstream task performance, which can be suboptimal or computationally expensive.
- **Significant Citations:**
    - **Claim:** Datasets for training LMs are typically sampled from a mixture of many domains.
    - **Citation:** Brown et al., 2020, Chowdhery et al., 2022, Du et al., 2021, Gao et al., 2020
    - **Relevance:** These citations provide examples of large-scale language model training datasets (The Pile, PaLM, GLaM) that are composed of data from multiple domains.
    - **Claim:** The composition of the pretraining data greatly affects the effectiveness of an LM.
    - **Citation:** Du et al., 2021, Hoffmann et al., 2022, Xie et al., 2023
    - **Relevance:** These works demonstrate the impact of data composition on language model performance, highlighting the importance of carefully selecting domain weights.

**2.2 Domain Reweighting with Minimax Optimization (DoReMi):**

- **Key Points:** DoReMi uses a small proxy model trained with Group Distributionally Robust Optimization (Group DRO) to determine optimal domain weights without knowledge of downstream tasks. It first trains a small reference model, then a small proxy model using Group DRO to minimize the worst-case excess loss (relative to the reference model) across domains. The resulting domain weights are used to train a larger LM.
- **Significant Citations:**
    - **Claim:** DoReMi minimizes the worst-case excess loss over domains.
    - **Citation:** Mindermann et al. (2022), Oren et al. (2019)
    - **Relevance:** These works introduce the concept of excess loss as a measure of model improvement potential and advocate for minimizing the worst-case excess loss for robustness.
    - **Claim:** DoReMi leverages distributionally robust optimization (DRO) to tune domain weights.
    - **Citation:** Oren et al. (2019)
    - **Relevance:** This work introduces DRO-LM, a framework for training robust language models using DRO, which DoReMi adapts for optimizing domain weights.
    - **Claim:** DoReMi uses the online learning-based optimizer from Group DRO.
    - **Citation:** Nemirovski et al. (2009), Sagawa et al. (2020)
    - **Relevance:** These works introduce Group DRO, a framework for robust optimization in the presence of group shifts, and its associated online optimizer, which DoReMi uses to dynamically update domain weights.

**3. Key Insights and Supporting Literature:**

- **Key Insight 1:** DoReMi improves perplexity across all domains on The Pile dataset, even when downweighting certain domains.
    - **Supporting Citations:** Gao et al. (2020) (for introducing The Pile dataset)
- **Key Insight 2:** DoReMi improves average few-shot downstream accuracy by 6.5% points and achieves baseline accuracy 2.6x faster on The Pile.
    - **Supporting Citations:** Brown et al. (2020) (for introducing the GPT-3 few-shot tasks used for evaluation)
- **Key Insight 3:** On the GLaM dataset, DoReMi achieves comparable performance to using domain weights tuned on downstream tasks.
    - **Supporting Citations:** Du et al. (2021) (for introducing the GLaM dataset and its downstream-tuned domain weights)

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:** DoReMi is evaluated on The Pile and GLaM datasets. Proxy and reference models are trained with 280M parameters, while the main model is trained with 8B parameters. Performance is measured using perplexity and few-shot downstream accuracy on GPT-3 tasks.
- **Citations for Methodology:**
    - **Transformer architecture:** Vaswani et al. (2017)
    - **Adam optimizer:** Kingma and Ba (2015)
    - **Adafactor optimizer:** Shazeer and Stern (2018)
    - **Group DRO optimizer:** Sagawa et al. (2020)
- **Novel Aspects:** DoReMi's novel aspect lies in adapting the DRO-LM framework to optimize domain weights instead of producing a robust model. The authors do not explicitly cite any works to justify this specific adaptation, but they build upon the foundations of DRO-LM and Group DRO.

**5. Results in Context:**

- **Main Results:** DoReMi significantly reduces perplexity across all domains on The Pile, improves downstream accuracy, and speeds up training. On GLaM, it achieves comparable performance to downstream-tuned domain weights.
- **Citations for Comparison:**
    - **Comparison with baseline domain weights on The Pile:** Gao et al. (2020)
    - **Comparison with downstream-tuned domain weights on GLaM:** Du et al. (2021)
- **Confirmation/Contradiction/Extension:** DoReMi's results confirm the importance of carefully selecting domain weights for LM pretraining. It extends existing work by providing a method for automatically determining these weights without relying on downstream tasks.

**6. Discussion and Related Work:**

- **Situating the Work:** The authors position DoReMi as a data-centric approach for improving LM training efficiency. They compare it with existing methods for curating pretraining data, general data selection methods, and distributionally robust optimization techniques.
- **Key Citations in Discussion/Related Work:**
    - **Curating pretraining data:** Du et al. (2021), Chowdhery et al. (2022), Raffel et al. (2019), Xie et al. (2023)
    - **General data selection methods:** Moore and Lewis (2010), Coleman et al. (2020), Mindermann et al. (2022)
    - **Distributionally robust optimization:** Ben-Tal et al. (2013), Oren et al. (2019), Sagawa et al. (2020), Sinha et al. (2018)
- **Highlighting Novelty/Importance:** The authors emphasize DoReMi's novelty by highlighting its ability to automatically determine domain weights without relying on downstream tasks, unlike existing methods that either use heuristics or require expensive tuning on downstream data.

**7. Future Work and Open Questions:**

- **Areas for Further Research:** The authors suggest exploring extrapolation techniques for saving compute in DoReMi, investigating the impact of the reference model choice, and extending DoReMi to fine-grained domains and example-level reweighting.
- **Citations for Future Work:** Oren et al. (2019) (for suggesting the use of clustering to find fine-grained domains)

**8. Critical Analysis of Citation Usage:**

- **Effectiveness:** The authors effectively use citations to support their claims and situate their work within the existing literature. They provide a clear and comprehensive overview of related work, highlighting the limitations of existing methods and the novelty of their approach.
- **Areas for Additional Citations:** While the citation usage is generally strong, additional citations could have been beneficial in the section on experimental methodology to further justify the specific hyperparameter choices and training setup.
- **Potential Biases:** There is no apparent bias in the selection of cited works. The authors cite a diverse range of papers from different authors and publications.

**9. Final Summary:**

- **Contribution:** DoReMi offers a novel and efficient method for automatically determining optimal domain weights for LM pretraining, leading to improved perplexity, downstream accuracy, and training speed.
- **Influential/Frequently Cited Works:** Oren et al. (2019), Sagawa et al. (2020), Gao et al. (2020), Du et al. (2021), Brown et al. (2020)
- **Integration of Existing Literature:** The paper effectively integrates existing literature to support its claims and findings, demonstrating a strong understanding of the relevant research landscape. It builds upon the foundations of DRO and data selection methods, adapting and extending them to address the specific challenge of optimizing data mixtures for LM pretraining.

**Overall, the paper "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining" presents a valuable contribution to the field of language model training. Its rigorous methodology, comprehensive analysis, and effective use of citations make it a strong and impactful piece of research.** 
