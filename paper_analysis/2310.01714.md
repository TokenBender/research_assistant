## Analysis of "Large Language Models as Analogical Reasoners" (ICLR 2024)

**1. Introduction:**

- **Title:** Large Language Models as Analogical Reasoners
- **Authors:** Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, Denny Zhou
- **Publication Date:** March 9, 2024 (arXiv preprint)
- **Objective:** This paper introduces analogical prompting, a novel prompting approach for large language models (LLMs) that aims to automatically guide their reasoning process by prompting them to self-generate relevant exemplars or knowledge before solving a given problem.
- **Total References:** 71

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Summary:** The introduction highlights the strengths of LLMs and the effectiveness of chain-of-thought (CoT) prompting for complex tasks. It also identifies the limitations of existing CoT methods (0-shot and few-shot) in terms of providing relevant guidance and minimizing manual labeling. The authors propose analogical prompting, inspired by human analogical reasoning (**Vosniadou & Ortony, 1989**), as a solution to these limitations.
- **Significant Citations:**
    - **Claim:** Chain-of-thought (CoT) prompting has demonstrated LLMs' abilities to tackle complex tasks.
    - **Citation:** Wei et al., 2022b; Kojima et al., 2022
    - **Relevance:** These citations establish the effectiveness of CoT prompting as a method for guiding LLMs in reasoning tasks, setting the stage for the authors' proposed improvement.
    - **Claim:** In analogical reasoning, humans draw from relevant past experiences to tackle new problems.
    - **Citation:** Vosniadou & Ortony, 1989
    - **Relevance:** This citation introduces the psychological concept of analogical reasoning, which serves as the foundational inspiration for the authors' proposed prompting approach.

**2.2 Related Works:**

- **Summary:** This section discusses related work in two areas: large language models and prompting, and chain-of-thought prompting. It highlights the advancements in LLMs and the emergence of prompting techniques, including in-context and few-shot learning. It also reviews existing CoT methods and their limitations, emphasizing the need for automated and tailored exemplar generation.
- **Significant Citations:**
    - **Claim:** LLMs with billions of parameters demonstrate in-context learning and few-shot learning abilities.
    - **Citation:** Brown et al., 2020; Liu et al., 2022; Su et al., 2022; Mishra et al., 2022; Wei et al., 2022a; Yasunaga et al., 2023; Shi et al., 2023
    - **Relevance:** These citations establish the capabilities of LLMs in leveraging provided context or exemplars for learning, which is crucial for the effectiveness of analogical prompting.
    - **Claim:** Few-shot CoT achieves stronger performance by providing multiple exemplars of reasoning process (question-rationale-answer).
    - **Citation:** Wei et al., 2022b
    - **Relevance:** This citation introduces few-shot CoT, a key existing method that analogical prompting aims to improve upon by automating the generation of exemplars.

**2.3 Preliminaries:**

- **Summary:** This section defines the problem-solving tasks addressed in the paper and introduces the concept of a prompting method as a function that transforms a problem statement into a textual input for an LLM.
- **Significant Citations:** None (This section primarily focuses on establishing definitions and notations).

**2.4 Approach:**

- **Summary:** This section details the analogical prompting approach, including two techniques: self-generated exemplars and self-generated knowledge + exemplars. It explains how the approach prompts LLMs to recall or generate relevant problems and solutions in context, enabling in-context learning for new problems. It also discusses key technical decisions, such as generating diverse exemplars and using a single-pass prompt.
- **Significant Citations:** None (This section primarily focuses on describing the authors' proposed approach).

**2.5 Experimental Setup:**

- **Summary:** This section describes the experimental setup, including the tasks used for evaluation (mathematical problem solving, code generation, and other reasoning tasks), the base LLMs employed (GPT-3.5, GPT-4, PaLM2), and the methods compared (0-shot, 0-shot CoT, few-shot CoT, few-shot retrieved CoT, and the authors' analogical prompting).
- **Significant Citations:**
    - **Claim:** Competitive programming is especially challenging, requiring reasoning about various algorithms like dynamic programming and graphs.
    - **Citation:** Li et al., 2022b; Kulal et al., 2019; Yasunaga & Liang, 2020
    - **Relevance:** These citations highlight the complexity of the Codeforces code generation task, justifying its inclusion in the evaluation to demonstrate the effectiveness of analogical prompting on challenging problems.

**2.6 Results:**

- **Summary:** This section presents the main results of the evaluation, demonstrating that analogical prompting outperforms 0-shot CoT and few-shot CoT across various tasks and base LLMs. It also highlights the benefits of self-generating knowledge in addition to exemplars for complex tasks like code generation.
- **Significant Citations:** None (This section primarily focuses on presenting the experimental results).

**2.7 Discussion:**

- **Summary:** This section discusses the key findings of the evaluation, including the effectiveness of knowledge generation, the comparison between generating and retrieving exemplars, the impact of LLM scale, and the optimal number of exemplars to generate. It also provides a qualitative analysis of the performance of analogical prompting.
- **Significant Citations:**
    - **Claim:** Few-shot in-context learning in LLMs typically achieves the best results with a small number of exemplars (K=3-5).
    - **Citation:** Brown et al., 2020
    - **Relevance:** This citation supports the authors' finding that generating K=3-5 exemplars works best for analogical prompting, aligning with established practices in few-shot learning.

**2.8 Limitations and Future Research:**

- **Summary:** This section acknowledges the limitations of analogical prompting, such as increased inference computation and potential failure of self-generation with weaker LLMs. It also suggests future research directions, including exploring methods to generate exemplars that facilitate generalization and addressing prompt sensitivity.
- **Significant Citations:**
    - **Claim:** LLM performance can be influenced by specific prompt phrases used to query the model.
    - **Citation:** Jiang et al., 2020
    - **Relevance:** This citation highlights the issue of prompt sensitivity, which is a known limitation of LLM prompting methods, including analogical prompting.

**3. Key Insights and Supporting Literature:**

- **Key Insight 1:** Analogical prompting, inspired by human analogical reasoning (**Vosniadou & Ortony, 1989**), can effectively guide the reasoning process of LLMs by prompting them to self-generate relevant exemplars or knowledge.
- **Key Insight 2:** Self-generating knowledge, in addition to exemplars, can further improve performance on complex tasks like code generation, as it provides high-level takeaways that complement low-level exemplars and aid in generalization.
- **Key Insight 3:** Analogical prompting excels with larger-scale LLMs, as they have learned more relevant tasks during training and can generate more useful exemplars.
- **Key Insight 4:** Generating K=3-5 exemplars generally yields the best results for analogical prompting, aligning with findings in standard few-shot in-context learning (**Brown et al., 2020**).

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:** The authors evaluate analogical prompting on a variety of reasoning tasks, including mathematical problem solving (GSM8K, MATH), code generation (Codeforces), and other reasoning tasks from BIG-Bench. They compare their approach with 0-shot, 0-shot CoT, few-shot CoT, and few-shot retrieved CoT using various base LLMs (GPT-3.5, GPT-4, PaLM2).
- **Cited Works as Basis for Methodology:** The authors draw inspiration from human analogical reasoning (**Vosniadou & Ortony, 1989**) and build upon existing CoT prompting techniques (**Wei et al., 2022b; Kojima et al., 2022**). They also follow established practices in few-shot learning (**Brown et al., 2020**) when determining the optimal number of exemplars to generate.
- **Novel Aspects of Methodology:** The novel aspect of the methodology is the self-generation of exemplars and knowledge within the prompt, eliminating the need for manual labeling or external retrieval. The authors do not explicitly cite any works to justify this novel approach, but they implicitly rely on the in-context learning abilities of LLMs, which are well-established in the literature (e.g., **Brown et al., 2020**).

**5. Results in Context:**

- **Main Results:** Analogical prompting outperforms 0-shot CoT and few-shot CoT across various tasks and base LLMs, achieving an average accuracy gain of +4%. Self-generating knowledge further improves performance on complex tasks like code generation.
- **Comparison with Existing Literature:** The authors compare their results with existing CoT methods (**Wei et al., 2022b; Kojima et al., 2022**) and few-shot retrieved CoT (**Zhang et al., 2022b; Shum et al., 2023**). Their findings demonstrate that analogical prompting can achieve superior performance, particularly with larger-scale LLMs.
- **Confirmation, Contradiction, or Extension of Cited Works:** The authors' results confirm the effectiveness of CoT prompting for guiding LLM reasoning (**Wei et al., 2022b; Kojima et al., 2022**) but extend this paradigm by automating the generation of exemplars. They also demonstrate that self-generation can be more effective than retrieval for larger-scale LLMs, which contrasts with findings that retrieval-based CoT excels with smaller base LLMs (**Zhang et al., 2022b; Shum et al., 2023**).

**6. Discussion and Related Work:**

- **Situating the Work within Existing Literature:** The authors position analogical prompting as an improvement upon existing CoT methods (**Wei et al., 2022b; Kojima et al., 2022**) by addressing the limitations of manual labeling and generic guidance. They also compare their approach with retrieval-based CoT (**Zhang et al., 2022b; Shum et al., 2023**) and highlight the advantages of self-generation for larger-scale LLMs.
- **Key Papers Cited:** The discussion and related work sections cite various papers related to LLMs, prompting, CoT, and analogical reasoning. Key citations include:
    - **Brown et al., 2020:** Establishes the in-context learning abilities of LLMs, which are crucial for analogical prompting.
    - **Wei et al., 2022b; Kojima et al., 2022:** Introduce 0-shot and few-shot CoT, the main existing methods that analogical prompting aims to improve upon.
    - **Zhang et al., 2022b; Shum et al., 2023:** Explore retrieval-based CoT, providing a comparison point for the authors' self-generation approach.
    - **Vosniadou & Ortony, 1989:** Introduces the concept of human analogical reasoning, which serves as the inspiration for analogical prompting.
- **Highlighting Novelty and Importance:** The authors emphasize the novelty of analogical prompting by highlighting its automated and tailored exemplar generation, which contrasts with the manual labeling required by few-shot CoT and the generic guidance offered by 0-shot CoT. They also demonstrate its superior performance, particularly with larger-scale LLMs, emphasizing its importance as a promising approach for guiding LLM reasoning.

**7. Future Work and Open Questions:**

- **Areas for Further Research:** The authors suggest several areas for future research, including exploring methods to generate exemplars that facilitate generalization, addressing prompt sensitivity (**Jiang et al., 2020**), and investigating the applicability of analogical prompting to other tasks beyond those evaluated in the paper.
- **Citations Supporting Future Work:** The authors cite **Jiang et al., 2020** to highlight the issue of prompt sensitivity, which is a known limitation of LLM prompting methods and an area that warrants further investigation in the context of analogical prompting.

**8. Critical Analysis of Citation Usage:**

- **Effectiveness of Citation Usage:** The authors effectively use citations to support their arguments and situate their work within the existing literature. They provide a comprehensive overview of related work in LLMs, prompting, and CoT, and they clearly articulate how analogical prompting builds upon and extends these existing methods.
- **Areas for Additional Citations:** While the authors provide a thorough review of related work, additional citations could have been beneficial in the following areas:
    - Further justification for the novel approach of self-generating exemplars within the prompt, potentially drawing upon works that explore the knowledge and reasoning capabilities of LLMs acquired during pre-training.
    - Deeper exploration of the connection between human analogical reasoning and the proposed prompting approach, potentially citing works that investigate the cognitive mechanisms underlying analogy and how they might be reflected in LLM behavior.
- **Potential Biases in Citation Selection:** The authors primarily cite works published in top-tier machine learning conferences and journals, which is common in academic research. However, this could potentially lead to a bias towards certain authors or research groups. Exploring and citing relevant work from a wider range of venues could provide a more comprehensive and balanced perspective.

**9. Final Summary:**

- **Contribution to the Field:** This paper introduces analogical prompting, a novel and promising approach for guiding LLM reasoning. It demonstrates that self-generating relevant exemplars or knowledge within the prompt can effectively improve performance on various reasoning tasks, particularly with larger-scale LLMs.
- **Influential/Frequently Cited Works:** The most influential or frequently cited works used throughout the paper include:
    - **Brown et al., 2020:** Establishes the in-context learning abilities of LLMs.
    - **Wei et al., 2022b; Kojima et al., 2022:** Introduce 0-shot and few-shot CoT, the main existing methods that analogical prompting aims to improve upon.
    - **Vosniadou & Ortony, 1989:** Introduces the concept of human analogical reasoning, which serves as the inspiration for analogical prompting.
- **Assessment of Literature Integration:** The paper effectively integrates existing literature to support its claims and findings. It provides a thorough review of related work, clearly articulates the limitations of existing methods, and demonstrates how analogical prompting addresses these limitations. The authors' use of citations strengthens their arguments and establishes the novelty and importance of their contribution to the field of LLM prompting and reasoning.