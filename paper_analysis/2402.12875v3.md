## Chain of Thought Empowers Transformers to Solve Inherently Serial Problems: A Citation-Centric Analysis

This analysis dissects the paper "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems" by Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma, published on arXiv on May 23, 2024 (v3). The paper investigates the theoretical and empirical benefits of Chain of Thought (CoT) prompting in enhancing the expressiveness of decoder-only transformers, particularly for tasks requiring serial computation. The paper cites **41 references** to support its claims and findings.

**1. Introduction:**

- **Objective:** The research aims to provide a theoretical understanding of how CoT prompting improves the reasoning capabilities of Large Language Models (LLMs), specifically decoder-only transformers. The authors hypothesize that CoT enables transformers to perform serial computations, which are otherwise limited by their parallel nature, especially at low depths.
- **References:** The introduction cites several key works demonstrating the effectiveness of CoT in LLMs:
    - **(Chowdhery et al., 2023; Anil et al., 2023; Achiam et al., 2023; Romera-Paredes et al., 2023; Trinh et al., 2024):** These citations showcase the superior performance of LLMs in complex reasoning tasks like problem-solving and code generation, highlighting the potential of CoT in unlocking these capabilities.
    - **(Ling et al., 2017; Cobbe et al., 2021; Nye et al., 2021; Chung et al., 2022; Reynolds & McDonell, 2021; Nye et al., 2021; Wei et al., 2022):** These works illustrate various methods for incorporating CoT into LLMs, including training, instruction tuning, and few-shot prompting.
    - **(Kojima et al., 2022; Wang et al., 2022a; Madaan & Yazdanbakhsh, 2022):** These citations present intriguing evidence that the form of CoT prompting, even with incorrect reasoning steps, can be as crucial as its content, suggesting a fundamental shift in computational capabilities.

**2. Section-by-Section Analysis with Citation Extraction:**

- **Section 2 (Notations and Preliminaries):** This section establishes the notation and defines decoder-only transformers, drawing upon the GPT architecture (**Radford et al., 2019**) as a reference. It also introduces the concept of circuit complexity, laying the groundwork for the theoretical analysis.
- **Section 3 (Expressiveness Theory for Transformers with CoT):**
    - **3.1 (Finite Precision Modeling):** This section addresses the practical aspect of finite precision in transformer computations, citing **(IEEE, 2008)** for the IEEE 754 standard and contrasting their approach with prior works assuming infinite precision (**Pérez et al., 2019; Dehghani et al., 2018**).
    - **3.2 (CoT: Complexity Class for Constant-depth Transformers with CoT):** The authors define a new complexity class, CoT, to formally characterize problems solvable by transformers with CoT. They differentiate their non-uniform complexity class from previous works focusing on uniform transformer classes (**Pérez et al., 2019, 2021; Yao et al., 2021; Weiss et al., 2021; Chiang et al., 2023; Hao et al., 2022; Merrill & Sabharwal, 2023a; Merrill et al., 2022**).
    - **3.3 (Tighter Upper Bounds on Transformer Expressiveness):** This section presents tighter upper bounds on the expressiveness of constant-precision transformers.
        - **Claim:** Constant-depth transformers with constant-bit precision can only solve problems in ACº, a proper subset of TCº.
        - **Citation:** **(Merrill & Sabharwal, 2023b; Liu et al., 2022a):** These works establish the TCº upper bound for log-precision transformers, which the authors refine for constant-precision.
        - **Claim:** Constant-depth, log-precision transformers with fixed-point numbers can be simulated by TCº circuits even with correct rounding.
        - **Citation:** **(Goldberg, 1991):** This work highlights the non-associativity of iterated addition with rounding, posing a challenge to the TCº bound, which the authors address for fixed-point numbers.
        - **Citation:** **(Liu et al., 2022a):** This work provides inspiration for the proof technique used in establishing the TCº bound for fixed-point numbers.
    - **3.4 (CoT Makes Transformers More Expressive):** This section demonstrates the increased expressiveness of transformers with CoT.
        - **Claim:** Constant-depth, constant-precision transformers with polynomial CoT steps and logarithmic embedding size can solve any problem in P/poly.
        - **Citation:** **(Pippenger & Fischer, 1979):** This work establishes the simulation of Turing Machines by polynomial-size circuits, connecting P/poly to the capabilities of transformers with CoT.
        - **Claim:** Transformers with linear CoT steps can compute all regular languages, including composition of non-solvable groups like S5.
        - **Citation:** **(Wei et al., 2022):** This work provides empirical support for the theoretical claim by showing that forcing intermediate outputs improves performance, aligning with the idea of CoT enabling serial computation.
        - **Claim:** The wording problem of S5 is in CoT[n, log n, 1] but not T[poly(n), log n].
        - **Citation:** **(Barrington, 1986):** This work establishes the NC¹-completeness of the S5 wording problem, providing a concrete example of a problem solvable by transformers with CoT but not without.
        - **Citation:** **(Yao, 1989):** This work provides the standard hardness assumption TCº ⊆ NC¹, which is used to prove the separation between transformers with and without CoT for the S5 wording problem.
        - **Claim:** Log-precision (resp. constant-precision) constant-depth poly-embedding-size transformers with T(n) CoT steps can simulate T(n)-size circuits with TCº (resp. ACº) oracle gates.
        - **Citation:** **(Wilson, 1985):** This work defines complexity classes solvable by circuits with oracles, providing the framework for analyzing transformers with poly(n) embedding size.

**3. Key Insights and Supporting Literature:**

- **CoT enables serial computation in transformers:** This is supported by the theoretical results showing that transformers with CoT can solve problems in P/poly (**Pippenger & Fischer, 1979**) and regular languages including S5 (**Barrington, 1986; Yao, 1989**), which are beyond the capabilities of transformers without CoT.
- **CoT with logarithmic embedding size is sufficient for polynomial-time computation:** This is demonstrated by Theorem 3.3, which shows that CoT[poly(n), log n, 1] = P/poly.
- **Increasing embedding size to poly(n) does not significantly improve expressiveness for polynomial CoT steps:** This is shown by Theorems 3.7 and 3.8, which establish the equivalence of SIZETC° [poly(n)] and SIZEAC° [poly(n)] with CoT[poly(n), poly(n), log n] and CoT[poly(n), poly(n), 1] respectively.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:** The authors train decoder-only transformers on four arithmetic tasks: modular addition, permutation composition (S5), iterated squaring, and circuit value problem. They compare performance in three settings: base (no CoT), CoT, and hint (providing intermediate steps as labels).
- **Citation:** **(Kingma & Ba, 2014):** The authors use the Adam optimizer for training the transformers.
- **Novel Aspects:** The authors introduce a novel "hint" setting to control for statistical advantages of providing intermediate steps.
- **Justification:** The authors do not explicitly cite any works to justify the "hint" setting, but it is motivated by the need to isolate the expressiveness gains of CoT from potential statistical benefits.

**5. Results in Context:**

- **Main Results:** As predicted by the theory, CoT significantly improves accuracy on tasks requiring serial computation (S5, iterated squaring, circuit value problem), especially for low-depth transformers.
- **Comparison with Existing Literature:**
    - **(Liu et al., 2022a):** The results on modular addition confirm the findings of this work, showing that constant-depth transformers can solve this parallelizable task.
    - **(Barrington, 1986; Yao, 1989):** The results on S5 composition empirically validate the theoretical separation between transformers with and without CoT for this NC¹-complete problem.
- **Extension of Cited Works:** The results on iterated squaring and circuit value problem extend the findings of cited works on the hardness of these problems, demonstrating the ability of CoT to overcome these limitations in practice.

**6. Discussion and Related Work:**

- **Situating the Work:** The authors discuss their findings in the context of existing literature on transformer expressiveness, highlighting the novelty of their theoretical framework and empirical results.
- **Key Citations:**
    - **(Merrill & Sabharwal, 2023b; Liu et al., 2022a):** The authors compare their tighter upper bounds for constant-precision transformers with the TCº bounds established in these works.
    - **(Edelman et al., 2022; Hahn, 2020; Merrill et al., 2021; Weiss et al., 2021; Dehghani et al., 2018; Giannou et al., 2023; Pérez et al., 2021; Clark et al., 2019; Tenney et al., 2019; Vig, 2019; Wang et al., 2022b):** The authors discuss their work in relation to these studies on the capabilities of self-attention, including its ability to create low-complexity circuits, form declarative programs, and simulate Turing machines.
- **Highlighting Novelty and Importance:** The authors emphasize the novelty of their CoT complexity class and its ability to capture the serial computation capabilities unlocked by CoT, distinguishing their work from previous studies focusing on parallel computation in transformers.

**7. Future Work and Open Questions:**

- **Areas for Further Research:** The authors suggest investigating the expressiveness of transformers with logarithmic precision and log bits for exponents, as well as exploring the impact of varying CoT length and embedding size.
- **Supporting Citations:** No specific citations are used to support these suggestions, but they arise naturally from the theoretical and empirical findings of the paper.

**8. Critical Analysis of Citation Usage:**

- **Effectiveness:** The authors effectively use citations to support their arguments, providing a strong foundation for their claims and situating their work within the broader research context.
- **Potential Biases:** The selection of cited works appears balanced, with no over-reliance on specific authors or publications.
- **Areas for Improvement:** While the authors thoroughly discuss related work on transformer expressiveness, they could have included more citations on the empirical effectiveness of CoT in various NLP tasks to further strengthen their argument for its practical significance.

**9. Final Summary:**

- **Contribution:** The paper makes a significant contribution to the field by providing a theoretical framework for understanding the power of CoT in enhancing transformer expressiveness, particularly for tasks requiring serial computation.
- **Influential Works:** Key citations include **(Pippenger & Fischer, 1979)** for connecting P/poly to transformers with CoT, **(Barrington, 1986)** for the NC¹-completeness of the S5 wording problem, and **(Merrill & Sabharwal, 2023b; Liu et al., 2022a)** for establishing previous bounds on transformer expressiveness.
- **Integration of Existing Literature:** The paper effectively integrates existing literature to support its claims and findings, demonstrating a strong understanding of the field and positioning its contribution within the broader research landscape.

**Overall, the paper presents a compelling case for the power of CoT in enhancing the expressiveness of decoder-only transformers, supported by a thorough theoretical analysis and convincing empirical evidence. The authors' meticulous use of citations strengthens their arguments and provides a valuable roadmap for future research in this rapidly evolving field.** 
