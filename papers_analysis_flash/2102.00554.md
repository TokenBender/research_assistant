## Sparsity in Deep Learning: Pruning and Growth for Efficient Inference and Training in Neural Networks

**1. Introduction**

- **Title:** Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks
- **Authors:** Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, Alexandra Peste
- **Publication Date:** 31 January 2021
- **Objective:** This paper provides a comprehensive survey of sparsity techniques in deep learning, covering both model sparsification (pruning and growth) and ephemeral sparsification (e.g., dropout). The authors aim to distill key ideas from over 300 research papers and provide guidance for practitioners and researchers working in the field.
- **Number of References:** 200+

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Point:** Deep learning models are often overparameterized, leading to high computational and memory costs, especially for mobile and resource-constrained devices.
- **Citation:** [Friston 2008]
- **Relevance:** This citation highlights the biological inspiration for sparsity in deep learning, drawing a parallel between the structure of biological brains and artificial neural networks.

- **Key Point:** Sparsity can improve generalization by reducing noise in the model and can also lead to computational savings.
- **Citation:** [Grünwald 2007]
- **Relevance:** This citation introduces the Minimum Description Length (MDL) framework, which provides a theoretical justification for sparsity as a form of regularization.

- **Key Point:** Modern deep learning models are computationally expensive to train and use, with some models requiring billions of parameters.
- **Citation:** [Szegedy et al. 2016], [Brown et al. 2020]
- **Relevance:** These citations provide examples of large, computationally expensive deep learning models, highlighting the need for efficient compression techniques.

**2.2 Overview of Model Compression Techniques**

- **Key Point:** The paper categorizes model compression techniques into six main categories: down-sizing models, operator factorization, value quantization, value compression, parameter sharing, and sparsification.
- **Citation:** [Hinton et al. 2015], [Elsken et al. 2019], [Sainath et al. 2013], [Kanjilal et al. 1993], [Zhao et al. 2017], [Han et al. 2016b], [Jin et al. 2019], [Plummer et al. 2020]
- **Relevance:** These citations provide examples of different model compression techniques and their applications in deep learning.

**2.3 Sparsification**

- **Key Point:** Sparsification involves removing or adding elements of a neural network to reduce its complexity.
- **Citation:** [Reed 1993]
- **Relevance:** This citation provides a historical overview of early sparsification techniques.

- **Key Point:** The authors distinguish between model sparsity (permanent changes to the network structure) and ephemeral sparsity (dynamic changes applied per example).
- **Citation:** [Sharma et al. 2017]
- **Relevance:** This citation highlights the distinction between model sparsity and ephemeral sparsity, which is crucial for understanding the different approaches to sparsification.

**2.4 When to Sparsify?**

- **Key Point:** The authors discuss three main sparsification schedules: train-then-sparsify, sparsify-during-training, and sparse training.
- **Citation:** [Janowsky 1989], [Engelbrecht and Cloete 1996], [Finnoff et al. 1993], [Ghosh and Tumer 1994], [Wortsman et al. 2019], [Lin et al. 2020], [Narasimha et al. 2008], [You et al. 2020], [Golub et al. 2019]
- **Relevance:** These citations provide examples of different sparsification schedules and their advantages and disadvantages.

- **Key Point:** The authors highlight the phenomenon of early structure adaptation, where the most important connections in a network are often determined early in the training process.
- **Citation:** [Shwartz-Ziv and Tishby 2017], [Achille et al. 2019], [Michel et al. 2019], [Ding et al. 2019b], [You et al. 2020], [Golub et al. 2019], [Li et al. 2020b]
- **Relevance:** These citations provide evidence for early structure adaptation and discuss its implications for sparsification.

**2.5 Ensembles**

- **Key Point:** Sparsification can be used to create ensembles of smaller models, which can improve performance over a single model with the same parameter budget.
- **Citation:** [Collins and Kohli 2014]
- **Relevance:** This citation provides an example of how ensembles of sparse models can be used to improve performance.

**3. Selecting Candidates for Removal**

- **Key Point:** The authors categorize element removal methods into three categories: data-free, data-driven, and training-aware.
- **Citation:** [Suzuki et al. 2001], [Changpinyo et al. 2017], [Mittal et al. 2018]
- **Relevance:** These citations provide examples of different element removal methods and their applications in deep learning.

**3.1 Structured vs. Unstructured Element Removal**

- **Key Point:** Structured sparsity constrains sparsity patterns to reduce indexing overheads and simplify processing.
- **Citation:** [Polyak and Wolf 2015], [Anwar et al. 2017], [Chellapilla et al. 2006], [Lebedev and Lempitsky 2015], [Meng et al. 2020], [Changpinyo et al. 2017]
- **Relevance:** These citations provide examples of different structured sparsity patterns and their advantages and disadvantages.

**3.2 Data-Free Selection Based on Magnitude**

- **Key Point:** Magnitude pruning involves removing weights with the smallest absolute magnitude.
- **Citation:** [Hagiwara 1993], [Gale et al. 2019], [Thimm and Fiesler 1995], [Han et al. 2016b], [Zhu and Gupta 2017]
- **Relevance:** These citations provide examples of magnitude pruning and its applications in deep learning.

- **Key Point:** The authors discuss methods for learning sparsification thresholds during training.
- **Citation:** [Kusupati et al. 2020], [Lillicrap et al. 2019], [Han et al. 2016b], [Li et al. 2017], [Narang et al. 2017], [Ström 1997]
- **Relevance:** These citations provide examples of different methods for learning sparsification thresholds and their advantages and disadvantages.

**3.3 Data-Driven Selection Based on Input or Output Sensitivity**

- **Key Point:** Data-driven methods consider the statistical sensitivity of the output of neurons or the whole network with respect to the training data.
- **Citation:** [Sietsma and Dow 1988], [Castellano et al. 1997], [Luo et al. 2017], [Yu et al. 2018], [Ding et al. 2019a], [Zeng and Yeung 2006], [Engelbrecht and Cloete 1996], [Tartaglione et al. 2018], [Thimm and Fiesler 1995], [Hagiwara 1993], [Hu et al. 2016], [Lauret et al. 2006], [Han and Qiao 2013], [Sietsma and Dow 1991], [Kameyama and Kosugi 1991], [Suau et al. 2019], [Sun et al. 2015], [Engelbrecht et al. 1995]
- **Relevance:** These citations provide examples of different data-driven methods for selecting elements to remove and their advantages and disadvantages.

**3.4 Selection Based on 1st Order Taylor Expansion of the Training Loss Function**

- **Key Point:** Gradient-based methods use the gradients of the weights, which are computed during training, to determine weight importance.
- **Citation:** [Karnin 1990], [Molchanov et al. 2019], [Mozer and Smolensky 1988], [Srinivas and Babu 2016], [You et al. 2019], [Ding et al. 2019b]
- **Relevance:** These citations provide examples of different gradient-based methods for selecting elements to remove and their advantages and disadvantages.

**3.5 Selection Based on 2nd Order Taylor Expansion of the Training Loss Function**

- **Key Point:** Second-order methods use the Hessian matrix of the loss function to determine weight importance.
- **Citation:** [Le Cun et al. 1990], [Hassibi and Stork 1992], [Cibas et al. 1996], [Theis et al. 2018], [Singh and Alistarh 2020], [Dong et al. 2017], [Hassibi and Stork 1992], [Amari 1998], [Singh and Alistarh 2020]
- **Relevance:** These citations provide examples of different second-order methods for selecting elements to remove and their advantages and disadvantages.

**3.6 Selection Based on Regularization of the Loss During Training**

- **Key Point:** Regularization methods add penalty terms to the loss function to encourage sparsity.
- **Citation:** [Krogh and Hertz 1991], [Ge et al. 2011], [Louizos et al. 2018], [Srinivas et al. 2016], [Yin et al. 2019], [Xiao et al. 2019], [Yu et al. 2012], [Collins and Kohli 2014], [Zhuang et al. 2020], [Williams 1995], [Liu et al. 2015b], [Chao et al. 2020], [Yang et al. 2020b], [Aghasi et al. 2017], [Yuan and Lin 2006], [Pan et al. 2016], [Liu et al. 2017], [Gordon et al. 2018], [Chauvin 1989], [Tartaglione et al. 2018], [van Baalen et al. 2020], [Azarian et al. 2020]
- **Relevance:** These citations provide examples of different regularization methods and their applications in deep learning.

**3.7 Variational Selection Schemes**

- **Key Point:** Variational methods use Bayesian inference to approximate the posterior distribution of weights and prune weights with high variance.
- **Citation:** [Tipping 2001], [Kingma et al. 2015], [Kingma and Welling 2013], [Rezende et al. 2014], [Molchanov et al. 2017], [Srivastava et al. 2014a], [Gale et al. 2019], [Neklyudov et al. 2017], [Ullrich et al. 2017], [Louizos et al. 2017], [Lobacheva et al. 2018], [Kodryan et al. 2019], [Dai et al. 2018b], [Engelbrecht 2001]
- **Relevance:** These citations provide examples of different variational methods for selecting elements to remove and their advantages and disadvantages.

**3.8 Other Selection Schemes**

- **Key Point:** The authors discuss other selection schemes, including genetic algorithms, sampling-based pruning with guarantees, diversity networks, and quantized networks.
- **Citation:** [White and Ligomenides 1993], [Whitley and Bogart 1990], [Baykal et al. 2018], [Liebenwein et al. 2020], [Mariet and Sra 2017], [Guerra et al. 2020], [Tang et al. 2021], [Hebb 1949], [Sietsma and Dow 1988], [Sietsma and Dow 1991], [Kameyama and Kosugi 1991], [Suau et al. 2019], [Sun et al. 2015], [Zeng and Yeung 2006], [Lauret et al. 2006], [Han and Qiao 2013], [Afghan and Naumann 2020]
- **Relevance:** These citations provide examples of different selection schemes and their advantages and disadvantages.

**3.9 Parameter Budgets Between Different Layers**

- **Key Point:** The authors discuss the importance of choosing sparsity parameters per layer or globally for the whole model.
- **Citation:** [See et al. 2016], [Mocanu et al. 2018], [Evci et al. 2020], [Mostafa and Wang 2019], [Sanh et al. 2020]
- **Relevance:** These citations provide examples of different approaches to choosing sparsity parameters and their advantages and disadvantages.

**4. Dynamic Pruning: Network Regrowth During Training**

- **Key Point:** Dynamic pruning involves adding elements to the network during training to maintain its size.
- **Citation:** [Han and Qiao 2013], [Narasimha et al. 2008], [Bellec et al. 2018], [Mocanu et al. 2018], [Mostafa and Wang 2019], [Gordon et al. 2018], [Lin et al. 2020], [Wortsman et al. 2019], [Dettmers and Zettlemoyer 2019], [Evci et al. 2020], [Jayakumar et al. 2020], [Ye et al. 2020], [Zhuang et al. 2019]
- **Relevance:** These citations provide examples of different dynamic pruning techniques and their advantages and disadvantages.

**5. Ephemeral Sparsification Approaches**

- **Key Point:** Ephemeral sparsification involves introducing sparsity in the activations, gradients, or errors during the forward pass of training or the backward pass of training.
- **Citation:** [Glorot et al. 2011b], [Rhu et al. 2018], [Mishra et al. 2017], [Alwani et al. 2016], [Gudovskiy et al. 2018], [Liu et al. 2019], [Georgiadis 2019], [Kurtz et al. 2020], [Dong et al. 2019], [Hinton et al. 2012], [Srivastava et al. 2014a], [Wan et al. 2013], [Tompson et al. 2015], [Ghiasi et al. 2018], [Krueger et al. 2017], [Huang et al. 2016], [Larsson et al. 2017], [Fan et al. 2020], [Gal et al. 2017], [Maddison et al. 2017], [Kingma et al. 2015], [Molchanov et al. 2017], [Gomez et al. 2019], [Ben-Nun and Hoefler 2018], [Sun et al. 2017], [Wei et al. 2017]
- **Relevance:** These citations provide examples of different ephemeral sparsification techniques and their advantages and disadvantages.

**6. Sparse Deep Learning Architectures**

- **Key Point:** The authors discuss specific applications of sparsity techniques in deep learning architectures, focusing on convolutional neural networks and transformer networks.
- **Citation:** [Szegedy et al. 2015], [Howard et al. 2017], [Kuzmin et al. 2019], [Tan and Le 2020], [Han et al. 2016b], [Sun et al. 2015], [Zhou et al. 2016], [Tartaglione et al. 2018], [Molchanov et al. 2017], [Guo et al. 2016], [Bellec et al. 2018], [Mostafa and Wang 2019], [Dettmers and Zettlemoyer 2019], [Azarian et al. 2020], [He et al. 2019a], [Evci et al. 2020], [Singh and Alistarh 2020], [Gale et al. 2019], [Vaswani et al. 2017], [Devlin et al. 2019], [Brown et al. 2020], [Li et al. 2020a], [Liu et al. 2019b], [Gordon et al. 2020], [Chen et al. 2020], [Prasanna et al. 2020], [McCarley et al. 2020], [Wang et al. 2020a], [Lin et al. 2020], [Fan et al. 2020], [Michel et al. 2019], [Voita et al. 2019], [Jan et al. 2019], [Lison et al. 2019], [Prasanna et al. 2020], [Guo et al. 2019b], [Tay et al. 2020], [Yun et al. 2020], [Child et al. 2019], [Beltagy et al. 2020], [Zaheer et al. 2020], [Li et al. 2020], [Zhao et al. 2019], [Correia et al. 2019], [Cui et al. 2019], [Martins and Astudillo 2016], [Malaviya et al. 2018], [Niculae and Blondel 2017]
- **Relevance:** These citations provide examples of different sparse deep learning architectures and their advantages and disadvantages.

**7. Speeding Up Sparse Models**

- **Key Point:** The authors discuss algorithmic and hardware solutions for accelerating sparse models.
- **Citation:** [Sanh et al. 2020], [Han et al. 2017], [Yu et al. 2017], [Park et al. 2017], [Gale et al. 2020], [Yu et al. 2017], [Chetlur et al. 2014], [Louizos et al. 2018], [Mozer and Smolensky 1988], [Molchanov et al. 2017], [Sanh et al. 2020], [Ivanov et al. 2020], [Unat et al. 2017], [Han et al. 2016a], [Kim et al. 2018], [Parashar et al. 2017], [Chen et al. 2019], [Albericio et al. 2016], [Niu et al. 2020], [Niu et al. 2019], [Han et al. 2017], [Gupta et al. 2019], [Zhu et al. 2017], [Zhu et al. 2016], [Lym et al. 2019], [Mao et al. 2017], [Gondimalla et al. 2019], [Qin et al. 2020], [Zhang et al. 2016], [Zhang et al. 2019b], [Scheffler et al. 2020], [Hegde et al. 2019], [Kung et al. 2018], [Li et al. 2019], [Zhang et al. 2019b], [Yang et al. 2020a], [Zhang et al. 2019]
- **Relevance:** These citations provide examples of different algorithmic and hardware solutions for accelerating sparse models and their advantages and disadvantages.

**8. Discussion**

- **Key Point:** The authors discuss open questions and challenges in the field of sparsity, including the need for better understanding of how pruning influences network behavior, the development of more robust pruning methods, and the co-design of hardware and software for sparse models.
- **Citation:** [Blalock et al. 2020], [Hoefler and Belli 2015], [Millidge et al. 2020], [Ahmad and Scheinkman 2019], [Changpinyo et al. 2017], [Cohen et al. 2017], [Sun et al. 2015], [See et al. 2016], [Frankle and Carbin 2019], [Ramanujan et al. 2020], [Zhou et al. 2020], [Malach et al. 2020], [Orseau et al. 2020], [Pensia et al. 2020], [Liu et al. 2019b], [Frankle et al. 2020b], [Renda et al. 2020], [Savarese et al. 2020], [Chen et al. 2020], [Ding et al. 2019b], [Elsen et al. 2019], [Lee et al. 2020a], [Gomez et al. 2019], [Bartoldson et al. 2020], [Mattson et al. 2020], [Ben-Nun et al. 2019], [Hooker et al. 2019], [Hooker et al. 2020], [Bianco et al. 2018], [Elsen et al. 2019], [Lee et al. 2020a], [Blalock et al. 2020], [Howard et al. 2017], [Iandola et al. 2016], [Li et al. 2020a]
- **Relevance:** These citations provide examples of different research directions and challenges in the field of sparsity.

**9. Challenges and Open Questions**

- **Key Point:** The authors outline ten key challenges and open questions in the field of sparsity, including the need for better sparse training methods, a deeper understanding of the tradeoff between structured and unstructured sparsity, and the development of more efficient hardware architectures for sparse models.
- **Relevance:** These challenges and open questions highlight the need for further research in the field of sparsity.

**10. Conclusions and Outlook**

- **Key Point:** The authors conclude that sparsity is a promising technique for improving the efficiency of deep learning models, and that the trend towards larger and more complex models will likely drive further research in this area.
- **Relevance:** This conclusion highlights the importance of sparsity for the future of deep learning.

**Key Insights and Supporting Literature**

- **Key Insight:** Sparsity can improve generalization by reducing noise in the model and can also lead to computational savings.
- **Supporting Citations:** [Grünwald 2007], [Friston 2008]
- **Explanation:** These citations provide theoretical and biological justifications for sparsity in deep learning.

- **Key Insight:** Modern deep learning models are computationally expensive to train and use, with some models requiring billions of parameters.
- **Supporting Citations:** [Szegedy et al. 2016], [Brown et al. 2020]
- **Explanation:** These citations highlight the need for efficient compression techniques to address the computational and memory costs of large deep learning models.

- **Key Insight:** The authors distinguish between model sparsity (permanent changes to the network structure) and ephemeral sparsity (dynamic changes applied per example).
- **Supporting Citations:** [Sharma et al. 2017]
- **Explanation:** This citation highlights the distinction between model sparsity and ephemeral sparsity, which is crucial for understanding the different approaches to sparsification.

- **Key Insight:** The authors highlight the phenomenon of early structure adaptation, where the most important connections in a network are often determined early in the training process.
- **Supporting Citations:** [Shwartz-Ziv and Tishby 2017], [Achille et al. 2019], [Michel et al. 2019], [Ding et al. 2019b], [You et al. 2020], [Golub et al. 2019], [Li et al. 2020b]
- **Explanation:** These citations provide evidence for early structure adaptation and discuss its implications for sparsification.

**Experimental Methodology and Its Foundations**

- **Experimental Setup:** The paper does not conduct its own experiments but rather provides a comprehensive survey of existing research.
- **Cited Works as Basis for Methodology:** The authors draw upon a wide range of cited works to support their analysis and discussion of different sparsification techniques.
- **Novel Aspects of Methodology:** The authors introduce a new metric for parameter efficiency, called hardness-normalized parameter efficiency, which takes into account the difficulty of classifying different ImageNet classes.
- **Cited Works to Justify Novel Approaches:** The authors do not cite any specific works to justify their novel approach to parameter efficiency.

**Results in Context**

- **Main Results:** The paper provides a comprehensive overview of sparsity techniques in deep learning, covering both model sparsification and ephemeral sparsification. The authors discuss the advantages and disadvantages of different techniques, highlight key challenges and open questions in the field, and provide a set of best practices for practitioners.
- **Citations Used for Comparison with Existing Literature:** The authors cite a wide range of works to support their analysis and discussion of different sparsification techniques.
- **Confirmation, Contradiction, or Extension of Cited Works:** The authors do not explicitly confirm, contradict, or extend any specific cited works.

**Discussion and Related Work**

- **Situating Work Within Existing Literature:** The authors provide a comprehensive overview of sparsity techniques in deep learning, drawing upon a wide range of cited works to support their analysis and discussion.
- **Key Papers Cited in Discussion or Related Work Section:** The authors cite a wide range of works to support their analysis and discussion of different sparsification techniques.
- **Highlighting Novelty or Importance of Work:** The authors highlight the importance of sparsity for the future of deep learning and outline key challenges and open questions in the field.

**Future Work and Open Questions**

- **Areas for Further Research:** The authors outline ten key challenges and open questions in the field of sparsity, including the need for better sparse training methods, a deeper understanding of the tradeoff between structured and unstructured sparsity, and the development of more efficient hardware architectures for sparse models.
- **Citations Used to Support Suggestions for Future Work:** The authors do not cite any specific works to support their suggestions for future work.

**Critical Analysis of Citation Usage**

- **Effectiveness of Citation Usage:** The authors effectively use citations to support their arguments and provide context for their discussion.
- **Areas Where Additional Citations Might Have Been Beneficial:** The authors could have provided more specific citations to support their claims about the biological inspiration for sparsity in deep learning and the theoretical justifications for sparsity as a form of regularization.
- **Potential Biases in Selection of Cited Works:** The authors appear to have selected a wide range of works from different research areas, suggesting a lack of bias in their selection of cited works.

**Final Summary**

- **Contribution to the Field:** This paper provides a comprehensive survey of sparsity techniques in deep learning, covering both model sparsification and ephemeral sparsification. The authors discuss the advantages and disadvantages of different techniques, highlight key challenges and open questions in the field, and provide a set of best practices for practitioners.
- **Influential or Frequently Cited Works:** The authors cite a wide range of works to support their analysis and discussion of different sparsification techniques.
- **Integration of Existing Literature:** The authors effectively integrate existing literature to support their claims and findings, providing a comprehensive overview of the field of sparsity in deep learning.

**Overall Assessment:** This paper provides a valuable and comprehensive overview of sparsity techniques in deep learning. The authors effectively integrate existing literature to support their claims and findings, providing a clear and insightful discussion of the field. The paper is well-written and well-organized, making it a valuable resource for practitioners and researchers working in the field.