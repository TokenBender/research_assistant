## Primer: Searching for Efficient Transformers for Language Modeling

**1. Introduction**

- **Title:** Primer: Searching for Efficient Transformers for Language Modeling
- **Authors:** David R. So, Wojciech Mańke, Hanxiao Liu, Zihang Dai, Noam Shazeer, Quoc V. Le
- **Publication Date:** 24 January 2022
- **Objective:** The paper aims to reduce the training and inference costs of Transformer models for language modeling by searching for a more efficient variant.
- **Number of References:** 60

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - Transformers have become increasingly popular and effective for NLP tasks, but their training costs have grown rapidly.
    - The authors propose searching for more efficient Transformer variants by modifying its TensorFlow computation graph.
    - They focus on decoder-only auto-regressive language modeling due to its generality and success.
- **Significant Citations:**
    - **Claim:** Transformers have been used extensively in many NLP advances over the past few years.
        - **Citation:** Vaswani et al., 2017; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Raffel et al., 2020; Adiwardana et al., 2020; Brown et al., 2020.
        - **Explanation:** This citation provides a broad overview of the widespread adoption of Transformers in NLP research.
    - **Claim:** The costs of training larger models have become prohibitively expensive.
        - **Citation:** Brown et al., 2020; Kaplan et al., 2020.
        - **Explanation:** This citation highlights the growing concern about the computational cost of training large language models.
    - **Claim:** The authors focus on decoder-only auto-regressive language modeling due to its generality and success.
        - **Citation:** Radford et al., 2019; Brown et al., 2020; Schick and Schütze, 2021; Wang et al., 2021; Gao et al., 2020; Rae et al., 2020; Tay et al., 2020.
        - **Explanation:** This citation provides context for the authors' choice of research focus, highlighting the recent success of decoder-only auto-regressive language models.

**2.2 Searching Over TensorFlow Programs**

- **Key Points:**
    - The authors construct a search space for Transformer alternatives using TensorFlow operations.
    - Each program defines a stackable decoder block of an auto-regressive language model.
    - The search space includes simple primitive TF functions and subprograms that can be combined to create more complex architectures.
- **Significant Citations:**
    - **Claim:** The authors use operations from TensorFlow (TF) to construct a search space for Transformer alternatives.
        - **Citation:** Abadi et al., 2016.
        - **Explanation:** This citation introduces TensorFlow, the framework used for the search space.
    - **Claim:** The authors use relative dimensions to resize the models.
        - **Citation:** So et al., 2019.
        - **Explanation:** This citation introduces the concept of relative dimensions, a technique used to efficiently scale models during the search process.
    - **Claim:** The authors use a shared bank of values for constants and dimension sizes.
        - **Citation:** So et al., 2019.
        - **Explanation:** This citation explains the use of shared value banks, a technique that allows for efficient parameter sharing and modification during the search.
    - **Claim:** The authors use causal masking to prevent future positions from seeing the token they are trying to predict.
        - **Citation:** Not explicitly cited, but a common practice in language modeling.
        - **Explanation:** This is a standard practice in language modeling to ensure that the model learns to predict tokens based on past context only.
    - **Claim:** The authors use branching to enable multi-head capabilities for the Transformer search seed.
        - **Citation:** So et al., 2019.
        - **Explanation:** This citation introduces the concept of branching, a technique used to explore multi-head architectures during the search.
    - **Claim:** The authors use a deterministic pseudorandom approach to resolve dimension mismatches.
        - **Citation:** Not explicitly cited, but a common practice in deep learning.
        - **Explanation:** This is a common practice in deep learning to handle situations where tensors have incompatible dimensions.

**2.3 Evolutionary Search**

- **Key Points:**
    - The authors use Regularized Evolution with hurdles to find the most training-efficient architecture in the search space.
    - They define fitness as perplexity on the One Billion Words Benchmark (LM1B).
    - The search algorithm uses a fixed training budget and an implicit efficiency objective.
    - The authors use a proxy for full training time to reduce the search cost.
    - The search space is open-ended and requires strong initialization.
- **Significant Citations:**
    - **Claim:** The authors use Regularized Evolution with hurdles to find the most training-efficient architecture in the search space.
        - **Citation:** Real et al., 2019; So et al., 2019.
        - **Explanation:** This citation introduces the search algorithm used in the paper.
    - **Claim:** The authors define fitness as perplexity on the One Billion Words Benchmark (LM1B).
        - **Citation:** Chelba et al., 2014.
        - **Explanation:** This citation introduces the benchmark dataset used for evaluating model performance.
    - **Claim:** The authors use a fixed training budget and an implicit efficiency objective.
        - **Citation:** Not explicitly cited, but a common approach in architecture search.
        - **Explanation:** This approach contrasts with explicit efficiency objectives that focus on reducing training or inference step time.
    - **Claim:** The authors use a proxy for full training time to reduce the search cost.
        - **Citation:** Not explicitly cited, but a common practice in architecture search.
        - **Explanation:** This approach helps to reduce the computational cost of evaluating models during the search.
    - **Claim:** The search space is open-ended and requires strong initialization.
        - **Citation:** Elsken et al., 2019; Li and Talwalkar, 2019; Yu et al., 2020; Bender et al., 2020; Real et al., 2020.
        - **Explanation:** This citation highlights the challenges of searching in open-ended spaces and the importance of proper initialization.

**2.4 Primer**

- **Key Points:**
    - The discovered model, named Primer, exhibits strong performance improvements over common Transformer variants.
    - Primer achieves a target quality using a smaller training cost, achieves higher quality given a fixed training cost, and achieves a target quality using a smaller inference cost.
    - Primer's improvements are robust and hold across model sizes, compute scales, datasets, hardware platforms, Transformer codebases, and model families.
    - The authors open source their comparisons to help with reproducibility.
- **Significant Citations:**
    - **Claim:** Primer exhibits strong performance improvements over common Transformer variants.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the experimental results presented in the paper.
    - **Claim:** Primer achieves a target quality using a smaller training cost, achieves higher quality given a fixed training cost, and achieves a target quality using a smaller inference cost.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the experimental results presented in the paper.
    - **Claim:** Primer's improvements are robust and hold across model sizes, compute scales, datasets, hardware platforms, Transformer codebases, and model families.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the experimental results presented in the paper.
    - **Claim:** The authors open source their comparisons to help with reproducibility.
        - **Citation:** Not explicitly cited, but a common practice in research.
        - **Explanation:** This practice promotes transparency and allows other researchers to verify the findings of the paper.

**2.5 Primer-EZ**

- **Key Points:**
    - Primer-EZ is a Transformer with just two easy modifications: squaring feed forward ReLUs and adding depthwise convolution to attention multi-head projections.
    - The authors recommend Primer-EZ as a starting point for language modeling practitioners interested in using Primer.
- **Significant Citations:**
    - **Claim:** Primer-EZ is a Transformer with just two easy modifications: squaring feed forward ReLUs and adding depthwise convolution to attention multi-head projections.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the experimental results presented in the paper.
    - **Claim:** The authors recommend Primer-EZ as a starting point for language modeling practitioners interested in using Primer.
        - **Citation:** Not explicitly cited, but a common practice in research.
        - **Explanation:** This recommendation is based on the simplicity and effectiveness of Primer-EZ.

**2.6 Squared ReLU**

- **Key Points:**
    - The authors propose using squared ReLU activations in the Transformer's feed forward block.
    - Squared ReLU has drastically different asymptotics compared to other common activation functions.
    - Squared ReLU has significant overlap with GLU variants and captures their benefits while being simpler and without additional parameters.
- **Significant Citations:**
    - **Claim:** The authors propose using squared ReLU activations in the Transformer's feed forward block.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the experimental results presented in the paper.
    - **Claim:** Squared ReLU has drastically different asymptotics compared to other common activation functions.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the visual comparison of activation functions in Figure 5.
    - **Claim:** Squared ReLU has significant overlap with GLU variants and captures their benefits while being simpler and without additional parameters.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the visual comparison of activation functions in Figure 5 and the experimental results presented in the paper.

**2.7 Multi-DConv-Head Attention (MDHA)**

- **Key Points:**
    - The authors propose adding 3x1 depthwise convolutions after each of the multi-head projections for query Q, key K, and value V in self-attention.
    - This ordering of pointwise followed by depthwise convolution is the reverse of typical separable convolution.
    - Depthwise convolutions have been used for Transformers before, but not in this specific configuration.
- **Significant Citations:**
    - **Claim:** The authors propose adding 3x1 depthwise convolutions after each of the multi-head projections for query Q, key K, and value V in self-attention.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the experimental results presented in the paper.
    - **Claim:** This ordering of pointwise followed by depthwise convolution is the reverse of typical separable convolution.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the experimental results presented in the paper.
    - **Claim:** Depthwise convolutions have been used for Transformers before, but not in this specific configuration.
        - **Citation:** Wu et al., 2020; Baevski and Auli, 2019.
        - **Explanation:** This citation provides context for the authors' novel approach to using depthwise convolutions in Transformers.

**2.8 Other Modifications**

- **Key Points:**
    - The authors explore other modifications to the Transformer architecture, but find them less effective.
    - These modifications include shared Q and K depthwise representation, pre and post normalization, custom normalization, 12X bottleneck projection, post-softmax spatial gating, and extraneous modifications.
- **Significant Citations:**
    - **Claim:** The authors explore other modifications to the Transformer architecture, but find them less effective.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the experimental results presented in the paper.
    - **Claim:** These modifications include shared Q and K depthwise representation, pre and post normalization, custom normalization, 12X bottleneck projection, post-softmax spatial gating, and extraneous modifications.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the detailed analysis of modifications presented in the paper.

**2.9 Results**

- **Key Points:**
    - Primer outperforms baseline models on the search task, achieving a speedup factor of 1.7X or more.
    - The relationship between Primer's compute savings over Transformers and model quality follows a power law at optimal model sizes.
    - Primer's gains transfer across datasets and codebases.
    - Primer enables 4.2X compute savings at a 500M parameter size using full compute T5 training.
    - Primer's gains transfer to the pretraining and one-shot downstream task setup established by GPT-3.
- **Significant Citations:**
    - **Claim:** Primer outperforms baseline models on the search task, achieving a speedup factor of 1.7X or more.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the experimental results presented in Figure 6.
    - **Claim:** The relationship between Primer's compute savings over Transformers and model quality follows a power law at optimal model sizes.
        - **Citation:** Kaplan et al., 2020.
        - **Explanation:** This citation provides context for the authors' findings on the scaling laws of Primer.
    - **Claim:** Primer's gains transfer across datasets and codebases.
        - **Citation:** Not explicitly cited, but a key finding of the paper.
        - **Explanation:** This claim is supported by the experimental results presented in Figures 6 and 9.
    - **Claim:** Primer enables 4.2X compute savings at a 500M parameter size using full compute T5 training.
        - **Citation:** Raffel et al., 2020.
        - **Explanation:** This citation provides context for the authors' comparison of Primer with the original T5 architecture.
    - **Claim:** Primer's gains transfer to the pretraining and one-shot downstream task setup established by GPT-3.
        - **Citation:** Brown et al., 2020.
        - **Explanation:** This citation provides context for the authors' evaluation of Primer's performance on downstream tasks.

**2.10 Discussion**

- **Key Points:**
    - The authors discuss the limitations of their study, including the smaller model sizes compared to state-of-the-art models and the focus on decoder-only models.
    - They recommend the adoption of Primer and Primer-EZ for auto-regressive language modeling due to their strong performance, simplicity, and robustness.
    - They encourage further research into the development of efficient Transformers, particularly focusing on activation functions and encoder-decoder models.
- **Significant Citations:**
    - **Claim:** The authors discuss the limitations of their study, including the smaller model sizes compared to state-of-the-art models and the focus on decoder-only models.
        - **Citation:** Brown et al., 2020.
        - **Explanation:** This citation highlights the limitations of the study in terms of model size and architecture.
    - **Claim:** They recommend the adoption of Primer and Primer-EZ for auto-regressive language modeling due to their strong performance, simplicity, and robustness.
        - **Citation:** Not explicitly cited, but a key conclusion of the paper.
        - **Explanation:** This recommendation is based on the experimental results and the ease of implementation of Primer and Primer-EZ.
    - **Claim:** They encourage further research into the development of efficient Transformers, particularly focusing on activation functions and encoder-decoder models.
        - **Citation:** Not explicitly cited, but a key suggestion for future work.
        - **Explanation:** This suggestion is based on the findings of the paper and the potential for further improvements in Transformer efficiency.

**3. Key Insights and Supporting Literature**

- **Key Insight:** Primer, a Transformer variant with squared ReLU activations and depthwise convolutions in multi-head attention, significantly reduces training costs while maintaining or improving performance.
    - **Supporting Citations:** Vaswani et al., 2017; Real et al., 2019; So et al., 2019; Chelba et al., 2014; Kaplan et al., 2020; Raffel et al., 2020; Brown et al., 2020.
    - **Explanation:** These citations provide the foundation for the paper's research, introducing Transformers, architecture search, and the benchmark datasets used for evaluation.
- **Key Insight:** The compute savings of Primer over Transformers increase as training cost grows, following a power law with respect to quality at optimal model sizes.
    - **Supporting Citations:** Kaplan et al., 2020.
    - **Explanation:** This citation provides context for the authors' findings on the scaling laws of Primer.
- **Key Insight:** Primer's improvements transfer across datasets, codebases, hardware platforms, and model families.
    - **Supporting Citations:** Not explicitly cited, but a key finding of the paper.
    - **Explanation:** This insight is supported by the experimental results presented in Figures 6 and 9.
- **Key Insight:** Primer-EZ, a simplified version of Primer with only squared ReLU activations and depthwise convolutions in multi-head attention, captures much of the gains of the full Primer.
    - **Supporting Citations:** Not explicitly cited, but a key finding of the paper.
    - **Explanation:** This insight is supported by the experimental results presented in the paper.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The authors use Regularized Evolution with hurdles to search for efficient Transformer variants in a TensorFlow-based search space.
    - They evaluate model performance using perplexity on the One Billion Words Benchmark (LM1B) and other datasets.
    - They compare Primer with baseline Transformer variants across different model sizes, compute scales, datasets, hardware platforms, Transformer codebases, and model families.
- **Foundations:**
    - **Search Algorithm:** Real et al., 2019; So et al., 2019.
    - **TensorFlow:** Abadi et al., 2016.
    - **Benchmark Dataset:** Chelba et al., 2014.
- **Novel Aspects:**
    - The authors use an open-ended search space that allows for more flexible modifications to the Transformer architecture.
    - They use a fixed training budget and an implicit efficiency objective, which contrasts with previous approaches that focus on reducing training or inference step time.
    - They use a proxy for full training time to reduce the search cost.
    - They use conceptual initialization to bias the search towards promising architectures.
- **Justification:**
    - The authors justify their novel approaches by highlighting the limitations of previous architecture search methods and the need for more flexible and efficient search strategies.

**5. Results in Context**

- **Main Results:**
    - Primer outperforms baseline models on the search task, achieving a speedup factor of 1.7X or more.
    - The relationship between Primer's compute savings over Transformers and model quality follows a power law at optimal model sizes.
    - Primer's gains transfer across datasets and codebases.
    - Primer enables 4.2X compute savings at a 500M parameter size using full compute T5 training.
    - Primer's gains transfer to the pretraining and one-shot downstream task setup established by GPT-3.
- **Comparison with Existing Literature:**
    - **Search Task:** The authors compare Primer with baseline models on the search task, demonstrating its superior performance.
    - **Scaling Laws:** The authors confirm the power law relationship between compute and quality observed in previous work.
    - **Transferability:** The authors demonstrate that Primer's gains transfer across datasets, codebases, and hardware platforms, extending the findings of previous work.
    - **Large-Scale Training:** The authors compare Primer with the original T5 architecture in a large-scale training regime, demonstrating significant compute savings.
    - **Downstream Tasks:** The authors compare Primer with GPT-3 XL on downstream tasks, demonstrating its ability to achieve similar performance with less training compute.
- **Confirmation, Contradiction, or Extension:**
    - The authors' results confirm the power law relationship between compute and quality observed in previous work.
    - The authors' results extend previous work by demonstrating the transferability of Primer's gains across different datasets, codebases, and hardware platforms.
    - The authors' results contradict previous work by showing that Primer can achieve similar performance as GPT-3 XL with less training compute.

**6. Discussion and Related Work**

- **Situating the Work:**
    - The authors situate their work within the broader context of research on Transformers and architecture search.
    - They acknowledge the limitations of their study, including the smaller model sizes compared to state-of-the-art models and the focus on decoder-only models.
    - They highlight the novelty of their approach, particularly the use of an open-ended search space and an implicit efficiency objective.
- **Key Papers Cited:**
    - **Transformers:** Vaswani et al., 2017; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Raffel et al., 2020; Adiwardana et al., 2020; Brown et al., 2020.
    - **Architecture Search:** Real et al., 2019; So et al., 2019; Elsken et al., 2019; Li and Talwalkar, 2019; Yu et al., 2020; Bender et al., 2020; Real et al., 2020.
    - **Scaling Laws:** Kaplan et al., 2020.
    - **GPT-3:** Brown et al., 2020.
- **Highlighting Novelty:**
    - The authors highlight the novelty of their approach by contrasting it with previous architecture search methods, particularly those that focus on explicit efficiency objectives and use more restrictive search spaces.
    - They emphasize the simplicity and effectiveness of Primer and Primer-EZ, making them attractive options for practitioners.

**7. Future Work and Open Questions**

- **Areas for Further Research:**
    - Investigating the effectiveness of Primer and Primer-EZ for encoder-decoder models and masked language modeling.
    - Exploring the potential of other simple modifications to Transformer architectures, particularly focusing on activation functions.
    - Scaling Primer to larger model sizes and comparing its performance with state-of-the-art models.
- **Citations:**
    - **Encoder-Decoder Models:** Narang et al., 2021.
    - **Masked Language Modeling:** Not explicitly cited, but a common task in NLP.
    - **GPT-3:** Brown et al., 2020.

**8. Critical Analysis of Citation Usage**

- **Effectiveness:**
    - The authors effectively use citations to support their arguments and findings.
    - They provide a comprehensive overview of the relevant literature and clearly distinguish their contributions from previous work.
- **Areas for Improvement:**
    - The authors could have provided more specific citations for some of their claims, particularly those related to common practices in deep learning and architecture search.
    - They could have included more citations to work on the environmental impact of large language models, particularly in the context of their carbon emission estimates.
- **Potential Biases:**
    - The authors primarily cite work from Google Research, which may reflect a bias towards their own institution.
    - They could have included more citations to work from other research groups and institutions to provide a more balanced perspective on the field.

**9. Final Summary**

- **Contribution:** The paper presents Primer, a novel Transformer variant that significantly reduces training costs while maintaining or improving performance.
- **Influential Works:** Vaswani et al., 2017; Real et al., 2019; So et al., 2019; Kaplan et al., 2020; Brown et al., 2020.
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. It provides a comprehensive overview of the relevant research and clearly distinguishes its contributions from previous work.

**Overall Assessment:** The paper makes a significant contribution to the field of efficient Transformer design. It presents a novel architecture search method and identifies two simple modifications that significantly improve training efficiency. The authors provide a thorough analysis of their findings and effectively situate their work within the broader context of research on Transformers and architecture search. The paper is well-written and well-supported by citations, making it a valuable resource for researchers and practitioners interested in efficient language modeling.
