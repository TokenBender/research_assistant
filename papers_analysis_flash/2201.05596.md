## DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale

**1. Introduction**

- **Title:** DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale
- **Authors:** Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He
- **Publication Date:** 21 Jul 2022 (v2)
- **Objective:** The paper aims to address the challenges of training and deploying large Mixture-of-Experts (MoE) models, particularly for auto-regressive natural language generation (NLG) tasks, by proposing novel MoE architectures, model compression techniques, and a highly optimized inference system.
- **Total References:** 53

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:** The authors highlight the increasing size of trained models and the associated computational challenges. They introduce MoE models as a promising solution for reducing training costs while maintaining or improving model quality. However, they point out the challenges of MoE inference, which limits its practical usage.
- **Significant Citations:**
    - **Claim:** "In the last three years, the largest trained model has increased in size by over 1000x, from a few hundred million parameters to half a trillion parameters (Megatron-Turing NLG 530B)."
    - **Citation:** [2] Nvidia. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World's Largest and Most Powerful Generative Language Model. https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/, 2021.
    - **Explanation:** This citation provides evidence for the rapid growth in model size, highlighting the Megatron-Turing NLG 530B model as a significant example.
    - **Claim:** "Another 3 to 5 times of increase in dense model size would be infeasible within a reasonable timeframe."
    - **Citation:** [2] Nvidia. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World's Largest and Most Powerful Generative Language Model. https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/, 2021.
    - **Explanation:** This citation further emphasizes the computational limitations of training extremely large dense models, setting the stage for the introduction of MoE models.

**2.2 Related Work**

**2.2.1 Large Scale Dense NLP Models**

- **Key Points:** The authors review the trend of increasing model size in large-scale dense NLP models, citing examples like BERT, XLNet, ROBERTa, ALBERT, GPT, GPT-2, TuringNLG, Megatron-LM, T5, and GPT-3. They highlight the limitations of simply increasing model size due to computational constraints.
- **Significant Citations:**
    - **Claim:** "To test and verify the upper bound of scaling law [6] for model capacity with respect to number of parameters, the pretrained natural language processing model size has been increasing 10x per year for the last several years."
    - **Citation:** [6] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
    - **Explanation:** This citation introduces the concept of scaling laws, which provides a theoretical framework for understanding the relationship between model size and performance.
    - **Claim:** "The GPT-3 [22] further pushes the upper limit to 175 billions parameters, and shows that with zero/few-shot learning, it can achieve comparable or even better performance than previous small scale models with finetuning."
    - **Citation:** [22] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
    - **Explanation:** This citation highlights the significant size of GPT-3 and its ability to achieve impressive performance with zero/few-shot learning, further emphasizing the trend of increasing model size.

**2.2.2 Reducing Training Cost by MoE Architecture**

- **Key Points:** The authors discuss the use of Mixture-of-Experts (MoE) models as a promising approach to reduce training costs while maintaining or improving model quality. They cite several works that demonstrate the effectiveness of MoE in scaling model size without increasing computational cost.
- **Significant Citations:**
    - **Claim:** "One promising way to reduce the training cost is using Mixture of Expert (MoE) [24]."
    - **Citation:** [24] Saeed Masoudnia and Reza Ebrahimpour. Mixture of experts: a literature survey. Artificial Intelligence Review, 42(2):275-293, 2014.
    - **Explanation:** This citation introduces the concept of MoE models and provides a general overview of the research area.
    - **Claim:** "GShard [4] utilizes MoE to train a transformer-based model [26] to 600B parameters for multi-language translation, and it shows that the training cost of this 600B MoE model is even cheaper than that of a 100B dense model."
    - **Citation:** [4] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.
    - **Explanation:** This citation provides a specific example of how MoE models can be used to train significantly larger models with lower training costs compared to dense models.
    - **Claim:** "Switch Transformer [5] continues this based on the T5 model and scales the model to 1.6 trillion. To achieve same accuracy performance, [5] shows a 2.5x faster training speed of MoE models as compared to large dense models."
    - **Citation:** [5] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.
    - **Explanation:** This citation further highlights the efficiency gains achieved by using MoE models for training, demonstrating a significant speed advantage over dense models.

**2.2.3 MoE Training and Inference Systems**

- **Key Points:** The authors discuss existing MoE training systems, including DeepSpeed-MoE, FastMoE, and Fairseq-MoE, highlighting their limitations in terms of scalability and support for inference. They emphasize the need for efficient and scalable MoE inference systems.
- **Significant Citations:**
    - **Claim:** "DeepSpeed MoE training system [32] was primarily targeted for optimized training of MoE models at scale."
    - **Citation:** [32] Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andrés Felipe Cruz-Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efficient moe training for multitask multilingual models. CoRR, abs/2109.10465, 2021.
    - **Explanation:** This citation introduces DeepSpeed-MoE as a specific example of an existing MoE training system, highlighting its focus on scalability.
    - **Claim:** "FastMoE [33] is a research software developed to show how MoE models can be trained under data and expert (model) parallelism."
    - **Citation:** [33] Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. Fastmoe: A fast mixture-of-expert training system. CoRR, abs/2103.13262, 2021.
    - **Explanation:** This citation introduces FastMoE as another example of an existing MoE training system, highlighting its focus on data and expert parallelism.
    - **Claim:** "Fairseq-MoE [31] offers an MOE API as well as a training pipeline for generic language models."
    - **Citation:** [31] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684, 2021.
    - **Explanation:** This citation introduces Fairseq-MoE as a third example of an existing MoE training system, highlighting its focus on providing an API and training pipeline for generic language models.

**3. DeepSpeed-MoE for NLG: Reducing the Training Cost of Language Models by 5 Times**

- **Key Points:** The authors present their approach for applying MoE models to auto-regressive NLG tasks, demonstrating a 5x reduction in training cost while achieving the same model quality as a dense NLG model. They introduce their MoE-based NLG model architecture and discuss the training and evaluation settings.
- **Significant Citations:**
    - **Claim:** "To create an MoE based NLG model, we studied the GPT like transformer-based NLG model."
    - **Citation:** [22] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
    - **Explanation:** This citation highlights the use of GPT-like transformer-based NLG models as a foundation for their MoE-based NLG model architecture.
    - **Claim:** "We pre-trained both the dense and MoE version of the above models using DeepSpeed on 128 Ampere A100 GPUs (Azure ND A100 instances)."
    - **Citation:** [23] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–16. IEEE, 2020.
    - **Explanation:** This citation highlights the use of DeepSpeed and A100 GPUs for training both dense and MoE models, demonstrating the scalability of their approach.

**3.1 MoE based NLG Model Architecture**

- **Key Points:** The authors describe their MoE-based NLG model architecture, which uses a dense model as the base and adds MoE layers on every other feedforward layer. They discuss the use of a gating function to activate a subset of experts in the MoE layer for each token.
- **Significant Citations:**
    - **Claim:** "We use a gating function to activate a subset of experts in the MoE layer for each token."
    - **Citation:** [3] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.
    - **Explanation:** This citation introduces the concept of gating functions, which are commonly used in MoE models to select a subset of experts for each input token.

**3.2 Training and Evaluation Settings**

- **Key Points:** The authors discuss the training and evaluation settings used for their MoE-based NLG models, including the hardware resources, training data, and evaluation metrics.
- **Significant Citations:**
    - **Claim:** "We used the same training data for the MT-NLG model [2]."
    - **Citation:** [2] Nvidia. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World's Largest and Most Powerful Generative Language Model. https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/, 2021.
    - **Explanation:** This citation highlights the use of the MT-NLG dataset for training their MoE-based NLG models, ensuring a fair comparison with existing work.

**3.3 MoE Leads to Better Quality for NLG Models**

- **Key Points:** The authors present experimental results demonstrating that their MoE-based NLG models achieve better validation loss and comparable or better performance on downstream tasks compared to their dense counterparts with 4-5x larger base models.
- **Significant Citations:**
    - **Claim:** "Figure 1 shows that the validation loss for the MoE versions of the model is significantly better than their dense counter parts."
    - **Citation:** [22] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
    - **Explanation:** This citation provides evidence for the improved validation loss achieved by their MoE-based NLG models compared to dense models.
    - **Claim:** "Furthermore, the model quality is on par not only for the validation loss but also for the zero-shot evaluation on the 6 downstream tasks as shown in Table 2, demonstrating that MoE models and their dense counter part with 4-5x larger base have very similar model quality."
    - **Citation:** [22] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
    - **Explanation:** This citation provides further evidence for the comparable performance of their MoE-based NLG models compared to dense models with significantly larger base models, highlighting the effectiveness of MoE in achieving similar quality with fewer resources.

**3.4 Same Quality with 5x Less Training Cost**

- **Key Points:** The authors highlight the significant training cost reduction achieved by using MoE models, demonstrating that they can achieve the same model quality as a dense model with 5x less training cost.
- **Significant Citations:**
    - **Claim:** "To conclude, this section shows significant training cost saving of using MoE on NLG models: by applying MoE we achieved the model quality of a 6.7B parameter dense NLG model at the cost of training a 1.3B base model, thanks to the sparse structure of MoE."
    - **Citation:** [22] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
    - **Explanation:** This citation provides a quantitative measure of the training cost reduction achieved by using MoE models, highlighting the significant advantage of MoE in terms of computational efficiency.

**4. PR-MoE and MoS: Reducing the Model Size and Improving Parameter Efficiency**

- **Key Points:** The authors introduce two novel MoE architectures: Pyramid-Residual MoE (PR-MoE) and Mixture-of-Students (MoS). PR-MoE reduces the model size by up to 3x without affecting model quality, while MoS further reduces the model size by up to 3.7x through knowledge distillation.
- **Significant Citations:**
    - **Claim:** "While MoE based models achieve the same quality with 5x training cost reduction in the NLG example, the resulting model has roughly 8x the parameters of the corresponding dense model (e.g., 6.7B dense model has 6.7 billion parameters and 1.3B+MoE-128 has 52 billion parameters)."
    - **Citation:** [22] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
    - **Explanation:** This citation highlights the significant increase in model size associated with MoE models, motivating the need for parameter efficiency improvements.
    - **Claim:** "This phenomenon is referred to as Phenomenon-II."
    - **Citation:** [37] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818-833. Springer, 2014.
    - **Explanation:** This citation provides a theoretical basis for the intuition behind the PR-MoE architecture, drawing a parallel to the concept of representation learning in convolutional neural networks.
    - **Claim:** "We design a novel MoE-to-MoE knowledge distillation technique to create a distilled version of PR-MOE, which we call Mixture-of-Students (MoS), that further reduces the MoE model size, optimizing inference time and cost."
    - **Citation:** [44] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015.
    - **Explanation:** This citation introduces the concept of knowledge distillation, which is used as a basis for the MoS architecture, highlighting its potential for reducing model size and improving inference efficiency.

**4.1 PR-MoE: Pyramid-Residual-MoE for Smaller Model Size and Fast Inference**

- **Key Points:** The authors describe the PR-MoE architecture, which combines the Pyramid-MoE and Residual-MoE designs to achieve smaller model size and faster inference. They discuss the intuition behind the PR-MoE design and its implementation.
- **Significant Citations:**
    - **Claim:** "First, the standard MoE architecture has the same number and structure of experts in all MoE layers. This reminds us a fundamental question in machine learning community: do all the layers in a Deep Neural Network learn the same representation?"
    - **Citation:** [37] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818-833. Springer, 2014.
    - **Explanation:** This citation provides a theoretical basis for the intuition behind the PR-MoE architecture, drawing a parallel to the concept of representation learning in convolutional neural networks.
    - **Claim:** "This also inspired transfer learning in CV to freeze shallow layers for finetuning [38]."
    - **Citation:** [38] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? arXiv preprint arXiv:1411.1792, 2014.
    - **Explanation:** This citation further supports the intuition behind the PR-MoE architecture, highlighting the concept of transfer learning in convolutional neural networks.

**4.2 Mixture-of-Students: Distillation for Even Smaller Model Size and Faster Inference**

- **Key Points:** The authors introduce the MoS architecture, which uses knowledge distillation to further reduce the model size of PR-MoE while maintaining comparable performance. They discuss the architecture choice, optimization objective, and staged knowledge distillation approach.
- **Significant Citations:**
    - **Claim:** "KD has been proven to be a successful way to compress a large model into a small one, which contains much fewer parameters and computations but still obtaining competitive results."
    - **Citation:** [44] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015.
    - **Explanation:** This citation introduces the concept of knowledge distillation, highlighting its potential for model compression and improving inference efficiency.
    - **Claim:** "We take a general formulation of the KD loss [50] as:"
    - **Citation:** [50] Dong Yu, Kaisheng Yao, Hang Su, Gang Li, and Frank Seide. Kl-divergence regularized deep neural network adaptation for improved large vocabulary speech recognition. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26-31, 2013, pages 7893-7897. IEEE, 2013.
    - **Explanation:** This citation provides a general formulation of the knowledge distillation loss, which is used as a basis for their MoS architecture.

**4.2.1 Mixture-of-Students via Staged KD**

- **Key Points:** The authors discuss the architecture choice and optimization objective for MoS, highlighting the use of staged knowledge distillation to improve student accuracy.
- **Significant Citations:**
    - **Claim:** "We first train a teacher MoE model. We reduce the depth of each expert branch in the teacher model to obtain a corresponding student. By doing so, the final student model that has the same sparsely gated architecture as the teacher MoE except that each expert branch has a smaller depth."
    - **Citation:** [44] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015.
    - **Explanation:** This citation provides a general description of the knowledge distillation process, which is used as a basis for their MoS architecture.

**4.2.2 Evaluation of Mixture-of-Students**

- **Key Points:** The authors evaluate the performance of their MoS models, demonstrating that staged knowledge distillation improves student accuracy compared to applying KD for the entire training process.
- **Significant Citations:**
    - **Claim:** "We first evaluate how the proposed stage-KD affects the pre-training convergence."
    - **Citation:** [44] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015.
    - **Explanation:** This citation highlights the use of knowledge distillation as a technique for improving model training convergence.

**5. DeepSpeed-MoE Inference: Serving MoE Models at Unprecedented Scale and Speed**

- **Key Points:** The authors introduce their DeepSpeed-MoE inference system, which leverages a combination of parallelism strategies and optimized kernels to achieve unprecedented scale and speed for MoE inference. They discuss the design of the DeepSpeed-MoE inference system, including the use of expert parallelism, expert-slicing, data parallelism, and tensor-slicing.
- **Significant Citations:**
    - **Claim:** "DeepSpeed MoE training system [32] was primarily targeted for optimized training of MoE models at scale."
    - **Citation:** [32] Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andrés Felipe Cruz-Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efficient moe training for multitask multilingual models. CoRR, abs/2109.10465, 2021.
    - **Explanation:** This citation highlights the use of DeepSpeed-MoE as a specific example of an existing MoE training system, highlighting its focus on scalability.
    - **Claim:** "To address these challenges, we develop and implement a flexible multi-expert and multi-data parallelism design on top of DeepSpeed-MoE, that allows for training different parts of the model with different expert and data parallelism degree."
    - **Citation:** [32] Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andrés Felipe Cruz-Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efficient moe training for multitask multilingual models. CoRR, abs/2109.10465, 2021.
    - **Explanation:** This citation highlights the use of DeepSpeed-MoE as a specific example of an existing MoE training system, highlighting its focus on scalability.

**5.1 Design of DeepSpeed-MoE Inference System**

- **Key Points:** The authors discuss the design of their DeepSpeed-MoE inference system, highlighting the use of expert parallelism, expert-slicing, data parallelism, and tensor-slicing to optimize inference performance.
- **Significant Citations:**
    - **Claim:** "From the best-case view, each input token of an MoE model (with top-1 gating) only activates a single expert at each MoE layer, resulting in a critical data path that is equivalent to the base dense model size, orders-of-magnitude smaller than the actual model size."
    - **Citation:** [3] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.
    - **Explanation:** This citation highlights the potential for MoE models to achieve low latency inference due to the sparse nature of their activation patterns.

**5.2 Flexible Combination of Tensor-Slicing, Expert-Slicing, Data Parallelism, and Expert Parallelism**

- **Key Points:** The authors discuss the use of various parallelism strategies in their DeepSpeed-MoE inference system, including expert parallelism, expert-slicing, data parallelism, and tensor-slicing, to optimize inference performance.
- **Significant Citations:**
    - **Claim:** "To achieve low latency and high throughput at an unprecedented scale for MoE, we design our inference system to minimize the critical data path per device, maximize the achievable aggregate memory bandwidth, and offer ample aggregate memory simultaneously to enable massive model sizes by using (1) expert parallelism [32] and slicing on expert parameters and (2) data parallelism and tensor-slicing for non-expert parameters."
    - **Citation:** [32] Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andrés Felipe Cruz-Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efficient moe training for multitask multilingual models. CoRR, abs/2109.10465, 2021.
    - **Explanation:** This citation highlights the use of DeepSpeed-MoE as a specific example of an existing MoE training system, highlighting its focus on scalability.

**5.3 Optimized Communication Subsystem: Grouping and Routing Tokens More Efficiently**

- **Key Points:** The authors discuss the optimization of communication in their DeepSpeed-MoE inference system, highlighting the use of hierarchical all-to-all communication and parallelism-coordinated communication optimization to reduce communication overhead.
- **Significant Citations:**
    - **Claim:** "Expert parallelism requires all-to-all communication between all expert parallel devices. By default, DS-MOE uses NCCL for this communication via "torch.distributed" interface, but we observe major overhead when it is used at scale (more results in Section 5.5)."
    - **Citation:** [52] Zixian Cai, Zhengyang Liu, Saeed Maleki, Madan Musuvathi, Todd Mytkowicz, Jacob Nelson, and Olli Saarikivi. SCCL: Synthesizing Optimal Collective Algorithms. CORR, abs/2008.08708, 2020.
    - **Explanation:** This citation highlights the use of NCCL as a standard communication library for expert parallelism, but also points out its limitations in terms of scalability.

**5.4 Highly Optimized Transformer and MoE Related Kernels**

- **Key Points:** The authors discuss the optimization of transformer and MoE-related kernels in their DeepSpeed-MoE inference system, highlighting the use of dense representation and kernel-fusion to reduce computational complexity and latency.
- **Significant Citations:**
    - **Claim:** "DS-MoE inference system consists of highly optimized multi-GPU transformer kernels as well as highly optimized MoE related kernels."
    - **Citation:** [53] DeepSpeed Team, Rangan Majumder, and Andrey Proskurin. DeepSpeed: Accelerating large-scale model inference and training via system optimizations and compression. https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/, 2021. [Online].
    - **Explanation:** This citation highlights the use of DeepSpeed as a framework for optimizing transformer kernels, providing a context for their MoE-related kernel optimizations.

**5.5 Performance Evaluation of DS-MoE Inference**

- **Key Points:** The authors present experimental results demonstrating the performance of their DeepSpeed-MoE inference system, highlighting its ability to achieve low latency and high throughput for MoE models at scale. They compare the performance of DeepSpeed-MoE with PyTorch, demonstrating the significant advantages of DeepSpeed-MoE in terms of scalability and efficiency.
- **Significant Citations:**
    - **Claim:** "For dense models, throughput can be increased by using multiple GPUs and data parallelism (independent replicas with no inter-GPU communication), whereas lower latency can be achieved by techniques like tensor-slicing to partition the model across multiple GPUs [53]."
    - **Citation:** [53] DeepSpeed Team, Rangan Majumder, and Andrey Proskurin. DeepSpeed: Accelerating large-scale model inference and training via system optimizations and compression. https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/, 2021. [Online].
    - **Explanation:** This citation highlights the use of DeepSpeed as a framework for optimizing dense model inference, providing a context for their MoE-related inference optimizations.

**5.5.1 Achieving Low Latency and Super-Linear Throughput Increase Simultaneously**

- **Key Points:** The authors discuss the unique properties of MoE inference that allow for achieving both low latency and high throughput simultaneously, highlighting the advantages of expert parallelism and DeepSpeed-MoE's ability to exploit these advantages.
- **Significant Citations:**
    - **Claim:** "Diving a bit deeper, we see two key properties of expert parallelism at play here: 1) when using expert parallelism, the number of experts per GPU decrease as we increase the number of GPUs. E.g. this 52B MoE model has 128 total experts; if we serve this using 8 GPUs, we need 16 experts per GPU, whereas on 64 GPUs, we only need 2 experts per GPU."
    - **Citation:** [32] Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andrés Felipe Cruz-Salinas, Liyang Lu, Amr Hendy, Samyam Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efficient moe training for multitask multilingual models. CoRR, abs/2109.10465, 2021.
    - **Explanation:** This citation highlights the use of DeepSpeed-MoE as a specific example of an existing MoE training system, highlighting its focus on scalability.

**5.5.2 Low Latency and High Throughput at Unprecedented Scale**

- **Key Points:** The authors present experimental results demonstrating the scalability of their DeepSpeed-MoE inference system, highlighting its ability to achieve low latency and high throughput for MoE models with up to 2 trillion parameters.
- **Significant Citations:**
    - **Claim:** "By effectively exploiting hundreds of GPUs in parallel, DeepSpeed-MoE achieves an unprecedented scale for inference at incredibly low latencies a staggering trillion parameter MoE model can be inferenced under 25ms."
    - **Citation:** [53] DeepSpeed Team, Rangan Majumder, and Andrey Proskurin. DeepSpeed: Accelerating large-scale model inference and training via system optimizations and compression. https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/, 2021. [Online].
    - **Explanation:** This citation highlights the use of DeepSpeed as a framework for optimizing dense model inference, providing a context for their MoE-related inference optimizations.

**5.5.3 Enhanced Benefits of PR-MoE and MoS**

- **Key Points:** The authors discuss the combined benefits of PR-MoE and MoS in conjunction with their DeepSpeed-MoE inference system, highlighting the reduction in the minimum number of GPUs required for inference and further improvements in latency and throughput.
- **Significant Citations:**
    - **Claim:** "For both Figures 12 and 13, we show a comparison of three model variants along with the baseline version (standard MoE on PyTorch): (i) the standard MoE Model denoted by MoE (DeepSpeed), (ii) the PR-MoE (DeepSpeed), and (iii) the PR-MOE+MoS (DeepSpeed)."
    - **Citation:** [53] DeepSpeed Team, Rangan Majumder, and Andrey Proskurin. DeepSpeed: Accelerating large-scale model inference and training via system optimizations and compression. https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/, 2021. [Online].
    - **Explanation:** This citation highlights the use of DeepSpeed as a framework for optimizing dense model inference, providing a context for their MoE-related inference optimizations.

**5.5.4 Better Latency and Throughput Than Quality-Equivalent Dense Models**

- **Key Points:** The authors compare the inference performance of their MoE models with quality-equivalent dense models, demonstrating that DeepSpeed-MoE achieves significantly better latency and throughput compared to PyTorch for both billion-scale and trillion-scale models.
- **Significant Citations:**
    - **Claim:** "To better understand the inference performance of MoE models compared to quality-equivalent dense models, it is important to note that although MoE models are 5x faster and cheaper to train, that may not be true for inference. Inference performance has different bottlenecks and its primary factor is the amount of data read from memory instead of computation."
    - **Citation:** [53] DeepSpeed Team, Rangan Majumder, and Andrey Proskurin. DeepSpeed: Accelerating large-scale model inference and training via system optimizations and compression. https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-