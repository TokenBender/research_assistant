## Unified Scaling Laws for Routed Language Models: A Citation-Focused Analysis

This analysis focuses on the paper "Unified Scaling Laws for Routed Language Models" by Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, et al., published on arXiv in February 2022. The paper investigates the scaling behavior of Routing Networks, a type of neural architecture that conditionally uses only a subset of its parameters during processing. The authors aim to derive and justify scaling laws for these models, generalizing existing laws for standard language models. The paper cites a total of 58 references.

### 1. Introduction

The paper explores the scaling behavior of Routing Networks, architectures that conditionally use only a subset of their parameters while processing an input. The authors aim to derive and justify scaling laws for these models, generalizing those known for standard language models. They evaluate these laws across a wide range of routing architectures and training techniques, including models with hundreds of experts and hundreds of billions of parameters.

### 2. Section-by-Section Analysis with Citation Extraction

**2.1. Introduction**

- **Claim:** It is commonly believed that increasing the size of a neural network leads to better performance, especially when training on large and diverse real-world datasets.
    - **Citation:** [Kaplan et al., 2020, Hernandez et al., 2021, Henighan et al., 2020, Rosenfeld et al., 2019]
    - **Relevance:** This citation establishes the context of the paper by referencing previous work that has shown empirical evidence for scaling laws in neural networks.
- **Claim:** These relationships are not well understood, but a key implication is that a sequence of small models can be used both to infer the performance of models many times more powerful, but also to provide global information about the scalability of an architecture.
    - **Citation:** [Kaplan et al., 2020, Hernandez et al., 2021, Henighan et al., 2020, Rosenfeld et al., 2019]
    - **Relevance:** This citation highlights the practical implications of scaling laws, which are central to the paper's argument.
- **Claim:** Enter Routing Networks: models with the unusual property that each input interacts with only a subset of the network's parameters chosen independently for each datapoint.
    - **Citation:** [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]
    - **Relevance:** This citation introduces the concept of Routing Networks, which are the focus of the paper.
- **Claim:** For a Routing Network, the number of parameters is nearly independent from the computational cost of processing a datapoint.
    - **Citation:** [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]
    - **Relevance:** This citation highlights the key characteristic of Routing Networks that distinguishes them from standard models.
- **Claim:** Specific Routing Networks have been trained successfully at large scales, but the general scaling behavior is not well understood.
    - **Citation:** [Fedus et al., 2021, Du et al., 2021, Artetxe et al., 2021]
    - **Relevance:** This citation provides evidence for the growing interest in Routing Networks and the need for further research on their scaling properties.

**2.2. Background**

- **Claim:** The objective is to maximize the likelihood of a sequence of tokens P(x1,...,xT) factored auto-regressively as p(x1,...,xт) = Пр(xi|xj<i).
    - **Citation:** [Henighan et al., 2020, Kaplan et al., 2020]
    - **Relevance:** This citation defines the language modeling problem, which is the context for the paper's analysis of Routing Networks.
- **Claim:** Our primary metric of performance is the negative log-likelihood of a validation dataset whose statistics match the training distribution.
    - **Citation:** [Henighan et al., 2020, Kaplan et al., 2020]
    - **Relevance:** This citation clarifies the performance metric used in the paper, which is essential for understanding the scaling laws.
- **Claim:** Kaplan et al. [2020] argue that the converged performance of a model trained on a dataset of infinite size is a power-law in the model's parameter count N.
    - **Citation:** [Kaplan et al., 2020]
    - **Relevance:** This citation introduces the concept of power-law scaling in language models, which is a key foundation for the paper's analysis.
- **Claim:** Our dataset is not infinite, but its size – and the lack of any observed overfitting – make this a reasonable approximation.
    - **Citation:** [Rae et al., 2021]
    - **Relevance:** This citation provides context for the dataset used in the paper and justifies the use of power-law scaling as an approximation.

**2.3. Routing Networks**

- **Claim:** Power-law scaling implies the performance of a language model increases with size, but so too does the compute needed to train the model.
    - **Citation:** [Kaplan et al., 2020]
    - **Relevance:** This citation reinforces the connection between model size and computational cost, which motivates the search for architectures that decouple these two factors.
- **Claim:** Routing Networks are one such class of model: a type of neural network that incorporates a specific flavor of conditional computation.
    - **Citation:** [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]
    - **Relevance:** This citation introduces Routing Networks as a potential solution to the problem of balancing model size and computational cost.
- **Claim:** In a Routing Network, each input (e.g., a token of text) is transformed into an output while only interacting with a fixed subset of the network's parameters – dynamically selected based on the input itself.
    - **Citation:** [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]
    - **Relevance:** This citation provides a detailed description of the core mechanism of Routing Networks.

**2.4. Routing a Single Layer**

- **Claim:** The core idea of a routed layer is that multiple versions of the parameters are kept, and a per-input decision on which version to use is made.
    - **Citation:** [Shazeer et al., 2017]
    - **Relevance:** This citation introduces the concept of "experts" in Routing Networks, which are multiple versions of the same layer with different parameters.
- **Claim:** To route a layer fe in E ways, we start by creating E separate versions of the parameters 0 ({01, ...θ£}) where f using the i-th version of the parameters (fi ≡ fo₁) is termed the i-th Expert.
    - **Citation:** [Shazeer et al., 2017]
    - **Relevance:** This citation provides a detailed explanation of how a single layer is routed in a Routing Network.
- **Claim:** To determine which expert to pick given the input, we introduce an additional router function p : RM → [1, E] associated to the layer, typically a small network itself, with parameters 4.
    - **Citation:** [Shazeer et al., 2017]
    - **Relevance:** This citation introduces the concept of a "router" function, which is responsible for selecting the appropriate expert for each input.
- **Claim:** The routed form h of f is then given by h(x) = fp(x)(x).
    - **Citation:** [Shazeer et al., 2017]
    - **Relevance:** This citation defines the mathematical operation of routing, where the output of the layer is determined by the selected expert.
- **Claim:** When performance increases with E, routing gives a method by which to improve a neural network with minimal computational increase (corresponding only to the compute needed by p(x)).
    - **Citation:** [Shazeer et al., 2017]
    - **Relevance:** This citation highlights the potential benefits of routing, which is to improve performance without significantly increasing computational cost.

**2.5. Routed Transformers**

- **Claim:** We apply routing to a decoder-only Transformer [Vaswani et al., 2017] to measure the scaling properties that result: an architecture chosen due to its state-of-the-art performance.
    - **Citation:** [Vaswani et al., 2017]
    - **Relevance:** This citation specifies the architecture used in the paper, which is a decoder-only Transformer.
- **Claim:** We will refer to non-routed Transformers as dense models, in opposition to Routed Transformers which sparsely activate some of their parameters.
    - **Citation:** [Lepikhin et al., 2020, Fedus et al., 2021]
    - **Relevance:** This citation clarifies the terminology used in the paper to distinguish between dense and routed models.
- **Claim:** Our conversion to a Routed Transformer is the same as is used in prior work [Lepikhin et al., 2020, Fedus et al., 2021].
    - **Citation:** [Lepikhin et al., 2020, Fedus et al., 2021]
    - **Relevance:** This citation indicates that the authors are building upon existing work on routing Transformers.
- **Claim:** Namely, we apply routing to every other set of feedforward components (FFWs) of the Transformer, sub-components that act on each timestep independently.
    - **Citation:** [Lepikhin et al., 2020, Fedus et al., 2021]
    - **Relevance:** This citation provides a specific example of how routing is applied to the Transformer architecture.

**2.6. Model Size and Inference Cost**

- **Claim:** We use N to indicate a network's dense model size: the number of parameters any one input interacts with.
    - **Citation:** [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]
    - **Relevance:** This citation defines the concept of "dense model size," which is the number of parameters that a single input interacts with in a standard model.
- **Claim:** This is in opposition to P: the total number of parameters.
    - **Citation:** [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]
    - **Relevance:** This citation clarifies the distinction between "dense model size" and "total number of parameters," which is important for understanding the scaling behavior of Routing Networks.
- **Claim:** For a dense model, P = N, whereas for a Routing Network P is roughly proportional to NE, with factors that depend on details of the routing architecture.
    - **Citation:** [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]
    - **Relevance:** This citation highlights the key difference in parameter count between dense and routed models.
- **Claim:** Except for a small overhead due to running the routers, the cost F (in TeraFLOPs) of executing a Routed Transformer is the same as its dense equivalent.
    - **Citation:** [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]
    - **Relevance:** This citation clarifies that the computational cost of routing is negligible compared to the cost of processing the dense model.

**2.7. Training Details**

- **Claim:** All models are trained on TPUs with JAX [Bradbury et al., 2018] using a combination of data, expert (see App. C) and sharding parallelism [Shoeybi et al., 2019].
    - **Citation:** [Bradbury et al., 2018, Shoeybi et al., 2019]
    - **Relevance:** This citation provides details about the hardware and software used for training the models.
- **Claim:** Models were trained with a sequence length of 2048 and batch size of 256 for 250,000 steps, i.e. 130 billion tokens, regardless of N.
    - **Citation:** [Shoeybi et al., 2019]
    - **Relevance:** This citation provides specific details about the training hyperparameters used in the paper.
- **Claim:** All were optimized with AdamW [Loshchilov and Hutter, 2018] and ZeRO Stage 1 was used to shard the optimizer state [Rajbhandari et al., 2020].
    - **Citation:** [Loshchilov and Hutter, 2018, Rajbhandari et al., 2020]
    - **Relevance:** This citation provides details about the optimization algorithm and techniques used for training the models.

### 3. Key Insights and Supporting Literature

- **Insight:** Routing improves the performance of language models across all sizes and variants attempted.
    - **Citation:** [Bengio et al., 2013, 2016, Denoyer and Gallinari, 2014, Shazeer et al., 2017, Lepikhin et al., 2020, Fedus et al., 2021, Du et al., 2021, Artetxe et al., 2021, Roller et al., 2021, Lewis et al., 2021]
    - **Relevance:** This insight is supported by a wide range of cited works, demonstrating the consistent benefits of routing across different architectures and training techniques.
- **Insight:** The performance of all Routing Networks is accurately described by scaling laws in the number of experts and in the underlying dense model size.
    - **Citation:** [Kaplan et al., 2020]
    - **Relevance:** This insight builds upon the existing work on scaling laws for dense models, extending them to Routing Networks.
- **Insight:** These laws can be restated in terms of parameter count and inference compute, capturing an even wider set of routing architectures under a shared fit.
    - **Citation:** [Kaplan et al., 2020]
    - **Relevance:** This insight demonstrates the generality of the scaling laws, showing that they apply across different representations of model size and computational cost.
- **Insight:** They further imply an Effective Parameter Count: a mapping equating the performance and scaling for both dense and routed networks.
    - **Citation:** [Kaplan et al., 2020]
    - **Relevance:** This insight introduces the concept of "Effective Parameter Count," which allows for a direct comparison of the performance of dense and routed models.

### 4. Experimental Methodology and Its Foundations

The authors evaluate Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters. They train these models using three different techniques: Sinkhorn-BASE, HASH Layers, and Routing via Reinforcement Learning (RL-R). The authors use TPUs with JAX for training and employ a combination of data, expert, and sharding parallelism.

- **Methodology:** The authors use a decoder-only Transformer architecture [Vaswani et al., 2017] as the basis for their Routing Networks.
    - **Citation:** [Vaswani et al., 2017]
    - **Relevance:** This citation provides the foundation for the architecture used in the paper.
- **Methodology:** The authors use the Sinkhorn algorithm [Cuturi, 2013] for rebalancing expert selections in the Sinkhorn-BASE method.
    - **Citation:** [Cuturi, 2013]
    - **Relevance:** This citation justifies the use of the Sinkhorn algorithm as a more efficient alternative to the Hungarian Matching algorithm used in previous work.
- **Methodology:** The authors use the SentencePiece tokenizer [Kudo and Richardson, 2018] for tokenizing the input text.
    - **Citation:** [Kudo and Richardson, 2018]
    - **Relevance:** This citation provides details about the tokenization process used in the paper.
- **Methodology:** The authors use AdamW [Loshchilov and Hutter, 2018] for optimizing the model parameters.
    - **Citation:** [Loshchilov and Hutter, 2018]
    - **Relevance:** This citation justifies the use of AdamW as the optimization algorithm.
- **Methodology:** The authors use ZeRO Stage 1 [Rajbhandari et al., 2020] for sharding the optimizer state.
    - **Citation:** [Rajbhandari et al., 2020]
    - **Relevance:** This citation justifies the use of ZeRO Stage 1 for efficient training on large models.

### 5. Results in Context

- **Result:** Routing improves the performance of language models across all sizes and variants attempted.
    - **Citation:** [Bengio et al., 2013, 2016, Denoyer and Gallinari, 2014, Shazeer et al., 2017, Lepikhin et al., 2020, Fedus et al., 2021, Du et al., 2021, Artetxe et al., 2021, Roller et al., 2021, Lewis et al., 2021]
    - **Relevance:** This result confirms the findings of previous work on Routing Networks, demonstrating the consistent benefits of routing.
- **Result:** The performance of all Routing Networks is accurately described by scaling laws in the number of experts and in the underlying dense model size.
    - **Citation:** [Kaplan et al., 2020]
    - **Relevance:** This result extends the existing work on scaling laws for dense models, showing that they apply to Routing Networks.
- **Result:** These laws can be restated in terms of parameter count and inference compute, capturing an even wider set of routing architectures under a shared fit.
    - **Citation:** [Kaplan et al., 2020]
    - **Relevance:** This result demonstrates the generality of the scaling laws, showing that they apply across different representations of model size and computational cost.
- **Result:** They further imply an Effective Parameter Count: a mapping equating the performance and scaling for both dense and routed networks.
    - **Citation:** [Kaplan et al., 2020]
    - **Relevance:** This result introduces the concept of "Effective Parameter Count," which allows for a direct comparison of the performance of dense and routed models.

### 6. Discussion and Related Work

The authors discuss their work in the context of existing literature on scaling laws for language models, Routing Networks, and Mixture of Experts. They highlight the novelty of their work in deriving scaling laws for Routing Networks that generalize existing laws for dense models. They also emphasize the importance of their findings for understanding the trade-offs between model size and computational cost in Routing Networks.

- **Citation:** [Kaplan et al., 2020, Henighan et al., 2020, Hernandez et al., 2021, Ghorbani et al., 2021, Hutter, 2021, Bahri et al., 2021]
    - **Relevance:** These citations provide context for the paper's contribution to the field of scaling laws for language models.
- **Citation:** [Radford et al., 2019, Brown et al., 2020, Lieber et al., 2021, Rae et al., 2021, Shoeybi et al., 2019, Narayanan et al., 2019, Kim et al., 2021, Xu et al., 2021]
    - **Relevance:** These citations highlight the progress made in scaling Transformers, which is the foundation for the paper's work on Routing Networks.
- **Citation:** [Bengio et al., 2013, 2016, Bengio, 2017, Denoyer and Gallinari, 2014, Jacobs et al., 1991, Collobert et al., 2003, Eigen et al., 2014, Ramachandran and Le, 2018, Rosenbaum et al., 2018, Shazeer et al., 2017, Lepikhin et al., 2020, Fedus et al., 2021, Du et al., 2021, Artetxe et al., 2021, Nie et al., 2021, Hazimeh et al., 2021, Ramachandran and Le, 2018, Caccia et al., 2021, Rajbhandari et al., 2022]
    - **Relevance:** These citations provide a comprehensive overview of the literature on Routing Networks and Mixture of Experts, highlighting the evolution of the field and the paper's contribution to it.

### 7. Future Work and Open Questions

The authors suggest several areas for future research, including:

- **Future Work:** Exploring the limit behavior of N and E, especially arriving at a more precise value of b.
    - **Relevance:** This suggestion is motivated by the limitations of the current study, which was constrained by computational resources.
- **Future Work:** Developing new routing techniques with lower scaling coefficients c and higher Emax.
    - **Relevance:** This suggestion is motivated by the observation that the scaling coefficient c limits the benefits of routing at large scales.
- **Future Work:** Validating new routing techniques at multiple values of N and E when comparing with prior work.
    - **Relevance:** This suggestion emphasizes the importance of rigorous evaluation of new routing techniques.

### 8. Critical Analysis of Citation Usage

The authors effectively use citations to support their arguments and findings. They provide a comprehensive overview of the relevant literature, citing both foundational works and recent advances in the field. The authors also use citations to highlight the novelty and importance of their own work.

- **Potential Bias:** The authors primarily cite works from DeepMind and Google Research, which may reflect a bias towards their own institution.
- **Potential Improvement:** The authors could have provided more citations to work on the theoretical foundations of scaling laws in neural networks.

### 9. Final Summary

The paper makes a significant contribution to the field of deep learning by deriving and justifying scaling laws for Routing Networks. The authors demonstrate that these laws generalize existing laws for standard language models and provide a framework for understanding the trade-offs between model size and computational cost in Routing Networks. The paper effectively integrates existing literature to support its claims and findings, providing a valuable resource for researchers working in this area.

The most influential or frequently cited works used throughout the paper include:

- [Kaplan et al., 2020]: This paper introduces the concept of scaling laws for language models, which is a key foundation for the paper's analysis.
- [Bengio et al., 2013, 2016, Denoyer and Gallinari, 2014]: These papers introduce the concept of Routing Networks and provide a foundation for the paper's work.
- [Shazeer et al., 2017]: This paper introduces the concept of "experts" in Routing Networks, which is a key component of the paper's analysis.

Overall, the paper provides a strong foundation for understanding the scaling behavior of Routing Networks and its implications for the future of deep learning. The authors' comprehensive analysis and clear presentation of their findings make this paper a valuable resource for researchers working in this area.
