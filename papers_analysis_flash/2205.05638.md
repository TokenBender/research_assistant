## Analysis of "Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning"

**1. Introduction**

- **Title:** Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning
- **Authors:** Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, Colin Raffel
- **Publication Date:** 26 August 2022 (v2)
- **Objective:** The paper aims to rigorously compare few-shot in-context learning (ICL) and parameter-efficient fine-tuning (PEFT) methods for adapting pre-trained language models to new tasks with limited labeled data.
- **References:** 81

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - Pre-trained language models (PLMs) have become essential for natural language processing due to their ability to improve data efficiency on tasks.
    - Fine-tuning PLMs on downstream tasks often leads to specialized models that are impractical for multi-task scenarios.
    - In-context learning (ICL) offers an alternative by prompting the model with a few examples, but it incurs significant computational and memory costs.
    - Parameter-efficient fine-tuning (PEFT) methods, which update only a small subset of parameters, provide a more efficient alternative.
- **Citations:**
    - **Claim:** "Fine-tuning has produced many state-of-the-art results [1], it results in a model that is specialized for a single task with an entirely new set of parameter values, which can become impractical when fine-tuning a model on many downstream tasks."
        - **Citation:** Sanh et al., 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.
        - **Relevance:** This citation introduces the concept of fine-tuning PLMs and highlights its limitations for multi-task scenarios.
    - **Claim:** "An alternative approach popularized by [3, 4] is in-context learning (ICL), which induces a model to perform a downstream task by inputting prompted examples."
        - **Citation:**
            - Radford et al., 2019. Language models are unsupervised multitask learners. OpenAI blog.
            - Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
        - **Relevance:** This citation introduces ICL and its reliance on prompting the model with examples.
    - **Claim:** "Performing ICL therefore solely relies on the capabilities that a model learned during pre-training."
        - **Citation:**  Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
        - **Relevance:** This citation emphasizes that ICL relies on the pre-trained knowledge of the model, without any gradient-based updates.
    - **Claim:** "These characteristics have led to a great deal of recent interest in ICL methods [5-10]."
        - **Citation:**
            - Chen et al., 2021. Meta-learning via language model in-context tuning. arXiv preprint arXiv:2110.07814.
            - Min et al., 2021. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943.
            - Lampinen et al., 2022. Can language models learn from explanations in context? ArXiv, abs/2204.02329.
            - Lazaridou et al., 2022. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115.
            - Min et al., 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.
            - Wang et al., 2022. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705.
        - **Relevance:** This citation highlights the growing interest in ICL research due to its potential benefits.

**2.2 Background**

- **Key Points:**
    - The paper discusses the computational, memory, and storage costs associated with ICL and PEFT methods.
    - ICL incurs significant costs due to processing all prompted examples for each prediction.
    - PEFT methods offer a more efficient alternative by updating only a small subset of parameters.
- **Citations:**
    - **Claim:** "Despite the practical benefits of ICL, it has several major drawbacks. First, processing all prompted input-target pairs every time the model makes a prediction incurs significant compute costs."
        - **Citation:** Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
        - **Relevance:** This citation highlights the computational cost associated with ICL.
    - **Claim:** "Second, ICL typically produces inferior performance compared to fine-tuning [4]."
        - **Citation:** Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
        - **Relevance:** This citation points out the accuracy limitations of ICL compared to fine-tuning.
    - **Claim:** "Finally, the exact formatting of the prompt (including the wording [11] and ordering of examples [12]) can have significant and unpredictable impact on the model's performance, far beyond inter-run variation of fine-tuning."
        - **Citation:**
            - Webson et al., 2021. Do prompt-based models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247.
            - Zhao et al., 2021. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690.
        - **Relevance:** This citation highlights the sensitivity of ICL to prompt formatting and ordering.
    - **Claim:** "Recent work has also demonstrated that ICL can perform well even when provided with incorrect labels, raising questions as to how much learning is taking place at all [9]."
        - **Citation:** Min et al., 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.
        - **Relevance:** This citation raises concerns about the actual learning happening in ICL, as it can perform well even with incorrect labels.
    - **Claim:** "Recent methods have matched the performance of fine-tuning the full model while only updating or adding a small fraction (e.g. 0.01%) of the full model's parameters [13, 14]."
        - **Citation:**
            - Hu et al., 2021. LoRA: Low-rank adaptation of large language models. ArXiv, abs/2106.09685.
            - Lester et al., 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.
        - **Relevance:** This citation introduces PEFT methods and their ability to achieve comparable performance to full fine-tuning with significantly fewer parameters.

**2.3 Few-shot in-context learning (ICL)**

- **Key Points:**
    - ICL aims to induce a model to perform a task by feeding in concatenated and prompted input-target examples (shots).
    - The paper discusses the computational, memory, and storage costs associated with ICL.
    - ICL exhibits unintuitive behavior, as the ordering of examples and even incorrect labels can influence the model's predictions.
- **Citations:**
    - **Claim:** "ICL [3, 4] aims to induce a model to perform a task by feeding in concatenated and prompted input-target examples (called "shots") along with an unlabeled query example."
        - **Citation:**
            - Radford et al., 2019. Language models are unsupervised multitask learners. OpenAI blog.
            - Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
        - **Relevance:** This citation defines ICL and its reliance on prompting with examples.
    - **Claim:** "Despite these advantages, ICL comes with significant practical drawbacks: First, making a prediction is dramatically more expensive because the model needs to process all of the in-context labeled examples."
        - **Citation:** Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
        - **Relevance:** This citation highlights the computational cost of ICL due to processing all examples.
    - **Claim:** "Beyond the aforementioned costs, ICL also exhibits unintuitive behavior. Zhao et al. [12] showed that the ordering of examples in the context heavily influences the model's predictions."
        - **Citation:** Zhao et al., 2021. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690.
        - **Relevance:** This citation points out the sensitivity of ICL to the order of examples.
    - **Claim:** "Min et al. [9] showed that ICL can still perform well even if the labels of the in-context examples are swapped (i.e. made incorrect), raising questions about whether ICL is really “learning" from the labeled examples."
        - **Citation:** Min et al., 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.
        - **Relevance:** This citation raises concerns about the actual learning happening in ICL, as it can perform well even with incorrect labels.

**2.4 Parameter-efficient fine-tuning**

- **Key Points:**
    - PEFT methods update or add a small number of parameters to adapt a pre-trained model to a new task.
    - The paper discusses various PEFT methods, including adapters, low-rank updates, and prompt tuning.
    - PEFT methods offer significant advantages in terms of memory and storage requirements, and they can be used for mixed-task batches.
- **Citations:**
    - **Claim:** "While standard fine-tuning updates all parameters of the pre-trained model, it has been demonstrated that it is possible to instead update or add a relatively small number of parameters."
        - **Citation:**  Hu et al., 2021. LoRA: Low-rank adaptation of large language models. ArXiv, abs/2106.09685.
        - **Relevance:** This citation introduces the concept of PEFT methods.
    - **Claim:** "Early methods proposed adding adapters [22–24], which are small trainable feed-forward networks inserted between the layers in the fixed pre-trained model."
        - **Citation:**
            -  Rebuffi et al., 2017. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30.
            -  Houlsby et al., 2019. Parameter-efficient transfer learning for NLP. arXiv preprint arXiv:1902.00751.
            -  Bapna et al., 2019. Simple, scalable adaptation for neural machine translation. arXiv preprint arXiv:1909.08478.
        - **Relevance:** This citation introduces the concept of adapters as a PEFT method.
    - **Claim:** "Since then, various sophisticated PEFT methods have been proposed, including methods that choose a sparse subset of parameters to train [25, 26], produce low-rank updates [13], perform optimization in a lower-dimensional subspace [27], add low-rank adapters using hypercomplex multiplication [28], and more."
        - **Citation:**
            - Guo et al., 2020. Parameter-efficient transfer learning with diff pruning. arXiv preprint arXiv:2012.07463.
            - Sung et al., 2021. Training neural networks with fixed sparse masks. arXiv preprint arXiv:2111.09839.
            - Hu et al., 2021. LoRA: Low-rank adaptation of large language models. ArXiv, abs/2106.09685.
            - Aghajanyan et al., 2020. Intrinsic dimensionality explains the effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255.
            - Mahabadi et al., 2021. Compacter: Efficient low-rank hypercomplex adapter layers. arXiv preprint arXiv:2106.04647.
        - **Relevance:** This citation lists various PEFT methods and their key features.
    - **Claim:** "Relatedly, prompt tuning [14] and prefix tuning [29] concatenate learned continuous embeddings to the model's input or activations to induce it to perform a task; this can be seen as a PEFT method [30]."
        - **Citation:**
            - Lester et al., 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.
            - Li et al., 2021. Prefix-Tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.
            - He et al., 2021. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366.
        - **Relevance:** This citation introduces prompt tuning and prefix tuning as PEFT methods.
    - **Claim:** "PEFT drastically reduces the memory and storage requirements for training and saving the model."
        - **Citation:** Hu et al., 2021. LoRA: Low-rank adaptation of large language models. ArXiv, abs/2106.09685.
        - **Relevance:** This citation highlights the memory and storage efficiency of PEFT methods.

**2.5 Designing the T-Few Recipe**

- **Key Points:**
    - The paper proposes a recipe called "T-Few" for achieving high accuracy on new tasks with limited labeled data using PEFT.
    - T-Few is based on the TO model, a variant of T5 fine-tuned on a multitask mixture of prompted datasets.
    - The paper introduces a new PEFT method called (IA)³ that scales activations by learned vectors.
    - T-Few also incorporates unlikelihood and length normalization loss terms to improve performance.
- **Citations:**
    - **Claim:** "Given that PEFT allows a model to be adapted to a new task with relatively small storage requirements and computational cost, we argue that PEFT presents a promising alternative to ICL."
        - **Citation:**  Hu et al., 2021. LoRA: Low-rank adaptation of large language models. ArXiv, abs/2106.09685.
        - **Relevance:** This citation highlights the advantages of PEFT over ICL.
    - **Claim:** "Our goal is therefore to develop a recipe that allows a model to attain high accuracy on new tasks with limited labeled examples while allowing mixed-task batches during inference and incurring minimal computational and storage costs."
        - **Citation:**
            - Perez et al., 2021. True few-shot learning with language models. arXiv preprint arXiv:2105.11447.
            - Oliver et al., 2018. Realistic evaluation of deep semi-supervised learning algorithms. Advances in Neural Information Processing Systems.
        - **Relevance:** This citation emphasizes the importance of developing a practical recipe for few-shot learning.
    - **Claim:** "TO was created by fine-tuning T5 on a multitask mixture of datasets in order to enable zero-shot generalization, i.e. the ability to perform tasks without any additional gradient-based training."
        - **Citation:**
            - Sanh et al., 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.
            - Raffel et al., 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683.
        - **Relevance:** This citation introduces the TO model and its zero-shot generalization capabilities.
    - **Claim:** "To improve performance on classification and multiple-choice tasks, we add unlikelihood [16, 17] and length normalization-based [4] loss terms."
        - **Citation:**
            - Tam et al., 2021. Improving and simplifying pattern exploiting training. arXiv preprint arXiv:2103.11955.
            - Welleck et al., 2019. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319.
            - Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
        - **Relevance:** This citation introduces the use of unlikelihood and length normalization loss terms in T-Few.

**2.6 Model and Datasets**

- **Key Points:**
    - The paper uses the TO model (specifically T0-3B) as the backbone for T-Few.
    - The paper evaluates T-Few on a set of held-out tasks from the TO training mixture and on the RAFT benchmark.
- **Citations:**
    - **Claim:** "As a first step, we must choose a pre-trained model. Ideally, the model should attain high performance on new tasks after fine-tuning on a limited number of labeled examples."
        - **Citation:**
            - Perez et al., 2021. True few-shot learning with language models. arXiv preprint arXiv:2105.11447.
            - Oliver et al., 2018. Realistic evaluation of deep semi-supervised learning algorithms. Advances in Neural Information Processing Systems.
        - **Relevance:** This citation highlights the importance of choosing a suitable pre-trained model for few-shot learning.
    - **Claim:** "TO was released in three billion and eleven billion parameter variants, referred to as “T0-3B” and simply “TO” respectively."
        - **Citation:** Sanh et al., 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.
        - **Relevance:** This citation provides details about the TO model variants.
    - **Claim:** "To ease comparison, we use the same number of few-shot training examples for each dataset as Brown et al. [4], which varies from 20 to 70."
        - **Citation:** Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
        - **Relevance:** This citation explains the choice of using the same number of examples as a previous study for comparison.

**2.7 Unlikelihood Training and Length Normalization**

- **Key Points:**
    - The paper explores the use of unlikelihood and length normalization loss terms to improve the performance of few-shot fine-tuning.
    - Unlikelihood loss discourages the model from predicting incorrect target sequences.
    - Length normalization loss accounts for the length of different answer choices during evaluation.
- **Citations:**
    - **Claim:** "For evaluation, we use rank classification (described in section 3.1) which depends on both the probability that the model assigns to the correct choice as well as the probabilities assigned by the model to the incorrect choices."
        - **Citation:**  Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
        - **Relevance:** This citation explains the evaluation metric used in the paper.
    - **Claim:** "To account for this during training, we consider adding an unlikelihood loss [16, 17]:"
        - **Citation:**
            - Tam et al., 2021. Improving and simplifying pattern exploiting training. arXiv preprint arXiv:2103.11955.
            - Welleck et al., 2019. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319.
        - **Relevance:** This citation introduces the concept of unlikelihood loss.
    - **Claim:** "We hypothesize that adding LUL will improve results on rank classification because the model will be trained to assign lower probabilities to incorrect choices, thereby improving the chance that the correct choice is ranked highest."
        - **Citation:**  Welleck et al., 2019. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319.
        - **Relevance:** This citation explains the rationale behind using unlikelihood loss.
    - **Claim:** "To rectify this, we consider using length normalization when performing rank classification, which divides the model's score on each possible answer choice by the number of tokens in the choice (as used in GPT-3 [4])."
        - **Citation:** Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
        - **Relevance:** This citation introduces the concept of length normalization.

**2.8 Parameter-efficient fine-tuning with (IA)³**

- **Key Points:**
    - The paper introduces a new PEFT method called (IA)³ that scales activations by learned vectors.
    - (IA)³ introduces a small number of parameters and allows for mixed-task batches.
    - The paper compares (IA)³ to other PEFT methods and finds that it outperforms full fine-tuning.
- **Citations:**
    - **Claim:** "As an alternative, we explored element-wise multiplication (i.e. rescaling) of the model's activations against a learned vector."
        - **Citation:**  Vaswani et al., 2017. Attention is all you need. Advances in Neural Information Processing Systems.
        - **Relevance:** This citation introduces the concept of element-wise multiplication of activations.
    - **Claim:** "We use “broadcasting notation” [46] so that the (i, j)th entry of l⊙x is ljxi,j."
        - **Citation:** Van Der Walt et al., 2011. The numpy array: a structure for efficient numerical computation. Computing in science & engineering, 13(2).
        - **Relevance:** This citation explains the broadcasting notation used in the paper.
    - **Claim:** "To validate (IA)³, we compare it to a large variety of existing adaptation methods in our setting of fine-tuning T0-3B on few-shot datasets from held-out tasks."
        - **Citation:**
            - Sanh et al., 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.
            - Raffel et al., 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683.
        - **Relevance:** This citation explains the experimental setup for comparing (IA)³ to other PEFT methods.
    - **Claim:** "We hypothesize the disagreement comes from us using a different model and different datasets."
        - **Citation:**
            - Mahabadi et al., 2021. Compacter: Efficient low-rank hypercomplex adapter layers. arXiv preprint arXiv:2106.04647.
            - Lester et al., 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.
            - Wei et al., 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.
        - **Relevance:** This citation acknowledges the potential impact of model and dataset choices on the results.

**2.9 Pre-training (IA)³**

- **Key Points:**
    - The paper explores pre-training the (IA)³ parameters on the same multitask mixture used to train TO.
    - Pre-training (IA)³ improves the performance of T-Few.
- **Citations:**
    - **Claim:** "In recent work, Gu et al. [18], Vu et al. [19] showed that pre-training the prompt embeddings in prompt tuning can improve performance when fine-tuning on downstream few-shot tasks."
        - **Citation:**
            - Gu et al., 2021. PPT: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332.
            - Vu et al., 2021. SPOT: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904.
        - **Relevance:** This citation introduces the concept of pre-training prompt embeddings.
    - **Claim:** "We follow Vu et al. [19] and simply pre-train the new parameters introduced by (IA)³ on the same multitask mixture used to train TO."
        - **Citation:** Vu et al., 2021. SPOT: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904.
        - **Relevance:** This citation explains the pre-training strategy used in the paper.

**2.10 Combining the Ingredients**

- **Key Points:**
    - The paper summarizes the T-Few recipe, which includes the TO model, (IA)³, pre-trained (IA)³, unlikelihood and length normalization loss terms, and a specific training schedule.
    - T-Few is designed to be a realistic option for few-shot learning settings where limited labeled data is available.
- **Citations:**
    - **Claim:** "In summary, the T-Few recipe is defined as follows: We use the T0 model as a backbone. We add (IA)³ for downstream task adaptation and use parameters initialized from pre-training (IA)³ on the same multitask mixture for TO."
        - **Citation:**
            - Sanh et al., 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.
            - Raffel et al., 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv, abs/1910.10683.
        - **Relevance:** This citation summarizes the key components of the T-Few recipe.
    - **Claim:** "Importantly, we apply this recipe to every downstream dataset in exactly the same way without per-dataset hyperparameter tuning or modifications."
        - **Citation:**
            - Perez et al., 2021. True few-shot learning with language models. arXiv preprint arXiv:2105.11447.
            - Oliver et al., 2018. Realistic evaluation of deep semi-supervised learning algorithms. Advances in Neural Information Processing Systems.
        - **Relevance:** This citation emphasizes the importance of using a consistent recipe across different tasks.

**3. Key Insights and Supporting Literature**

- **Key Insight 1:** Parameter-efficient fine-tuning (PEFT) methods can outperform in-context learning (ICL) for few-shot adaptation of pre-trained language models.
    - **Supporting Citations:**
        - Hu et al., 2021. LoRA: Low-rank adaptation of large language models. ArXiv, abs/2106.09685.
        - Lester et al., 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.
        - Sanh et al., 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.
        - Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
    - **Explanation:** The authors demonstrate that their proposed T-Few recipe, which utilizes PEFT, achieves higher accuracy than ICL on various tasks, including those from the RAFT benchmark.
- **Key Insight 2:** The (IA)³ PEFT method, which scales activations by learned vectors, outperforms other PEFT methods and even full fine-tuning.
    - **Supporting Citations:**
        - Vaswani et al., 2017. Attention is all you need. Advances in Neural Information Processing Systems.
        - Mahabadi et al., 2021. Compacter: Efficient low-rank hypercomplex adapter layers. arXiv preprint arXiv:2106.04647.
        - Lester et al., 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.
        - Wei et al., 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.
    - **Explanation:** The authors compare (IA)³ to other PEFT methods, including adapters, prompt tuning, and low-rank updates, and show that (IA)³ consistently achieves the best performance.
- **Key Insight 3:** Pre-training the (IA)³ parameters on the same multitask mixture used to train TO further improves the performance of T-Few.
    - **Supporting Citations:**
        - Gu et al., 2021. PPT: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332.
        - Vu et al., 2021. SPOT: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904.
    - **Explanation:** The authors demonstrate that pre-training (IA)³ leads to a significant improvement in accuracy compared to not pre-training.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The paper uses the T0-3B and TO models as baselines for comparison.
    - The paper evaluates T-Few on a set of held-out tasks from the TO training mixture and on the RAFT benchmark.
    - The paper compares T-Few to various ICL and PEFT methods, including full fine-tuning, BitFit, adapters, prompt tuning, prefix tuning, FishMask, Intrinsic SAID, and LoRA.
    - The paper uses rank classification as the evaluation metric.
    - The paper reports median accuracy across all prompt templates and few-shot data subsets.
- **Foundations:**
    - The paper builds upon previous work on ICL and PEFT methods, citing relevant papers for each approach.
    - The paper uses the TO model, which was specifically designed for zero-shot generalization, as a starting point for T-Few.
- **Novel Aspects:**
    - The paper introduces the (IA)³ PEFT method, which is a novel approach to scaling activations by learned vectors.
    - The paper proposes a simple recipe called T-Few that can be applied to new tasks without task-specific tuning or modifications.
    - The paper validates the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark.
    - The authors cite previous work on pre-training prompt embeddings [18, 19] to justify their approach of pre-training (IA)³.

**5. Results in Context**

- **Main Results:**
    - T-Few outperforms ICL (even against 16× larger models) and achieves super-human performance on the RAFT benchmark.
    - T-Few requires dramatically less compute and allows for mixed-task batches during inference.
    - (IA)³ consistently outperforms other PEFT methods and even full fine-tuning.
    - Pre-training (IA)³ further improves the performance of T-Few.
- **Comparison with Existing Literature:**
    - The authors compare T-Few to strong ICL baselines, including T5+LM and GPT-3 variants.
    - The authors compare (IA)³ to other PEFT methods, including adapters, prompt tuning, prefix tuning, FishMask, Intrinsic SAID, and LoRA.
    - The authors cite previous work on pre-training prompt embeddings [18, 19] to contextualize their findings.
- **Confirmation, Contradiction, or Extension:**
    - The authors' results confirm the findings of previous work on the limitations of ICL for few-shot learning [10].
    - The authors' results extend previous work on pre-training prompt embeddings [18, 19] by demonstrating the effectiveness of pre-training (IA)³.
    - The authors' results contradict previous findings that Compacter and Compacter++ outperform full fine-tuning [28] and that prompt tuning can match full fine-tuning [14, 48].

**6. Discussion and Related Work**

- **Situating the Work:**
    - The authors situate their work within the broader context of research on few-shot learning, ICL, and PEFT methods.
    - The authors highlight the limitations of ICL and the advantages of PEFT for few-shot learning.
    - The authors emphasize the importance of developing practical recipes for few-shot learning.
- **Key Papers Cited:**
    - Sanh et al., 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.
    - Brown et al., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
    - Hu et al., 2021. LoRA: Low-rank adaptation of large language models. ArXiv, abs/2106.09685.
    - Lester et al., 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.
    - Perez et al., 2021. True few-shot learning with language models. arXiv preprint arXiv:2105.11447.
    - Oliver et al., 2018. Realistic evaluation of deep semi-supervised learning algorithms. Advances in Neural Information Processing Systems.
    - Mahabadi et al., 2021. Compacter: Efficient low-rank hypercomplex adapter layers. arXiv preprint arXiv:2106.04647.
    - Gu et al., 2021. PPT: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332.
    - Vu et al., 2021. SPOT: Better frozen model adaptation through soft prompt transfer. arXiv preprint arXiv:2110.07904.
- **Novelty and Importance:**
    - The authors highlight the novelty of their (IA)³ PEFT method and its ability to outperform other PEFT methods and even full fine-tuning.
    - The authors emphasize the importance of their T-Few recipe for achieving high accuracy on new tasks with limited labeled data.

**7. Future Work and Open Questions**

- **Areas for Further Research:**
    - Applying T-Few to generative tasks, such as summarization and question answering.
    - Exploring the use of T-Few for other NLP tasks, such as machine translation and text classification.
    - Investigating the potential of (IA)³ for other PEFT methods, such as adapters and prompt tuning.
    - Analyzing the impact of different pre-training strategies on the performance of (IA)³.
- **Citations:**
    - The authors do not explicitly cite any papers to support their suggestions for future work.

**8. Critical Analysis of Citation Usage**

- **Effectiveness:**
    - The authors effectively use citations to support their arguments and findings.
    - The citations are relevant and up-to-date, providing a strong foundation for the paper's claims.
