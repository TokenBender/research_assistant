## Analysis of "GLM-130B: An Open Bilingual Pre-Trained Model"

**1. Introduction:**

- **Title:** GLM-130B: An Open Bilingual Pre-Trained Model
- **Authors:** Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang
- **Publication Date:** Published as a conference paper at ICLR 2023
- **Objective:** The paper introduces GLM-130B, a 130 billion parameter bilingual (English and Chinese) language model, and details its open-source training process, design choices, and engineering efforts. The authors aim to demonstrate that a 100B-scale model can be successfully pre-trained and open-sourced, offering a valuable resource for research and development.
- **References:** The paper cites a total of 138 references.

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:** The introduction highlights the emergence of large language models (LLMs) with over 100 billion parameters and their impressive scaling laws. The authors emphasize the significance of GPT-3 as a pioneer in this field, but also point out the lack of transparency in its training process and model architecture. They argue for the importance of open-sourcing high-quality LLMs and discuss the challenges associated with training models at this scale, particularly in terms of efficiency, stability, and convergence.
- **Citations:**
    - **Claim:** "Large language models (LLMs), particularly those with over 100 billion (100B) parameters (Brown et al., 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022; Wang et al., 2021), have presented attractive scaling laws (Wei et al., 2022b), where emergent zero-shot and few-shot capabilities suddenly arose."
    - **Citation:** Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems*, *33*, 1877-1901.
    - **Relevance:** This citation introduces GPT-3, a seminal work in the field of large language models, and highlights its significant contribution to the study of scaling laws.
    - **Claim:** "However, both GPT-3 (and many other closed-sourced 100B-scale ones)—the model itself and how it can be trained, have been thus far intransparent to the public."
    - **Citation:**  None.
    - **Relevance:** This claim emphasizes the lack of transparency surrounding the training process and model architecture of GPT-3 and other closed-sourced LLMs, motivating the authors' decision to open-source GLM-130B.
    - **Claim:** "We thus aim to pre-train an open and highly-accurate 100B-scale model with ethical concerns in mind."
    - **Citation:** None.
    - **Relevance:** This statement outlines the paper's main objective: to develop and open-source a high-quality, 100B-scale LLM.
    - **Claim:** "Similar difficulties have also been concurrently observed in training OPT-175B (Zhang et al., 2022) and BLOOM-176B (Scao et al., 2022), further demonstrating the significance of GPT-3 as a pioneer study."
    - **Citation:** Zhang, S., Zhou, Y., Dai, Z., ... & LeCun, Y. (2022). OPT: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*.
    - **Relevance:** This citation acknowledges the challenges faced by other researchers in training large-scale LLMs, further emphasizing the significance of the authors' work in open-sourcing GLM-130B.

**2.2 Introduction:**

- **Key Points:** This section introduces the GLM-130B model, highlighting its bilingual nature, 130 billion parameters, and its pre-training on 400 billion tokens. The authors emphasize the model's outperformance over GPT-3 175B on various benchmarks, particularly in English, while also noting its consistent and significant outperformance over ERNIE TITAN 3.0 260B in Chinese. The section concludes by discussing the model's unique scaling property that allows for INT4 quantization without post-training, enabling efficient inference on affordable GPUs.
- **Citations:**
    - **Claim:** "We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters."
    - **Citation:** None.
    - **Relevance:** This statement introduces the GLM-130B model, a key focus of the paper.
    - **Claim:** "It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained."
    - **Citation:** Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems*, *33*, 1877-1901.
    - **Relevance:** This claim highlights the authors' ambition to open-source a high-quality, 100B-scale model, drawing a comparison to GPT-3 (davinci) as a benchmark.
    - **Claim:** "The resultant GLM-130B model offers significant outperformance over GPT-3 175B (davinci) on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B."
    - **Citation:** Zhang, S., Zhou, Y., Dai, Z., ... & LeCun, Y. (2022). OPT: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*.
    - **Relevance:** This claim highlights the model's performance advantage over GPT-3 175B on English benchmarks, while also acknowledging that this advantage is not observed in OPT-175B and BLOOM-176B.
    - **Claim:** "It also consistently and significantly outperforms ERNIE TITAN 3.0 260B—the largest Chinese language model across related benchmarks."
    - **Citation:** Wang, S., Sun, Y., Xiang, Y., ... & Zhou, M. (2021). ERNIE 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language understanding and generation. *arXiv preprint arXiv:2112.12731*.
    - **Relevance:** This claim emphasizes the model's strong performance in Chinese, outperforming the largest existing Chinese language model, ERNIE TITAN 3.0 260B.
    - **Claim:** "Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization without post training, with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most affordable GPUs required for using 100B-scale models."
    - **Citation:** None.
    - **Relevance:** This claim highlights the model's unique scaling property that enables INT4 quantization without post-training, making it more efficient and accessible for inference on affordable GPUs.

**2.3 The Design Choices of GLM-130B:**

- **Key Points:** This section delves into the design choices behind GLM-130B, focusing on its architecture, layer normalization, positional encoding, and pre-training setup. The authors explain their decision to use the GLM algorithm as the backbone for the model, highlighting its advantages in terms of bidirectional attention and autoregressive blank infilling. They discuss the challenges associated with training stability at this scale and present their solution using DeepNorm, a novel layer normalization technique. The section also details the use of Rotary Positional Encoding (RoPE) and GLU with GeLU activation for positional encoding and feed-forward networks, respectively.
- **Citations:**
    - **Claim:** "Instead of using the GPT-style architecture, we adopt the General Language Model (GLM) algorithm (Du et al., 2022) to leverage its bidirectional attention advantage and autoregressive blank infilling objective."
    - **Citation:** Du, Z., Qian, Y., Liu, X., ... & Tang, J. (2022). GLM: General language model pretraining with autoregressive blank infilling. *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, *1*, 320-335.
    - **Relevance:** This citation introduces the GLM algorithm, which serves as the backbone for GLM-130B, and highlights its key advantages.
    - **Claim:** "Our search is later focused on Post-LN due to its favorable downstream results in preliminary experiments though it does not stabilize GLM-130B. Fortunately, one of the attempts on Post-LN initialized with the newly-proposed DeepNorm (Wang et al., 2022b) generates promising training stability."
    - **Citation:** Wang, H., Ma, S., Dong, L., ... & Wei, F. (2022b). Deepnet: Scaling transformers to 1,000 layers. *arXiv preprint arXiv:2203.00555*.
    - **Relevance:** This citation introduces DeepNorm, a novel layer normalization technique that the authors found to be crucial for stabilizing the training of GLM-130B.
    - **Claim:** "To improve FFNs in Transformer, we pick GLU with the GeLU (Hendrycks & Gimpel, 2016) activation as the replacement."
    - **Citation:** Hendrycks, D., & Gimpel, K. (2016). Gaussian error linear units (gelus). *arXiv preprint arXiv:1606.08415*.
    - **Relevance:** This citation introduces GLU with GeLU activation, a technique used to improve the feed-forward networks in GLM-130B.

**2.4 GLM-130B's Pre-Training Setup:**

- **Key Points:** This section details the pre-training setup for GLM-130B, outlining the use of self-supervised blank infilling and multi-task instruction pre-training (MIP). The authors explain the rationale behind using both [MASK] and [gMASK] tokens for blank infilling, highlighting their respective advantages in terms of understanding and generation. They also discuss the inclusion of MIP, emphasizing its potential to improve downstream zero-shot performance while avoiding negative impacts on the model's general abilities.
- **Citations:**
    - **Claim:** "Inspired by recent works (Aribandi et al., 2022; Wei et al., 2022a; Sanh et al., 2022), the GLM-130B pre-training objective includes not only the self-supervised GLM autoregressive blank infilling) but also multi-task learning for a small portion of tokens."
    - **Citation:** Aribandi, V., Tay, Y., Schuster, T., ... & Bahri, J. (2022). Ext5: Towards extreme multi-task scaling for transfer learning. *International Conference on Learning Representations*.
    - **Relevance:** This citation acknowledges the influence of recent works that have explored the benefits of multi-task learning in pre-training LLMs.
    - **Claim:** "Compared to recent works (Wei et al., 2022a; Sanh et al., 2022) that leverage multi-task prompted fine-tuning to improve zero-shot task transfer, MIP only accounts for 5% tokens and is set in the pre-training stage to prevent spoiling LLMs' other general ability, e.g., unconditional free generation."
    - **Citation:** Wei, J., Tay, Y., Bommasani, R., ... & Metzler, D. (2022a). Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*.
    - **Relevance:** This citation highlights the authors' approach to incorporating MIP into the pre-training stage, emphasizing its potential to improve zero-shot performance without negatively impacting the model's general abilities.

**2.5 Platform-Aware Parallel Strategies and Model Configurations:**

- **Key Points:** This section discusses the platform-aware parallel strategies and model configurations used for training GLM-130B. The authors explain their use of a 3D parallel strategy, combining data parallelism, tensor model parallelism, and pipeline model parallelism, to optimize GPU utilization and handle the large memory requirements of the model. They also detail the specific configurations of GLM-130B, including its hidden state dimension, number of layers, and training parameters.
- **Citations:**
    - **Claim:** "The data parallelism (Valiant, 1990) and tensor model parallelism (Shoeybi et al., 2019) are the de facto practices for training billion-scale models (Wang & Komatsuzaki, 2021; Du et al., 2022)."
    - **Citation:** Valiant, L. G. (1990). A bridging model for parallel computation. *Communications of the ACM*, *33*(8), 103-111.
    - **Relevance:** This citation introduces data parallelism and tensor model parallelism, two common techniques for training large-scale models.
    - **Claim:** "To further handle the huge GPU memory requirement and the decrease in overall GPU utilization resulted from applying tensor parallel between nodes—as 40G rather than 80G A100s are used for training GLM-130B, we combine the pipeline model parallelism with the other two strategies to form a 3D parallel strategy."
    - **Citation:** Wang, B., & Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. *https://github.com/kingoflolz/mesh-transformer-jax*.
    - **Relevance:** This citation acknowledges the challenges associated with using tensor model parallelism for training large-scale models, motivating the authors' decision to adopt a 3D parallel strategy.
    - **Claim:** "We leverage the PipeDream-Flush (Narayanan et al., 2021) implementation from DeepSpeed (Rasley et al., 2020) to train GLM-130B with a relative big global batch size (4,224) to reduce time and GPU memory wasting."
    - **Citation:** Narayanan, D., Phanishayee, K., Shi, X., ... & Zaharia, M. (2021). Memory-efficient pipeline-parallel dnn training. *International Conference on Machine Learning*, pp. 7937-7947.
    - **Relevance:** This citation introduces PipeDream-Flush, a technique used to optimize GPU memory usage and reduce training time.

**2.6 The Training Stability of GLM-130B:**

- **Key Points:** This section focuses on the challenges associated with training stability for large-scale LLMs, particularly due to the use of mixed precision. The authors discuss the trade-off between efficiency and stability when using low-precision floating-point formats and highlight the common practice of mixed precision. They then delve into the specific challenges faced during the training of GLM-130B, including frequent loss spikes and the potential for gradient norm divergence. The authors present their solutions to these challenges, including the use of DeepNorm, Embedding Gradient Shrink (EGS), and INT4 quantization.
- **Citations:**
    - **Claim:** "We follow the common practice of a mixed-precision (Micikevicius et al., 2018) strategy (Apex O2), i.e., FP16 for forwards and backwards and FP32 for optimizer states and master weights, to reduce the GPU memory usage and improve training efficiency."
    - **Citation:** Micikevicius, P., Narang, S., Alben, J., ... & Wu, H. (2018). Mixed precision training. *International Conference on Learning Representations*.
    - **Relevance:** This citation introduces the concept of mixed precision, a common technique used to improve training efficiency.
    - **Claim:** "Similar to OPT-175B and BLOOM-176B (C.f. Figure 10 in Appendix), the training of GLM-130B faces frequent loss spikes resulted from this choice, which tends to become increasingly frequent as the training goes on."
    - **Citation:** Zhang, S., Zhou, Y., Dai, Z., ... & LeCun, Y. (2022). OPT: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*.
    - **Relevance:** This citation acknowledges the challenges associated with training stability for large-scale LLMs, particularly due to the use of mixed precision.
    - **Claim:** "BLOOM-176B (Dettmers et al., 2021). We spent months to empirically investigate the spikes and realize that a few issues emerge when transformers scale up."
    - **Citation:** Dettmers, T., Lewis, M., Shleifer, S., & Zettlemoyer, L. (2021). 8-bit optimizers via block-wise quantization. *arXiv preprint arXiv:2110.02861*.
    - **Relevance:** This citation highlights the challenges associated with training stability for large-scale LLMs, particularly due to the use of mixed precision.
    - **Claim:** "First, the transformer main branch's value scale can be extremely large in deeper layers if using Pre-LN. This is addressed in GLM-130B by using DeepNorm based Post-LN (Cf. Section 2.1), which makes the value scale always bounded."
    - **Citation:** None.
    - **Relevance:** This claim introduces DeepNorm, a novel layer normalization technique that the authors found to be crucial for stabilizing the training of GLM-130B.
    - **Claim:** "Second, the attention scores grow so large that they exceed FP16's range, as the model scales up. There are a few options to overcome this issue in LLMs. In CogView (Ding et al., 2021), PB-Relax is proposed to remove bias terms and deduct extremum value in attention computation to avoid the problem, which unfortunately does not help avoid divergence in GLM-130B. In BLOOM-176B, the BF16 format is used instead of FP16, due to its wide range of values on NVIDIA Ampere GPUs (i.e., A100). However, BF16 consumes ~15% more run-time GPU memory than FP16 in our experiments due to its conversion to FP32 in gradient accumulation, and more importantly it is not supported on other GPU platforms (e.g., NVIDIA Tesla V100), limiting the accessibility of produced LLMs. Another option from BLOOM-176B is to apply embedding norm with BF16, but in sacrifice of a significant penalty on model performance, as they notice that embedding norm can harm model's zero-shot learning (Cf. Section 4.3 in (Scao et al., 2022))."
    - **Citation:** Ding, M., Yang, Z., Hong, W., ... & Zou, X. (2021). Cogview: Mastering text-to-image generation via transformers. *Advances in Neural Information Processing Systems*, *34*, 19822-19835.
    - **Relevance:** This citation discusses various approaches to address the issue of attention scores exceeding the range of FP16, highlighting the challenges associated with training stability for large-scale LLMs.
    - **Claim:** "Our empirical search identifies that the gradient norm can serve as an informative indicator of training collapses. Specifically, we find that a training collapse usually lags behind a “spike” in gradient norm by a few training steps. Such spikes are usually caused by the embedding layer's abnormal gradients, as we observe that its gradient norm is often several magnitude larger that those of other layers in GLM-130B's early stage training (Cf. Figure 4 (a)). In addition, it tends to fluctuate dramatically in the early training. The problem is handled in vision models (Chen et al., 2021) via freezing the patch projection layer. Unfortunately, we cannot freeze the training of the embedding layer in language models."
    - **Citation:** Chen, X., Xie, S., & He, K. (2021). An empirical study of training self-supervised vision transformers. *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 9640-9649.
    - **Relevance:** This citation discusses the challenges associated with training stability for large-scale LLMs, particularly due to the use of mixed precision.
    - **Claim:** "Finally, we find the gradient shrink on embedding layers could overcome loss spikes and thus stabilize GLM-130B's training. It is first used in the multi-modal transformer CogView (Ding et al., 2021). Let a be the shrinking factor, the strategy can be easily implemented via word_embedding = word_embedding * a + word_embedding.detach() * (1 – a). Figure 4 (b) suggests that empirically, setting a = 0.1 wipes out most spikes we would have met, with negligible latency."
    - **Citation:** Ding, M., Yang, Z., Hong, W., ... & Zou, X. (2021). Cogview: Mastering text-to-image generation via transformers. *Advances in Neural Information Processing Systems*, *34*, 19822-19835.
    - **Relevance:** This citation introduces Embedding Gradient Shrink (EGS), a technique used to stabilize the training of GLM-130B.
    - **Claim:** "First, instead of using 175B+ parameters as OPT and BLOOM, the 130B size is decided because such a size supports inference on a single A100 (8×40G) server."
    - **Citation:** None.
    - **Relevance:** This claim highlights the authors' decision to use a 130B parameter model, which allows for inference on a single A100 (8×40G) server.
    - **Claim:** "Second, to further lower the GPU requirements, we quantize GLM-130B into INT4 precision without post training while OPT and BLOOM can only reach INT8."
    - **Citation:** None.
    - **Relevance:** This claim highlights the authors' use of INT4 quantization, which further reduces GPU requirements and enables efficient inference on affordable GPUs.

**2.7 GLM-130B Inference on RTX 2080 Ti:**

- **Key Points:** This section focuses on the inference capabilities of GLM-130B, particularly its ability to run on affordable GPUs like RTX 2080 Ti. The authors discuss the challenges associated with running large-scale LLMs on less powerful hardware and highlight the importance of efficient inference solutions. They present their approach to achieving this goal, including the use of FasterTransformer for C++ implementation and INT4 quantization for model compression.
- **Citations:**
    - **Claim:** "As mentioned, the model size of 130B is determined for running the full GLM-130B model on a single A100 (40G×8) server, rather than the high-end A100 (80G×8) machine required by OPT-175B and BLOOM-176B."
    - **Citation:** None.
    - **Relevance:** This claim highlights the authors' goal of making GLM-130B accessible on affordable GPUs.
    - **Claim:** "To accelerate GLM-130B inference, we also leverage FasterTransformer (Timonin et al., 2022) to implement GLM-130B in C++."
    - **Citation:** Timonin, D., Hsueh, B. Y., & Nguyen, V. (2022). Accelerated inference for large transformer models using nvidia triton inference server. *NVIDIA blog*.
    - **Relevance:** This citation introduces FasterTransformer, a technique used to accelerate the inference of GLM-130B.
    - **Claim:** "Typically, the practice is to quantize both model weights and activations to INT8. However, our analysis in Appendix B.6 suggests that LLMs' activations may contain extreme outliers. Concurrently, the emergent outliers in OPT-175B and BLOOM-176B are also discovered (Dettmers et al., 2022), which influence only about 0.1% feature dimensions and are thus solved by matrix multiplication decomposition for the outlying dimensions. Differently, there exist about 30% outliers in GLM-130B's activations, making the technique above far less efficient. Thus, we decide to focus on the quantization of model weights (i.e., mostly linear layers) while keeping the FP16 precision for activations. The quantized model is dynamically converted to FP16 precision at runtime, introducing a small computational overhead but greatly reducing the GPU memory usage for storing model weights."
    - **Citation:** Dettmers, T., Lewis, M., Shleifer, S., & Zettlemoyer, L. (2022). Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv preprint arXiv:2208.07339*.
    - **Relevance:** This citation discusses the challenges associated with quantizing activations in LLMs, highlighting the importance of the authors' approach to focus on quantizing weights instead.

**2.8 The Results:**

- **Key Points:** This section presents the results of evaluating GLM-130B on various benchmarks, including LAMBADA, Pile, MMLU, BIG-bench-lite, CLUE, and FewCLUE. The authors highlight the model's strong performance on these benchmarks, particularly in zero-shot and few-shot settings. They also discuss the limitations of their evaluation, emphasizing the need for further research to address the challenges associated with evaluating LLMs in a comprehensive and unbiased manner.
- **Citations:**
    - **Claim:** "LAMBADA (Paperno et al., 2016) is a dataset to test the last word language modeling capability. The results previously shown in Figure 2 suggest GLM-130B achieves a zero-shot accuracy of 80.2 with its bidirectional attention, setting up a new record on LAMBADA."
    - **Citation:** Paperno, D., Kruszewski, G., Lazaridou, A., ... & Boleda, G. (2016). The lambada dataset: Word prediction requiring a broad discourse context. *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 1525-1534.
    - **Relevance:** This citation introduces the LAMBADA benchmark and highlights GLM-130B's strong performance on this task.
    - **Claim:** "The Pile test-set (Gao et al., 2020) includes a series of benchmarks for language modeling. On average, GLM-130B performs the best on its 18 shared test sets in terms of weighted BPB when compared to GPT-3 and Jurassic-1 (Lieber et al., 2021) whose results are directly adopted from the latter, demonstrating its strong language capability (Cf. Appendix C.4 for details)."
    - **Citation:** Gao, L., Biderman, S., Black, S., ... & Foster, J. (2020). The pile: An 800gb dataset of diverse text for language modeling. *arXiv preprint arXiv:2101.00027*.
    - **Relevance:** This citation introduces the Pile benchmark and highlights GLM-130B's strong performance on this task.
    - **Claim:** "MMLU (Hendrycks et al., 2021) is a diverse benchmark including 57 multi-choice question answering tasks concerning human knowledge ranging from high-school-level to expert-level. It is released after the crawling of Pile and serves as an ideal test-bed for LLMs' few-shot learning. The GPT-3 result is adopted from MMLU and BLOOM-176B is tested by using the same prompts as GLM-130B's (Cf. Appendix C.6 and Table 15 for details)."
    - **Citation:** Hendrycks, D., Burns, C., Basart, S., ... & Steinhardt, J. (2021). Measuring massive multitask language understanding. *International Conference on Learning Representations*.
    - **Relevance:** This citation introduces the MMLU benchmark and highlights GLM-130B's strong performance on this task.
    - **Claim:** "BIG-bench (Srivastava et al., 2022) benchmarks challenging tasks concerning models' ability on reasoning, knowledge, and commonsense. Given evaluating on its 150 tasks is time-consuming for LLMs, we report the BIG-bench-lite-an official 24-task sub-collection for now. Observed from Figure 7 and Table 4, GLM-130B outperforms GPT-3 175B and even PaLM 540B (4× larger) in zero-shot setting. This is probably owing to GLM-130B's bidirectional context attention and MIP, which has been proved to improve zero-shot results in unseen tasks (Wei et al., 2022a; Sanh et al., 2022). As the number of shots increases, GLM-130B's performance keeps going up, maintaining its outperformance over GPT-3 (Cf. Appendix C.5 and Table 14 for details on each model and task)."
    - **Citation:** Srivastava, A., Rastogi, A., Rao, A., ... & Sutskever, I. (2022). Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*.
    - **Relevance:** This citation introduces the BIG-bench benchmark and highlights GLM-130B's strong performance on this task.
    - **Claim:** "We evaluate GLM-130B's Chinese zero-shot performance on established Chinese NLP benchmarks, CLUE (Xu et al., 2020) and FewCLUE (Xu et al., 2021). Note that we do not include any Chinese downstream tasks in MIP. To date, we have finished testing on part of the two benchmarks, including 7 CLUE and 5 FewCLUE datasets (Cf. Appendix C.7 for details). We compare GLM-130B to the largest existing Chinese monolingual language model-the 260B ERNIE Titan 3.0 (Wang et al., 2021). We follow its setting to report zero-shot results on dev datasets. GLM-130B consistently outperforms ERNIE Titan 3.0 across 12 tasks (Cf. Figure 8). Interestingly, GLM-130B performs at least 260% better than ERNIE on two abstractive MRC datasets (DRCD and CMRC2018), possibly due to GLM-130B's pre-training objective that naturally resonates to abstractive MRC's form."
    - **Citation:** Xu, L., Lu, X., Yuan, C., ... & Wei, G. (2020). CLUE: A chinese language understanding evaluation benchmark. *Proceedings of the 28th International Conference on Computational Linguistics*, pp. 4762-4772.
    - **Relevance:** This citation introduces the CLUE benchmark and highlights GLM-130B's strong performance on this task.

**2.9 Related Work:**

- **Key Points:** This section provides a brief overview of related work in the areas of pre-training, transferring, and inference for large language models. The authors highlight the growing trend towards open-sourcing LLMs and discuss the importance of efficient inference solutions for making these models more accessible. They also mention recent advancements in parameter-efficient learning and prompt tuning, suggesting potential areas for future research.
- **Citations:**
    - **Claim:** "Recently, transformer-based (Vaswani et al., 2017) language models present a fascinating scaling law: new abilities (Wei et al., 2022b) arise as models scale up, from 1.5B (Radford et al., 2019), 10B-scale language models (Raffel et al., 2020; Shoeybi et al., 2019; Black et al., 2022), to 100B-scale GPT-3 (Brown et al., 2020)."
    - **Citation:** Vaswani, A., Shazeer, N., Parmar, N., ... & Polosukhin, I. (2017). Attention is all you need. *Advances in neural information processing systems*, *30*.
    - **Relevance:** This citation highlights the growing trend towards larger language models and their impressive scaling laws.
    - **Claim:** "Later, despite many 100B-scale LLMs (Lieber et al., 2021; Thoppilan et al., 2022; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022; Wu et al., 2021; Zeng et al., 2021; Wang et al., 2021) in both English and Chinese, they are not available to public or only accessible via limited APIs. The closeness of LLMs severely stymies its development. GLM-130B's efforts, along with recent ElutherAI, OPT-175B (Zhang et al., 2022), and BLOOM-176B (Scao et al., 2022), aim to offer high-quality open-sourced LLMs to our community."
    - **Citation:** Zhang, S., Zhou, Y., Dai, Z., ... & LeCun, Y. (2022). OPT: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*.
    - **Relevance:** This citation highlights the importance of open-sourcing LLMs and acknowledges the contributions of other researchers in this area.
    - **Claim:** "Though fine-tuning has been a de facto way for transfer learning, the evaluation for LLMs has been focused on prompting and in-context learning due to their tremendous sizes (Brown et al., 2020; Liu et al., 2021a). Nevertheless, some recent attempts has been on parameter-efficient learning on language models (Houlsby et al., 2019) and prompt tuning (i.e., P-tuning, Li & Liang (2021); Liu et al. (2021b); Lester et al. (2021); Liu et al. (2022)). For now we do not focus on them and will leave the comprehensive testing of them on GLM-130B in future study."
    - **Citation:** Houlsby, N., Giurgiu, A., Jastrzebski, S., ... & Gelly, S. (2019). Parameter-efficient transfer learning for nlp. *International Conference on Machine Learning*, pp. 2790-2799.
    - **Relevance:** This citation highlights the importance of parameter-efficient learning and prompt tuning, suggesting potential areas for future research.
    - **Claim:** "Most public-accessible LLMs nowadays