## BLOOM: A 176B-Parameter Open-Access Multilingual Language Model - Citation Analysis

This analysis focuses on extracting and presenting the citations used in the paper "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model" by the BigScience Workshop, published on arXiv on June 27, 2023. The paper details the development and evaluation of BLOOM, a large language model (LLM) designed to be open-access and multilingual.

**1. Introduction**

- **Title:** BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
- **Authors:** BigScience Workshop (with a list of major contributors)
- **Publication Date:** June 27, 2023 (arXiv)
- **Objective:** The research aims to democratize access to powerful LLMs by developing and releasing BLOOM, a 176B-parameter open-access multilingual language model.
- **Total References:** The paper cites a total of 104 references.

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:** The introduction highlights the increasing importance of pretrained language models in NLP, emphasizing their ability to achieve high performance with limited labeled data. It discusses the development of key models like ELMO, ULMFiT, GPT, and BERT, and the subsequent shift towards using pretrained models as initializations for downstream tasks. The authors also note the growing trend of scaling LLMs to larger sizes, leading to improved performance but also raising concerns about accessibility, cost, and environmental impact. The introduction concludes by presenting BLOOM as a solution to these issues, emphasizing its open-access and multilingual nature.

- **Significant Citations:**

    - **Claim:** Pretrained language models have become a cornerstone of modern natural language processing (NLP) pipelines because they often produce better performance from smaller quantities of labeled data.
    - **Citation:** Peters, Matthew E., et al. "Deep contextualized word representations." Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics. 2018.
    - **Relevance:** This citation introduces the concept of pretrained language models and their impact on NLP performance.

    - **Claim:** The development of ELMO, ULMFiT, GPT, and BERT led to the widespread use of pretrained models as an initialization for finetuning on downstream tasks.
    - **Citation:** 
        - Peters, Matthew E., et al. "Deep contextualized word representations." Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics. 2018.
        - Howard, Jeremy, and Sebastian Ruder. "Universal language model fine-tuning for text classification." Proceedings of the 2018 Annual Meeting of the Association for Computational Linguistics. 2018.
        - Radford, Alec, et al. "Improving language understanding by generative pre-training." arXiv preprint arXiv:1803.10655. 2018.
        - Devlin, Jacob, et al. "BERT: Pre-training of deep bidirectional transformers for language understanding." Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. 2019.
    - **Relevance:** This citation highlights the key models that paved the way for the widespread adoption of pretrained language models in NLP.

    - **Claim:** The trend of increasing scale (Zeng et al., 2021; Rae et al., 2021; Smith et al., 2022; Chowdhery et al., 2022) has led to a trend of increasing scale.
    - **Citation:** 
        - Zeng, Aohan, et al. "GLM-130b: An open bilingual pre-trained model." arXiv preprint arXiv:2210.02414. 2022.
        - Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." arXiv preprint arXiv:2112.11446. 2021.
        - Smith, Shaden, et al. "Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-scale generative language model." arXiv preprint arXiv:2201.11990. 2022.
        - Chowdhery, Aakanksha, et al. "PaLM: Scaling language modeling with pathways." arXiv preprint arXiv:2204.02311. 2022.
    - **Relevance:** This citation highlights the growing trend of scaling LLMs to larger sizes, which is a key factor driving the development of BLOOM.

    - **Claim:** Apart from environmental concerns (Strubell et al., 2019; Lacoste et al., 2019; Schwartz et al., 2020), the costs of training large language models (LLMs) are only affordable for well-resourced organizations.
    - **Citation:** 
        - Strubell, Emma, Ananya Ganesh, and Andrew McCallum. "Energy and policy considerations for deep learning in NLP." Proceedings of the 2019 Annual Meeting of the Association for Computational Linguistics. 2019.
        - Lacoste, Alexandre, et al. "Quantifying the carbon emissions of machine learning." arXiv preprint arXiv:1910.09700. 2019.
        - Schwartz, Roy, et al. "Green AI." Communications of the ACM 63.12 (2020): 36-41.
    - **Relevance:** This citation highlights the concerns about the cost and environmental impact of training large LLMs, which motivates the development of BLOOM as an open-access model.

    - **Claim:** To address these issues, we present the BigScience Large Open-science Open-access Multilingual Language Model (BLOOM, BigScience Workshop, 2022).
    - **Citation:** BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." arXiv preprint arXiv:2211.05100. 2022.
    - **Relevance:** This citation introduces BLOOM and its key features, highlighting its open-access and multilingual nature.

**2.2 Background**

- **Key Points:** This section provides background information on LLMs and the BigScience project. It defines language modeling as the task of predicting the probability of a sequence of tokens in a text, tracing its history from early n-gram models to the rise of neural language models based on the Transformer architecture. The section also discusses the concept of transfer learning, where pretrained models are fine-tuned on downstream tasks, and the emergence of few-shot and zero-shot learning, which allows LLMs to perform tasks without any additional training. The authors highlight the social limitations of LLM development, including concerns about accessibility, computational cost, and environmental impact.

- **Significant Citations:**

    - **Claim:** Language modeling refers to the task of modeling the probability of a sequence of tokens in a text (Shannon, 1948), where a token is a unit of text (e.g. word, subword, character or byte, etc., as discussed by Mielke et al., 2021).
    - **Citation:** 
        - Shannon, Claude Elwood. "A mathematical theory of communication." The Bell System Technical Journal 27.3 (1948): 379-423.
        - Mielke, Sabrina J., et al. "Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP." arXiv preprint arXiv:2112.10508. 2021.
    - **Relevance:** This citation defines language modeling and introduces the concept of tokens, which are fundamental units in NLP.

    - **Claim:** Early language models (such as those developed by Shannon, 1948) were primarily n-gram models that estimate the probability of a length-n sequence of tokens in accordance with the number of times it appears in a training corpus.
    - **Citation:** Shannon, Claude Elwood. "A mathematical theory of communication." The Bell System Technical Journal 27.3 (1948): 379-423.
    - **Relevance:** This citation introduces the concept of n-gram models, which were early approaches to language modeling.

    - **Claim:** Consequently, the Transformer has become the de facto choice for language models.
    - **Citation:** 
        - Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems 30 (2017).
        - Radford, Alec, et al. "Improving language understanding by generative pre-training." arXiv preprint arXiv:1803.10655. 2018.
        - Al-Rfou, Rami, et al. "Character-level language modeling with deeper self-attention." Proceedings of the AAAI Conference on Artificial Intelligence. 2019.
        - Kaplan, Jared, et al. "Scaling laws for neural language models." arXiv preprint arXiv:2001.08361. 2020.
    - **Relevance:** This citation highlights the dominance of the Transformer architecture in modern language modeling.

    - **Claim:** In transfer learning, the parameters of a model are first pretrained on a data-rich task before being finetuned on a downstream task.
    - **Citation:** 
        - Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." Advances in neural information processing systems 26 (2013).
        - Peters, Matthew E., et al. "Deep contextualized word representations." Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics. 2018.
        - Howard, Jeremy, and Sebastian Ruder. "Universal language model fine-tuning for text classification." Proceedings of the 2018 Annual Meeting of the Association for Computational Linguistics. 2018.
        - Radford, Alec, et al. "Improving language understanding by generative pre-training." arXiv preprint arXiv:1803.10655. 2018.
        - Devlin, Jacob, et al. "BERT: Pre-training of deep bidirectional transformers for language understanding." Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. 2019.
    - **Relevance:** This citation introduces the concept of transfer learning, which is a key technique used in the development of LLMs.

    - **Claim:** While finetuning a pretrained model remains an effective way of attaining high performance with limited labeled data, a parallel line of work has demonstrated that pretrained language models can be induced to perform tasks without any subsequent training.
    - **Citation:** 
        - Vinyals, Oriol, and Quoc V. Le. "A neural conversational model." arXiv preprint arXiv:1506.05869. 2015.
        - Radford, Alec, et al. "Language models are unsupervised multitask learners." OpenAI, 2019.
    - **Relevance:** This citation introduces the concept of few-shot and zero-shot learning, which are emerging techniques for using pretrained LLMs.

    - **Claim:** The computational expense of large models also prohibits the majority of the research community from participating in their development, evaluation and routine use.
    - **Citation:** Bender, Emily M., et al. "On the dangers of stochastic parrots: Can language models be too big?" Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 2021.
    - **Relevance:** This citation highlights the concerns about the accessibility and cost of developing large LLMs, which motivates the development of BLOOM as an open-access model.

    - **Claim:** Contributing to an increase in the global carbon footprint exacerbates climate change which most severely affects already-marginalized communities (Westra and Lawson, 2001).
    - **Citation:** Westra, Laura S., and Bill E. Lawson. Faces of Environmental Racism: Confronting Issues of Global Justice. Rowman & Littlefield Publishers, 2001.
    - **Relevance:** This citation highlights the environmental impact of training large LLMs, which is a key concern addressed by the BigScience project.

**2.3 BigScience**

- **Key Points:** This section provides an overview of the BigScience project, highlighting its collaborative nature and its goal of democratizing access to LLMs. It describes the project's origins, its funding, and its growth from a small group of researchers to a large international collaboration. The section also emphasizes the project's commitment to diversity and inclusivity, noting the participation of researchers from various backgrounds and disciplines.

- **Significant Citations:**

    - **Claim:** Participants BLOOM's development was coordinated by BigScience, an open research collaboration whose goal was the public release of an LLM.
    - **Citation:** BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." arXiv preprint arXiv:2211.05100. 2022.
    - **Relevance:** This citation introduces the BigScience project and its key objective.

    - **Claim:** The project's released artifacts. While the largest number of participants ultimately originated from the US, 38 countries were represented.
    - **Citation:** BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." arXiv preprint arXiv:2211.05100. 2022.
    - **Relevance:** This citation highlights the international scope of the BigScience project.

    - **Claim:** The set of related research questions tackled by the BigScience effort was reflected in the project's organization into working groups.
    - **Citation:** BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." arXiv preprint arXiv:2211.05100. 2022.
    - **Relevance:** This citation highlights the organizational structure of the BigScience project.

    - **Claim:** In particular, the charter emphasizes values of inclusivity and diversity, openness and reproducibility, and responsibility in various aspects of the organization (Akiki et al., 2022).
    - **Citation:** Akiki, Christopher, et al. "BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model." arXiv preprint arXiv:2212.04960. 2022.
    - **Relevance:** This citation highlights the ethical considerations guiding the BigScience project.

**3. BLOOM**

- **Key Points:** This section delves into the technical details of BLOOM's development, covering its training dataset, architecture, tokenizer, computing infrastructure, and training hyperparameters.

**3.1 Training Dataset**

- **Key Points:** The authors describe the ROOTS corpus, a composite collection of 498 Hugging Face datasets spanning 46 natural languages and 13 programming languages, as the training dataset for BLOOM. They highlight the process of dataset curation, emphasizing the importance of data governance, ethical considerations, and human involvement. The authors also discuss the selection of languages for inclusion in the corpus, emphasizing the importance of balancing representation with available resources and expertise.

- **Significant Citations:**

    - **Claim:** BLOOM was trained on the ROOTS corpus (Laurençon et al., 2022), a composite collection of 498 Hugging Face datasets (Lhoest et al., 2021) amounting to 1.61 terabytes of text that span 46 natural languages and 13 programming languages.
    - **Citation:** 
        - Laurençon, Hugo, et al. "The BigScience ROOTS corpus: A 1.6TB composite multilingual dataset." Proceedings of the 36th Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2022.
        - Lhoest, Quentin, et al. "Datasets: A community library for natural language processing." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2021.
    - **Relevance:** This citation introduces the ROOTS corpus, which is the primary training dataset for BLOOM.

    - **Claim:** The BigScience effort aimed to address these needs through a multidisciplinary lens combining technical, legal, and sociological expertise.
    - **Citation:** BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." arXiv preprint arXiv:2211.05100. 2022.
    - **Relevance:** This citation highlights the importance of data governance in the BigScience project.

    - **Claim:** In the context of the BigScience workshop, and in accordance with its Ethical Charter, we aimed to prioritize human involvement, local expertise, and language expertise in our data curation and documentation process, as outlined in the following sections.
    - **Citation:** BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." arXiv preprint arXiv:2211.05100. 2022.
    - **Relevance:** This citation highlights the ethical considerations guiding the data curation process for BLOOM.

    - **Claim:** We started with a list of eight of the world's largest languages by number of speakers for which we did active outreach in the early stages of the project to invite fluent speakers to join the data efforts.
    - **Citation:** BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." arXiv preprint arXiv:2211.05100. 2022.
    - **Relevance:** This citation highlights the process of language selection for the ROOTS corpus.

    - **Claim:** Finally, we proposed that any group of 3 or more participants fluent in an additional language could add it to the supported list if they would commit to selecting sources and guiding processing choices in the language in order to avoid common issues with corpora selected through automatic language identification without specific language expertise (Caswell et al., 2022).
    - **Citation:** Caswell, Isaac, et al. "Exploring bert's vocabulary." 2019.
    - **Relevance:** This citation highlights the importance of involving language experts in the data curation process.

**3.2 Model Architecture**

- **Key Points:** This section discusses the design methodology and architecture of BLOOM, emphasizing the choice of a causal decoder-only Transformer model and the rationale behind specific architectural deviations. The authors highlight the importance of zero-shot generalization in their evaluation process and describe their approach to ablating different architectural components and hyperparameters.

- **Significant Citations:**

    - **Claim:** The design space of possible architectures is immense, making exhaustive exploration impossible.
    - **Citation:** Narang, Sharan, et al. "Efficient Large-Scale Language Model Training on GPU Clusters using Megatron-LM." Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.
    - **Relevance:** This citation highlights the challenges of exploring the vast design space of possible architectures for LLMs.

    - **Claim:** One option would be to exactly replicate the architecture of an existing large language model.
    - **Citation:** 
        - Devlin, Jacob, et al. "BERT: Pre-training of deep bidirectional transformers for language understanding." Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. 2019.
        - Radford, Alec, et al. "Language models are unsupervised multitask learners." OpenAI, 2019.
    - **Relevance:** This citation highlights the common practice of replicating existing architectures for LLMs.

    - **Claim:** On the other hand, a great deal of work on improving existing architectures has seen relatively little adoption (Narang et al., 2021); adopting some of these recommended practices could yield a significantly better model.
    - **Citation:** Narang, Sharan, et al. "Efficient Large-Scale Language Model Training on GPU Clusters using Megatron-LM." Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.
    - **Relevance:** This citation highlights the need for exploring novel architectural improvements for LLMs.

    - **Claim:** We take a middle ground and focus on model families that have been shown to scale well, and that have reasonable support in publicly available tools and codebases.
    - **Citation:** 
        - Shoeybi, Mohammad, et al. "Megatron-LM: Training multi-billion parameter language models using model parallelism." arXiv preprint arXiv:1909.08053. 2019.
        - Brown, Tom, et al. "Language models are few-shot learners." Advances in Neural Information Processing Systems 33 (2020).
    - **Relevance:** This citation highlights the importance of choosing scalable and well-supported architectures for LLMs.

    - **Claim:** We conducted our ablation experiments using smaller models.
    - **Citation:** 
        - Wang, Thomas, et al. "What language model architecture and pretraining objective works best for zero-shot generalization?" Proceedings of Machine Learning Research 162 (2022).
        - Le Scao, Teven, et al. "What language model to train if you have one million GPU hours?" Proceedings of the 2022 Workshop on Challenges & Perspectives in Creating Large Language Models. 2022.
    - **Relevance:** This citation highlights the use of smaller models for ablating different architectural components and hyperparameters.

    - **Claim:** Recently, Dettmers et al. (2022) identified a phase transition for models larger than 6.7B, in which the emergence of "outliers features" is observed.
    - **Citation:** Dettmers, Tim, et al. "LLM.int8(): 8-bit matrix multiplication for transformers at scale." arXiv preprint arXiv:2208.07339. 2022.
    - **Relevance:** This citation highlights the importance of considering the scaling behavior of LLMs when interpreting ablation results.

    - **Claim:** We did not consider mixture-of-experts (MoE) (Shazeer et al., 2017), due to a lack of widely used GPU-based codebases suitable for training them at scale.
    - **Citation:** Shazeer, Noam, et al. "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer." International Conference on Learning Representations. 2017.
    - **Relevance:** This citation highlights the limitations of using mixture-of-experts architectures for LLMs.

    - **Claim:** Similarly, we also did not consider state-space models (Gu et al., 2020).
    - **Citation:** Gu, Albert, et al. "Hippo: Recurrent memory with optimal polynomial projections." Advances in Neural Information Processing Systems 33 (2020).
    - **Relevance:** This citation highlights the limitations of using state-space models for LLMs.

    - **Claim:** Both of these approaches are promising, and have now demonstrated competitive results—at large scales for MoE (Fedus et al., 2022; Srivastava et al., 2022), and at smaller scale for state-space models with H3 (Fu et al., 2023).
    - **Citation:** 
        - Fedus, William, et al. "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity." Journal of Machine Learning Research 23.120 (2022): 1-39.
        - Srivastava, Aarohi, et al. "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models." arXiv preprint arXiv:2206.04615. 2022.
        - Fu, Daniel Y., et al. "Hungry hungry hippos: Towards language modeling with state space models." Proceedings of the 11th International Conference on Learning Representations. 2023.
    - **Relevance:** This citation highlights the recent progress in using mixture-of-experts and state-space models for LLMs.

    - **Claim:** Although most modern language models are based on the Transformer architecture, there are significant deviations between architectural implementations.
    - **Citation:** 
        - Devlin, Jacob, et al. "BERT: Pre-training of deep bidirectional transformers for language understanding." Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. 2019.
        - Radford, Alec, et al. "Improving language understanding by generative pre-training." arXiv preprint arXiv:1803.10655. 2018.
    - **Relevance:** This citation highlights the diversity of architectural implementations for Transformer-based LLMs.

    - **Claim:** Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022).
    - **Citation:** 
        - Brown, Tom, et al. "Language models are few-shot learners." Advances in Neural Information Processing Systems 33 (2020).
        - Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." arXiv preprint arXiv:2112.11446. 2021.
        - Chowdhery, Aakanksha, et al. "PaLM: Scaling language modeling with pathways." arXiv preprint arXiv:2204.02311. 2022.
    - **Relevance:** This citation highlights the dominance of causal decoder-only models in the realm of large LLMs.

    - **Claim:** Prior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives.
    - **Citation:** Wang, Thomas, et al. "What language model architecture and pretraining objective works best for zero-shot generalization?" Proceedings of Machine Learning Research 162 (2022).
    - **Relevance:** This citation highlights the novelty of the authors' work in systematically evaluating the zero-shot generalization capabilities of different architectures and pretraining objectives.

    - **Claim:** Our results show that immediately after pretraining, causal decoder-only models performed best - validating the choice of state-of-the-art LLMs.
    - **Citation:** Wang, Thomas, et al. "What language model architecture and pretraining objective works best for zero-shot generalization?" Proceedings of Machine Learning Research 162 (2022).
    - **Relevance:** This citation highlights the authors' findings that support the choice of causal decoder-only models for large LLMs.

    - **Claim:** Beyond choosing an architecture and pretraining objective, a number of changes to the original Transformer architecture have been proposed.
    - **Citation:** 
        - Su, Jianlin, et al. "RoFormer: Enhanced transformer with rotary position embedding." arXiv preprint arXiv:2104.09864. 2021.
        - Press, Oren, et al. "Train short, test long: Attention with linear biases enables input length extrapolation." International Conference on Learning Representations. 2021.
        - Shazeer, Noam. "GLU variants improve transformer." arXiv preprint arXiv:2002.05202. 2020.
    - **Relevance:** This citation highlights the ongoing research efforts to improve the Transformer architecture.

**3.3 Tokenization**

- **Key Points:** This section discusses the design and validation of BLOOM's tokenizer, emphasizing the importance of careful design choices for handling the diverse nature of the training data. The authors highlight the use of byte-level BPE for lossless tokenization and vocabulary sharing across languages. They also describe the validation process using fertility metrics and the rationale behind choosing a vocabulary size of 250,680 tokens.

- **Significant Citations:**

    - **Claim:** The design decisions when training a tokenizer are often neglected in favour of "default" settings (Mielke et al., 2021).
    - **Citation:** Mielke, Sabrina J., et al. "Between words and characters: A brief history of open-vocabulary modeling and tokenization in NLP." arXiv preprint arXiv:2112.10508. 2021.
    - **Relevance:** This citation highlights the common practice of using default tokenizers without careful consideration.

    - **Claim:** For instance, OPT (Zhang et al., 2022) and GPT-3 (Brown et al., 2020) both use GPT-2's tokenizer, trained for English.
    - **Citation:** 
        - Zhang, Susan, et al. "OPT: Open pre-trained transformer language models." arXiv preprint arXiv:2205.01068. 2022.
        - Brown, Tom, et al. "Language models are few-shot learners." Advances in Neural Information Processing Systems 33 (2020).
    - **Relevance:** This citation highlights the common practice of using English-specific tokenizers for LLMs.

    - **Claim:** We use the fertility (Ács, 2019) of our tokenizer compared to existing monolingual tokenizers as a metric for sanity checks.
    - **Citation:** Ács, Judit. "Exploring bert's vocabulary." 2019.
    - **Relevance:** This citation introduces the concept of fertility as a metric for evaluating tokenizers.

    - **Claim:** A very high fertility on a language compared to a monolingual tokenizer may indicate a degradation on the downstream multilingual performance of the model (Rust et al., 2021).
    - **Citation:** Rust, Phillip, et al. "How good is your tokenizer? On the monolingual performance of multilingual language models." Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.
    - **Relevance:** This citation highlights the potential impact of tokenizer fertility on multilingual performance.

    - **Claim:** For all experiments, the Hugging Face Tokenizers library (Moi et al., 2019) was used to design and train the tested tokenizers.
    - **Citation:** Moi, Anthony, et al. "Hugging face tokenizers library." 2019.
    - **Relevance:** This citation highlights the use of the Hugging Face Tokenizers library for tokenizer development.

    - **Claim:** We initially used a non-deduplicated subset of ROOTS.
    - **Citation:** BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." arXiv preprint arXiv:2211.05100. 2022.
    - **Relevance:** This citation highlights the initial approach to tokenizer training data.

    - **Claim:** These issues motivated us to remove duplicated lines in the tokenizer training training data.
    - **Citation:** BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." arXiv preprint arXiv:2211.05100. 2022.
    - **Relevance:** This citation highlights the importance of data cleaning for tokenizer training.

    - **Claim:** We conducted validation experiments using 150k and 250k vocabulary sizes to make comparisons with existing multilingual modeling literature easier (Conneau et al., 2020; Xue et al., 2021).
    - **Citation:** 
        - Conneau, Alexis, et al. "Unsupervised cross-lingual representation learning at scale." Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020.
        - Xue, Linting, et al. "mT5: A massively multilingual pre-trained text-to-text transformer." Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.
    - **Relevance:** This citation highlights the importance of comparing vocabulary sizes with existing literature.

    - **Claim:** Since the vocabulary size determines the embedding matrix size, it also had to be divisible by 128 for GPU efficiency reasons and by 4 to be able to use Tensor Parallelism.
    - **Citation:** 
        - Shoeybi, Mohammad, et al. "Megatron-LM: Training multi-billion parameter language models using model parallelism." arXiv preprint arXiv:1909.08053. 2019.
        - Narayanan, Deepak, et al. "Efficient Large-Scale Language Model Training on GPU Clusters using Megatron-LM." Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021.
    - **Relevance:** This citation highlights the practical considerations for choosing a vocabulary size.

    - **Claim:** The tokenizer is a learned subword tokenizer trained using the Byte Pair Encoding (BPE) algorithm introduced by Gage (1994).
    - **Citation:** Gage, Philip. "A new algorithm for data compression." C Users J. 12.2 (1994): 23-38.
    - **Relevance:** This citation introduces the concept of Byte Pair Encoding (BPE) for tokenizer training.

    - **Claim:** This way, tokenization never results in unknown tokens because all 256 bytes can be contained in the vocabulary of the tokenizer.
    - **Citation:** Radford, Alec, et al. "Language models are unsupervised multitask learners." OpenAI, 2019.
    - **Relevance:** This citation highlights the advantage of byte-level BPE for handling unknown tokens.

    - **Claim:** Byte-level BPE maximizes vocabulary sharing between languages (Wang et al., 2020).
    - **Citation:** Wang, Changhan, Kyunghyun Cho, and Jiatao Gu. "Neural machine translation with byte-level subwords." Proceedings of the AAAI Conference on Artificial Intelligence. 2020.
    - **Relevance:** This citation highlights the advantage of byte-level BPE for multilingual tokenization.

**3.4 Engineering**

- **Key Points:** This section describes the hardware and software infrastructure used for training BLOOM, highlighting the use of the Jean Zay supercomputer, the Megatron-DeepSpeed framework, and 3D parallelism. The authors also discuss the challenges of scaling training to hundreds of GPUs and the use of mixed-precision training and fused CUDA kernels for optimization.

- **Significant Citations:**

    - **Claim:** The model was trained on Jean Zay, a French government-funded supercomputer owned by GENCI and operated at IDRIS, the national computing center for the French National Center for Scientific Research (CNRS).
    - **Citation:** BigScience Workshop. "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model." arXiv preprint arXiv:2211.05100. 2022.
    - **Relevance:** This citation highlights the hardware infrastructure used for training BLOOM.

    - **Claim:** BLOOM was trained using Megatron-DeepSpeed (Smith et al., 2022), a framework for large-scale distributed training.
    - **Citation:** Smith, Shaden, et al. "Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-scale generative language model." arXiv preprint arXiv:2201.11990. 2022.
    - **Relevance:** This citation introduces the Megatron-DeepSpeed framework, which is a key component of the training infrastructure.

    - **Claim:** It consists of two parts: Megatron-LM (Shoeybi et al., 2019) provides the Transformer implementation, tensor parallelism, and data loading primitives, whereas DeepSpeed (Rasley et al., 2020) provides the ZeRO optimizer, model pipelining, and general distributed training components.
    - **Citation:** 
        - Shoeybi, Mohammad, et al. "Megatron-LM: Training multi-billion parameter language models using model parallelism." arXiv preprint arXiv:1909.08053. 2019.
        - Rasley, Jeff, et al. "DeepSpeed: System optimizations enable training deep learning models with over 100 billion parameters." Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020.
    - **Relevance:** This citation highlights the key components of the Megatron-DeepSpeed framework.

    - **Claim:** This framework allows us to train efficiently with 3D parallelism (Narayanan et al