## Analysis of "Hungry Hungry Hippos: Towards Language Modeling with State Space Models"

**1. Introduction:**

- **Title:** Hungry Hungry Hippos: Towards Language Modeling with State Space Models
- **Authors:** Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré
- **Publication Date:** December 28, 2022 (v3: April 29, 2023)
- **Objective:** The paper aims to understand the expressivity gap between state space models (SSMs) and attention in language modeling and to reduce the hardware barrier between these two approaches.
- **References:** The paper cites a total of 66 references.

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:**
    - SSMs have shown promise in various domains but underperform attention in language modeling.
    - The authors question whether this gap is due to inherent limitations of SSMs or to the resources and hardware optimizations dedicated to attention-based models.
- **Significant Citations:**
    - **Claim:** "State space models (SSMs) have achieved state-of-the-art sequence modeling performance in domains ranging from time series analysis [25] to audio generation [22]."
    - **Citation:** [25] Gu, A., Goel, K., & Ré, C. (2022). Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR).
    - **Relevance:** This citation highlights the success of SSMs in other domains, setting the stage for the paper's focus on language modeling.
    - **Claim:** "However, they have yet to match the performance of Transformers on language modeling, often underperforming Transformers by multiple points in perplexity [25]."
    - **Citation:** [25] Gu, A., Goel, K., & Ré, C. (2022). Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR).
    - **Relevance:** This citation emphasizes the performance gap between SSMs and Transformers in language modeling, motivating the research presented in the paper.
    - **Claim:** "An natural question is whether this gap in performance is due to inherent inductive biases and capabilities in attention [17, 49], or whether it is a function of the significant organizational resources that have been spent training and tuning large attention-based language models [10, 32, 66], as well as specialized hardware support for attention, ranging from tensor cores [45] to transformer chips [34, 48]."
    - **Citations:**
        - [17] Edelman, B. L., Goel, S., Kakade, S., & Zhang, C. (2022). Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pages 5793-5831. PMLR.
        - [49] Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., ... & Olah, C. (2022). In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
        - [10] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., ... & Gehrmann, S. (2022). Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.
        - [32] Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., ... & Clark, A. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.
        - [66] Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., ... & Lin, X. V. (2022). OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.
        - [45] NVIDIA. (2017). Nvidia Tesla V100 GPU architecture.
        - [34] Kao, S.-C., Subramanian, S., Agrawal, G., & Krishna, T. (2021). An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419.
        - [48] NVIDIA. (2022). Nvidia H100 tensor core GPU architecture.
    - **Relevance:** This set of citations highlights the key questions the paper aims to address: whether the performance gap is due to fundamental differences between SSMs and attention or to the resources and hardware advantages of attention-based models.

**2.2 Background:**

- **Key Points:**
    - The authors provide a brief overview of state space models (SSMs) and linear attention, highlighting their key properties and connections to recurrent neural networks (RNNs).
- **Significant Citations:**
    - **Claim:** "A continuous-time state-space representation [6] defines a linear mapping from an input signal u(t) ∈ R (as a function of time t) to an output signal y(t) ∈ R through a state-variable x(t) ∈ Rm, with the following differential equation, for some matrices A ∈ Rm×m, B ∈ Rm×1, C∈ R1×m, D∈ R1×1: x(t) = Ax(t)+Bu(t), y(t) = Cx(t) + Du(t)."
    - **Citation:** [6] Brogan, W. L. (1974). Modern control theory.
    - **Relevance:** This citation introduces the mathematical framework of continuous-time state space models, providing a foundation for understanding the SSMs discussed in the paper.
    - **Claim:** "Similarly, a discrete-time state-space representation defines a linear mapping from a discrete input signal ui (for i = 1,2,...) to a discrete output signal yi though a state-variable xi ∈ Rm: Xi = Axi−1 + Bui Yi = Cxi + Dui."
    - **Citation:** None.
    - **Relevance:** This section introduces the discrete-time state space model, which is the primary focus of the paper. While no specific citation is provided, this section builds upon the general framework of state space models introduced in the previous section.
    - **Claim:** "A state-space model (SSM) uses these representations as a layer in a deep learning pipeline, where the matrices A, B, C, D are learned from data (e.g., with gradient-based optimization). One often has d of these SSMs in parallel, each corresponding to one hidden dimension. To preserve the sequence history, HiPPO [24] projects the history on a basis of orthogonal polynomials, which translates to having SSMs whose A, B matrices are initialized to some special matrices."
    - **Citation:** [24] Gu, A., Dao, T., Ermon, S., Rudra, A., & Ré, C. (2020). Hippo: Recurrent memory with optimal polynomial projections. Advances in Neural Information Processing Systems, 33:1474-1487.
    - **Relevance:** This citation introduces HiPPO, a specific type of SSM that utilizes orthogonal polynomials to preserve sequence history. This is relevant to the paper's discussion of SSMs as a potential alternative to attention.
    - **Claim:** "This recurrent form of SSMs allows efficient inference (i.e., generation): to generate the output of the next time-step, one only needs the state of the current time-step, not the entire input history. Furthermore, SSMs can freely extrapolate to sequences longer than seen during training."
    - **Citation:** None.
    - **Relevance:** This section highlights the key advantages of SSMs: efficient inference and the ability to extrapolate to longer sequences. While no specific citation is provided, these points are crucial for understanding the potential of SSMs in language modeling.
    - **Claim:** "SSMs as Convolution. For efficient training, given the entire sequence of the input 41,..., un, the output sequence y1,..., yn can also be written as the convolution of the input with the filter [27]: f = [CB, CAB, CA2B, . . ., CAN-1B]."
    - **Citation:** [27] Gu, A., Johnson, I., Goel, K., Saab, K., Dao, T., Rudra, A., & Ré, C. (2021). Combining recurrent, convolutional, and continuous-time models with linear state-space layers. Advances in neural information processing systems, 34.
    - **Relevance:** This citation introduces the concept of SSMs as convolutions, which is crucial for understanding the efficiency of training and inference with SSMs.
    - **Claim:** "That is, from an initial condition 20, we have yi = CABxo + (f * u)i + Dui, where (f * u) denotes a linear convolution between f and u. If we set the initial condition 2o to be zero, then y is exactly a linear convolution of u, with a residual connection Du. More generally, any linear time-invariant system (of which SSMs are a special case) can be written as a convolution."
    - **Citation:** None.
    - **Relevance:** This section further elaborates on the convolution representation of SSMs, highlighting its connection to linear time-invariant systems.
    - **Claim:** "Given a 1D input sequence u ∈ RN of length N, we denote the 1D output sequence y ∈ RN of an SSM parameterized by matrices A, B, C, D as y = SSMA,B,C,D(u)."
    - **Citation:** None.
    - **Relevance:** This section introduces the notation used to represent SSMs, which is essential for understanding the subsequent discussion.
    - **Claim:** "To simplify notation, we omit the reference to A, B, C, D and write y = SSM(u) if they are clear from context. When u is multidimensional of dimension d, we stack d of these SSMs together that defines a mapping from u∈ RN×d to y∈ RN×d, using the same notation y = SSM(u)."
    - **Citation:** None.
    - **Relevance:** This section further clarifies the notation used for SSMs, extending it to multidimensional inputs.
    - **Claim:** "SSM through FFTs. Computing the convolution naively through conventional matrix operations is expensive for long kernels, scaling as O(N2). Instead, we can use FFTs: take the FFT of f and u, multiply them together pointwise, and then take the inverse FFT. This yields an O(N log N) algorithm."
    - **Citations:**
        - [26] Gu, A., Gupta, A., Goel, K., & Ré, C. (2022). On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems.
        - [29] Gupta, A., Gu, A., & Berant, J. (2022). Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems.
    - **Relevance:** This section introduces the use of Fast Fourier Transform (FFT) for efficient computation of convolutions, which is a key technique for scaling SSMs to longer sequences.
    - **Claim:** "We describe linear attention [35] and its connection to RNNs, which inspired our model design (Section 3)."
    - **Citation:** [35] Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020). Transformers are RNNS: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pages 5156-5165. PMLR.
    - **Relevance:** This citation introduces linear attention, which is a key inspiration for the H3 layer proposed in the paper.
    - **Claim:** "In standard attention [62], we have N query/key/value tokens Qi, Ki, Vi ∈ Rd for i = 1, . . ., N, where N is the sequence length and d is the head dimension. For some similarity metric Sim: Rd × Rd → R, we want to compute the output: 0₁ = Σ=1 Sim(Qi, Kj)Vj Σ=1 Sim(Qi, Kj) ∈ Rd."
    - **Citation:** [62] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.
    - **Relevance:** This citation provides a brief overview of standard attention, setting the stage for the discussion of linear attention.
    - **Claim:** "For standard softmax attention, Sim(q, k) = eq¹k (often the dot product is scaled by 1/√d). Linear attention makes the assumption that Sim has the form Sim(q,k) = $(q)T(k), for some (nonlinear) function ф."
    - **Citation:** None.
    - **Relevance:** This section defines linear attention, highlighting its key assumption about the form of the similarity function.
    - **Claim:** "The output is then Oi =
Let Si = Σ=1 (Kj)V; ∈ Rd×d, zi = ∑=1$(Kj) ∈ Rd,
di
= $(Qi)Tzi ∈ R. Then O₁ = $(Q)Si. This connects linear attention to RNNs: the output O₁ is a function of Si and zi, both of which are incrementally updated (as cumulative sums)."
    - **Citation:** None.
    - **Relevance:** This section further explains linear attention, highlighting its connection to RNNs.

**2.3 Hungry Hungry Hippos Layer to Model Discrete Sequences:**

- **Key Points:**
    - The authors introduce two synthetic language modeling tasks to understand the expressivity gap between SSMs and attention.
    - They propose a new SSM layer, H3, designed to address the limitations of existing SSMs in these tasks.
    - H3 outperforms existing SSMs and comes close to attention in terms of perplexity on OpenWebText.
    - A hybrid H3-attention model surprisingly outperforms Transformers on OpenWebText.
- **Significant Citations:**
    - **Claim:** "To understand the gap between SSMs and attention on language modeling, we examine two synthetic language modeling tasks. These tasks motivate our H3 layer to add a discrete SSM (based on shift matrix) and multiplicative interaction to effectively model discrete sequences. We then show that the H3 layer is expressive enough to solve these synthetic tasks, and that this understanding leads to better performance on a real language modeling benchmark."
    - **Citation:** [49] Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., ... & Olah, C. (2022). In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.
    - **Relevance:** This citation introduces the synthetic language modeling tasks used to understand the expressivity gap between SSMs and attention.
    - **Claim:** "The Induction Head task tests how well a model can recall content after a special token (e.g., F in Table 1). At the end of the sequence, the model must recall the token that appeared immediately after the special token earlier in the sequence. Associative Recall [1] is similar to the induction head task, but requires the model to remember multiple key-value pairs. At the end of the sequence, the model must recall a specific value belonging to a specific key."
    - **Citation:** [1] Ba, J., Hinton, G. E., Mnih, V., Leibo, J. Z., & Ionescu, C. (2016). Using fast weights to attend to the recent past. Advances in neural information processing systems, 29.
    - **Relevance:** This citation introduces the two synthetic tasks: Induction Head and Associative Recall.
    - **Claim:** "Table 2 (for two-layer models) shows that S4D [26] and Gated State Spaces [42] both fail to model these synthetic languages, which suggests they may not have the expressivity for general language. We argue that these failures suggest two missing capabilities: (i) to remember tokens that appear after a particular event (e.g., the special token in the induction head task), and (ii) to compare tokens across the sequence (e.g., comparing keys to decide which value to recall). Attention has both these capabilities: it can compare tokens by constructing the quadratic attention matrix QKT, and it can recall tokens by direct copying (multiplying softmax(QKT) with V). In Section 3.2, we design our new layer H3 to enable these capabilities in SSMs, narrowing the expressivity gap between SSMs and attention."
    - **Citations:**
        - [26] Gu, A., Gupta, A., Goel, K., & Ré, C. (2022). On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems.
        - [42] Mehta, H., Gupta, A., Cutkosky, A., & Neyshabur, B. (2022). Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947.
    - **Relevance:** This citation analyzes the performance of existing SSMs on the synthetic tasks, highlighting their limitations and motivating the design of H3.
    - **Claim:** "H3 matches attention on the synthetic languages and almost closes the gap with Transformers on language modeling-coming within 0.4 perplexity of Transformers on OpenWebText (compared to 3.4 ppl for existing SSMs even those explicitly designed for language modeling [42]). Furthermore, a simple hybrid H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 perplexity."
    - **Citation:** [42] Mehta, H., Gupta, A., Cutkosky, A., & Neyshabur, B. (2022). Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947.
    - **Relevance:** This citation highlights the performance of H3 and the hybrid H3-attention model on OpenWebText, demonstrating their effectiveness in language modeling.

**2.4 H3 Layer:**

- **Key Points:**
    - The authors describe the architecture of the H3 layer, which combines shift and diagonal SSMs with multiplicative interactions.
    - H3 is designed to address the limitations of existing SSMs in recalling earlier tokens and comparing tokens across the sequence.
- **Significant Citations:**
    - **Claim:** "H3 uses SSMs with shift and diagonal matrices, along with multiplicative operations against projections of the input to capture the missing capabilities identified by the synthetics."
    - **Citation:** None.
    - **Relevance:** This section introduces the key components of the H3 layer: shift and diagonal SSMs and multiplicative interactions.
    - **Claim:** "High-level Intuition. (i) To remember tokens from the past, we want the state xi to copy from the input ui, and then pass that information to the next state xi+1. As xi+1 relates to xiby Axi, we use a discrete SSM with a shift matrix A (described formally below) that shifts the elements of a state vector (e.g., mapping [a, b, c] → [0, a, b]). (ii) To compare tokens across the sequence, we use multiplicative interaction: the output of an SSM, containing information from previous time steps, is multiplied with the input at the current time steps, thus measuring similarity between tokens."
    - **Citation:** None.
    - **Relevance:** This section provides a high-level explanation of the intuition behind the design of H3, highlighting its ability to recall past tokens and compare tokens across the sequence.
    - **Claim:** "H3 is loosely inspired by linear attention (Section 2): we project the input u to get three signals Q, K, V. Then we replace the non-linearity (K) with an SSM where A is a shift matrix (SSMshift), and we replace the summation S₁ with a SSM with diagonal A (SSMdiag). The output, for the case of head dimension d₁ = 1, is: QSSMdiag (SSMshift (K) V), where denotes pointwise multiplication. We can view this form as stacking two SSMs with multiplicative interaction (each is a "hungry hippo", hence the name of our layer). A more formal connection between linear attention, time-varying systems, and H3 can be found in Appendix B."
    - **Citation:** None.
    - **Relevance:** This section further explains the connection between H3 and linear attention, highlighting the key similarities and differences between these two approaches.
    - **Claim:** "Remembering Key Tokens: Shift and Diagonal SSMs. The shift and diagonal SSMs are designed to address the capability to log tokens after particular events. In the shift SSM, we constrain A ∈ Rm×m to be a shift matrix Ai,j =
The action of this matrix on the hidden state xi is to shift each
0
coordinate down by one thereby creating a "memory" of the previous states. For example, if B = e1, the first basis vector, then xi = [Ui, Ui−1,..., Ui-m+1] contains the inputs from the previous m time steps. We learn B and C (B can also be fixed to e₁ for simplicity, in which case the output is a 1D conv. with kernel size m)."
    - **Citation:** None.
    - **Relevance:** This section explains the role of shift and diagonal SSMs in H3, highlighting their ability to recall past tokens.
    - **Claim:** "The diagonal SSM constrains A to be diagonal and initializes it from the diagonal version of HiPPO (S4D [26]). This parameterization allows the model to remember state over the entire sequence. The shift SSM can detect when a particular event occurs, and the diagonal SSM can remember a token afterwards for the rest of the sequence."
    - **Citation:** [26] Gu, A., Gupta, A., Goel, K., & Ré, C. (2022). On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems.
    - **Relevance:** This citation explains the connection between H3 and S4D, highlighting the use of diagonal SSMs for long-term memory.
    - **Claim:** "Multiplicative Interaction for Comparison. We take the multiplicative interactions from linear attention, but they provide another missing capability when combined with a shift matrix: comparing tokens across the sequence. The multiplicative interactions between the output of the shift SSM and the V projection mimics local multiplicative interactions in linear attention (depending on the size of the hidden state). Similarly, multiplicative interactions with the Q projection and the output of the diagonal SSM allows comparisons between tokens over the entire sequence."
    - **Citation:** None.
    - **Relevance:** This section explains the role of multiplicative interactions in H3, highlighting their ability to compare tokens across the sequence.
    - **Claim:** "H3 Layer. The overall layer is given in Algorithm 1 and shown schematically in Figure 1 (left). We use the H3 layer to construct a model in the same style as Transformers by interleaving it with MLPs, connected by residual connection and layer norm (i.e., pre-norm architecture [2]). We will also consider a hybrid H3-attention model (two attention layers while the rest are H3, Sections 3.3 and 5)."
    - **Citation:** [2] Baevski, A., & Auli, M. (2018). Adaptive input representations for neural language modeling. In International Conference on Learning Representations.
    - **Relevance:** This citation introduces the pre-norm architecture used in the H3 model, highlighting its connection to Transformers.

**2.5 Expressivity:**

- **Key Points:**
    - The authors demonstrate that H3 can solve the synthetic language modeling tasks.
    - They present a hybrid H3-attention model that outperforms Transformers on OpenWebText.
- **Significant Citations:**
    - **Claim:** "We show that H3 can model our synthetic languages, as well as natural language on OpenWebText [23]. We also present a hybrid H3-attention extension that outperforms Transformers on OpenWebText."
    - **Citation:** [23] Gokaslan, A., Cohen, V., Pavlick, E., & Tellex, S. (2019). Openwebtext corpus.
    - **Relevance:** This citation introduces OpenWebText, the dataset used to evaluate the performance of H3 and the hybrid H3-attention model.
    - **Claim:** "Mechanism for Solving Associative Recall with H3. H3 is expressive enough to solve our synthetic language modeling tasks, as shown in Table 2. Figure 1 (middle) shows a mechanism for a single H3 layer to solve the associative recall task for a particular key-value pair (a, 3). The shift SSM and following multiplicative interaction act as a gate on whether to let a value through to the diagonal SSM, based on whether the previous token was key a. The diagonal SSM stores the value 3 in memory, and continually outputs it. The final multiplicative interaction gates whether to let the diagonal SSM's output through-based on whether the current input token is the key a. We formally construct the weights of an H3 layer to solve this task in Appendix D.1."
    - **Citation:** None.
    - **Relevance:** This section explains how H3 can solve the Associative Recall task, highlighting the key mechanisms involved.
    - **Claim:** "Better Synthetic Language Modeling Translates to Better Natural Language Modeling. We validate that when H3 can solve these synthetic tasks, it also improves the modeling capability on natural language (e.g., on the OpenWebText dataset). As shown in Table 3, H3 comes within 0.4 perplexity points of Transformers when trained for 50B tokens on OpenWebText, and performs much better than existing SSM variants (S4D, GSS), by 3 – 3.9 points. Extension: H3-attention Hybrid Model. A simple hybrid H3-attention language model surprisingly outperforms Transformers on OpenWebText by 1.0 point. Our hybrid model simply retains two self-attention layers: one in the second layer, and one in the middle (layer 2 + N/2 for an N-layer model, N even). The H3-attention hybrid also outperforms the GSS-attention hybrid [42]."
    - **Citations:**
        - [42] Mehta, H., Gupta, A., Cutkosky, A., & Neyshabur, B. (2022). Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947.
    - **Relevance:** This section highlights the performance of H3 and the hybrid H3-attention model on OpenWebText, demonstrating their effectiveness in language modeling.

**2.6 Efficiency:**

- **Key Points:**
    - The authors analyze the computational complexity of H3, showing that it scales as O(N log N) with sequence length, which is more efficient than attention's O(N2d) scaling.
- **Significant Citations:**
    - **Claim:** "Efficiency We show that H3 scales as O(N log N) with sequence length N-asymptotically more efficient than attention, which typically requires O(N2d) time and O(N2) space³ (proof in Appendix D.3)."
    - **Citation:** None.
    - **Relevance:** This section introduces the computational complexity analysis of H3, highlighting its efficiency compared to attention.

**2.7 FlashConv: Efficiently Training SSMs:**

- **Key Points:**
    - The authors propose FLASHCONV, a new algorithm for training SSMs efficiently on modern hardware.
    - FLASHCONV combines kernel fusion and block FFT to improve hardware utilization.
    - A state-passing algorithm is introduced to scale FLASHCONV to longer sequences.
- **Significant Citations:**
    - **Claim:** "To improve the efficiency of SSMs on modern hardware, we propose FLASHCONV. FLASHCONV fuses the FFT, pointwise multiply, and inverse FFT to reduce memory reads/writes. It also uses a block FFT algorithm to make use of specialized matrix multiply units (e.g., tensor cores on A100) for sequence lengths up to 8K. For sequences longer than 8K, the computation no longer fits in GPU SRAM¹, so we propose a novel state-passing algorithm that splits the sequence into chunks to compute the FFT convolution one chunk at a time. FLASHCONV can speed up any SSMs (not just H3)."
    - **Citations:**
        - [46] NVIDIA. (2020). Nvidia A100 tensor core GPU architecture.
        - [15] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C. (2022). Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems.
    - **Relevance:** This section introduces FLASHCONV, highlighting its key features and motivations.

**2.8 Fused Block FFTConv:**

- **Key Points:**
    - The authors describe the two key techniques used in FLASHCONV: kernel fusion and block FFT.
    - Kernel fusion reduces IO bottlenecks by fusing the FFT convolution into a single kernel.
    - Block FFT utilizes specialized matrix multiplication units for sequences shorter than 8K.
- **Significant Citations:**
    - **Claim:** "We deploy two techniques to speed up the FFT-based convolution for sequences shorter than 8K: kernel fusion and block FFT. Kernel fusion addresses IO bottlenecks due to reading and writing of intermediate results, while block FFT allows the FFT-based convolution to utilize specialized matrix multiplication units. These techniques allow us to speed up FFTConv by 2× (Section 6) for sequences shorter than 8k."
    - **Citation:** None.
    - **Relevance:** This section explains the two key techniques used in FLASHCONV: kernel fusion and block FFT.
    - **Claim:** "Kernel Fusion. Naive implementations of FFTConv using standard libraries such as cuFFT are IO-bound due to repeated reading and writing of intermediate results. The FFT convolution in an SSM with input u and filter f has the form iFFT(FFT(u) ⊙ FFT(f)) (where denotes pointwise multiplication). There are several memory-efficient algorithms for attention [15, 52], though their time complexity is still quadratic in N, which is a lower-bound for attention [36]."
    - **Citations:**
        - [15] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & Ré, C. (2022). Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems.
        - [52] Rabe, M. N., & Staats, C. (2021). Self-attention does not need O(n²) memory. arXiv preprint arXiv:2112.05682.
        - [36] Keles, F. D., Wijewardena, P. M., & Hegde, C. (2022). On the computational complexity of self-attention. arXiv preprint arXiv:2209.04881.
    - **Relevance:** This section explains the motivation for kernel fusion, highlighting the IO bottlenecks of naive FFTConv implementations.
    - **Claim:** "Block FFT. To further speed up the computation of FFT-based convolution, we exploit specialized matrix multiplication hardware on modern GPUs (e.g., Tensor Cores on Nvidia GPUs perform fast 16 × 16 matrix multiplication). We appeal to classical results that show that the FFT can be written as a series of block-diagonal matrix multiplications interleaved with permutation. We note that such algorithms are not new, but our setting (fused FFTConv on GPU) introduces new bottlenecks-by removing the IO bottlenecks, compute becomes the bottleneck (note that a single FFT on GPU is usually IO bound)."
    - **Citation:** [46] NVIDIA. (2020). Nvidia A100 tensor core GPU architecture.
    - **Relevance:** This section explains the motivation for block FFT, highlighting the use of specialized matrix multiplication units for improved efficiency.

**2.9 State-Passing:**

- **Key Points:**
    - The authors introduce a state-passing algorithm to scale FLASHCONV to sequences longer than 8K.
    - The state-passing algorithm exploits the recurrent properties of SSMs to process the input in chunks.
- **Significant Citations:**
    - **Claim:** "However, the fused kernel cannot run if the sequence is too long to fit into GPU SRAM (longer than 8K on A100). We show how to exploit the particular form of the FFT in SSM to speed it up for long sequences. The recurrent nature of SSMs allows us to split the FFTConv of a length-N sequence into chunks of size N' each (N' is the longest FFT we can fit into SRAM), assuming N is a multiple of N'). We use FFTConv to compute each chunk and use a recurrence to connect the chunks. In particular, we split the inputs u into C = N/N' chunks u(c) ∈ RN' for c = 1, ..., C. Similarly, split the states x into x(c) ∈ RN'×m and the output y into y(c) ∈ RN' for i = 1,..., C. We will only need the end-state x of each chunk c."
    - **Citation:** None.
    - **Relevance:** This section introduces the state-passing algorithm, highlighting its motivation and key features.

**2.10 H3 Evaluation:**

- **Key Points:**
    - The authors evaluate the performance of H3 and the hybrid H3-attention model on various language modeling tasks.
    - Hybrid H3 models outperform Transformers in perplexity and zero/few-shot learning.
    - H3 models retain strong performance on non-text sequence modeling tasks.
- **Significant Citations:**
    - **Claim:** "To understand how well capturing the synthetics in Section 3.1 translates to language modeling, we train two hybrid hybrid H3-attention language models at sizes 125M, 355M, 1.3B, and 2.7B, and we evaluate their performance against Transformers. The hybrid models match or exceed the quality of Transformers in perplexity and zero/few-shot learning. We also validate that H3 models retain strong performance on non-text sequence modeling. Appendix F contains additional experiments on more datasets, length extrapolation, and scaling with data."
    - **Citation:** None.
    - **Relevance:** This section introduces the evaluation of H3 and the hybrid H3-attention model on various language modeling tasks.

**2.11 Language Modeling:**

- **Key Points:**
    - The authors compare the performance of hybrid H3 models against Transformer-based language models.
    - Hybrid H3 models outperform Transformers in perplexity, zero-shot learning, and few-shot learning.
    - Hybrid H3 models generate text 2.4x faster than Transformers.
- **Significant Citations:**
    - **Claim:** "We compare hybrid H3-attention language models against Transformer-based language models. We evaluate language modeling performance using perplexity, zero-shot learning, and few-shot learning performance. Hybrid H3 models outperform Transformers, which suggests that closing the gap between SSMs and attention on the synthetic languages translates to real language modeling capabilities. We also report the generation speed of hybrid H3 models compared to Transformers; since SSMs are recurrent models, they can generate tokens 2.4x faster than Transformers. Appendix F shows performance of pure H3 language models on these same evaluation metrics."
    - **Citation:** None.
    - **Relevance:** This section introduces the comparison of hybrid H3 models against Transformer-based language models.

**2.12 FlashConv Evaluation:**

- **Key Points:**
    - The authors evaluate the performance of FLASHCONV on the long range arena benchmark.
    - FLASHCONV sets state-of-the-art performance on the long range arena benchmark, outperforming Transformers by 5.8x.
    - FLASHCONV demonstrates nearly linear scaling with sequence length.
- **Significant Citations:**
    - **Claim:** "We evaluate how well FLASHCONV speeds up SSMs. FLASHCONV sets state-of-the-art performance on the long range arena benchmark [59] using S4 [25]. We report performance of training H3 module with FLASHCONV compared to attention at various sequence lengths,