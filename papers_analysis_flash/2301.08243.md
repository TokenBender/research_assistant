Okay, here's a comprehensive analysis of the paper "Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture" in Markdown format, following the structure you provided:


# Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture: Citation Analysis

## 1. Introduction

**Title:** Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture

**Authors:** Mahmoud Assran, Quentin Duval, Ishan Misra, Pascal Vincent, Michael Rabbat, Yann LeCun, Piotr Bojanowski, Nicolas Ballas

**Publication Date:** arXiv preprint, April 13, 2023 (v3)

**Main Objective:** This research introduces I-JEPA, a novel non-generative self-supervised learning approach for image representation learning that avoids hand-crafted data augmentations and achieves strong downstream performance.

**Total Number of References:** 79


## 2. Section-by-Section Analysis with Citation Extraction

### 2.1 Introduction

**Summary:** The introduction establishes the context of self-supervised learning in computer vision, highlighting the two main families of approaches: invariance-based and generative methods. It then introduces I-JEPA as a non-generative approach that aims to learn semantic image representations without relying on hand-crafted data augmentations.

**Significant Citations:**

* **Claim:** "In computer vision, there are two common families of approaches for self-supervised learning from images: invariance-based methods [1, 4, 10, 17, 18, 24, 35, 37, 74] and generative methods [8, 28, 36, 57]."
    * **Citation:** 
        * [1] Asano, Y. M., Rupprecht, C., & Vedaldi, A. (2020). Self-labelling via simultaneous clustering and representation learning. *International Conference on Learning Representations*.
        * [4] Assran, M., Caron, M., Misra, I., Bojanowski, P., Bordes, F., Vincent, P., ... & Ballas, N. (2022). Masked Siamese networks for label-efficient learning. *European Conference on Computer Vision*.
        * [10] Bardes, A., Ponce, J., & LeCun, Y. (2021). Vicreg: Variance-invariance-covariance regularization for self-supervised learning. *arXiv preprint arXiv:2105.04906*.
        * [17] Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., & Joulin, A. (2020). Unsupervised learning of visual features by contrasting cluster assignments. *arXiv preprint arXiv:2006.09882*.
        * [18] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. *arXiv preprint arXiv:2104.14294*.
        * [24] Chen, X., & He, K. (2020). Exploring simple Siamese representation learning. *arXiv preprint arXiv:2011.10566*.
        * [35] Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P. H., Buchatskaya, E., ... & Azar, M. G. (2020). Bootstrap your own latent: A new approach to self-supervised learning. *arXiv preprint arXiv:2006.07733*.
        * [37] He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2019). Momentum contrast for unsupervised visual representation learning. *arXiv preprint arXiv:1911.05722*.
        * [74] Zbontar, J., Jing, L., Misra, I., LeCun, Y., & Deny, S. (2021). Barlow twins: Self-supervised learning via redundancy reduction. *arXiv preprint arXiv:2103.03230*.
        * [8] Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., & Auli, M. (2022). Data2vec: A general framework for self-supervised learning in speech, vision and language. *arXiv preprint arXiv:2202.03555*.
        * [28] Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*.
        * [36] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. *IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
        * [57] Pathak, D., Krähenbühl, P., Donahue, J., Darrell, T., & Efros, A. A. (2016). Context encoders: Feature learning by inpainting. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
    * **Relevance:** This citation sets the stage for the paper by outlining the existing landscape of self-supervised learning in computer vision, positioning I-JEPA within this context as a novel approach that addresses limitations of existing methods.


* **Claim:** "Often, it is unclear how to generalize these biases for tasks requiring different levels of abstraction."
    * **Citation:** [11] Bardes, A., Ponce, J., & LeCun, Y. (2022). Vicregl: Self-supervised learning of local visual features. *arXiv preprint arXiv:2210.01571*.
    * **Relevance:** This citation highlights a specific limitation of invariance-based methods, which is the difficulty in generalizing the learned biases to diverse downstream tasks, motivating the need for a more flexible and generalizable approach like I-JEPA.


* **Claim:** "Cognitive learning theories have suggested that a driving mechanism behind representation learning in biological systems is the adaptation of an internal model to predict sensory input responses [31, 59]."
    * **Citation:**
        * [31] Friston, K. (2005). A theory of cortical responses. *Philosophical Transactions of the Royal Society B: Biological Sciences*, *360*(1456), 815-836.
        * [59] Rao, R. P. N., & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. *Nature Neuroscience*, *2*(1), 79-87.
    * **Relevance:** This citation introduces the theoretical foundation for generative methods, which are based on the idea of predicting missing or corrupted sensory inputs. This connection is relevant because I-JEPA, while not generative, draws inspiration from this principle by predicting representations in a latent space.


* **Claim:** "Masked pretraining tasks require less prior knowledge than view-invariance approaches and easily generalize beyond the image modality [8]."
    * **Citation:** [8] Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., & Auli, M. (2022). Data2vec: A general framework for self-supervised learning in speech, vision and language. *arXiv preprint arXiv:2202.03555*.
    * **Relevance:** This citation highlights a key advantage of masked pretraining approaches, which is their ability to generalize across different modalities. This is relevant to I-JEPA because it suggests that the approach could potentially be extended to other domains beyond images.


### 2.2 Background

**Summary:** This section provides a deeper dive into the theoretical underpinnings of self-supervised learning, framing it within the context of Energy-Based Models (EBMs). It then discusses two common architectural paradigms for self-supervised learning: Joint-Embedding Architectures (JEAs) and Generative Architectures. It also introduces Joint-Embedding Predictive Architectures (JEPAs) as a hybrid approach that combines aspects of both JEAs and Generative Architectures.

**Significant Citations:**

* **Claim:** "Self-supervised learning is an approach to representation learning in which a system learns to capture the relationships between its inputs. This objective can be readily described using the framework of Energy-Based Models (EBMs) [49]..."
    * **Citation:** [49] LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). A tutorial on energy-based learning. *Predicting structured data*, *1*(0).
    * **Relevance:** This citation introduces the fundamental concept of EBMs, which provides a unified theoretical framework for understanding self-supervised learning. It's crucial for the paper because it helps to clarify the underlying objective of I-JEPA.


* **Claim:** "Invariance-based pretraining can be cast in the framework of EBMs using a Joint-Embedding Architecture (JEA), which learns to output similar embeddings for compatible inputs, x, y, and dissimilar embeddings for incompatible inputs; see Figure 2a."
    * **Citation:** [20] Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). A simple framework for contrastive learning of visual representations. *arXiv preprint arXiv:2002.05709*.
    * **Relevance:** This citation connects the concept of JEAs to the EBM framework, explaining how invariance-based methods can be understood as a specific instance of EBM optimization. This is important for understanding the relationship between I-JEPA and other self-supervised learning methods.


* **Claim:** "The main challenge with JEAs is representation collapse, wherein the energy landscape is flat (i.e., the encoder produces a constant output regardless of the input)."
    * **Citation:** [15] Bromley, J., Bentz, J. W., Bottou, L., Guyon, I., LeCun, Y., Moore, C., ... & Säckinger, E. (1993). Signature verification using a "siamese" time delay neural network. *International Journal of Pattern Recognition and Artificial Intelligence*, *7*(04), 669-688.
    * **Relevance:** This citation introduces the problem of representation collapse, a common issue in JEAs, which can lead to poor performance. This is important for understanding the design choices made in I-JEPA to mitigate this issue.


* **Claim:** "...several approaches have been investigated to prevent representation collapse, such as contrastive losses that explicitly push apart embeddings of negative examples [15, 24, 37], non-contrastive losses that minimize the informational redundancy across embeddings [10, 74], and clustering-based approaches that maximize the entropy of the average embedding [4, 5, 18]."
    * **Citation:**
        * [15] Bromley, J., Bentz, J. W., Bottou, L., Guyon, I., LeCun, Y., Moore, C., ... & Säckinger, E. (1993). Signature verification using a "siamese" time delay neural network. *International Journal of Pattern Recognition and Artificial Intelligence*, *7*(04), 669-688.
        * [24] Chen, X., & He, K. (2020). Exploring simple Siamese representation learning. *arXiv preprint arXiv:2011.10566*.
        * [37] He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2019). Momentum contrast for unsupervised visual representation learning. *arXiv preprint arXiv:1911.05722*.
        * [10] Bardes, A., Ponce, J., & LeCun, Y. (2021). Vicreg: Variance-invariance-covariance regularization for self-supervised learning. *arXiv preprint arXiv:2105.04906*.
        * [74] Zbontar, J., Jing, L., Misra, I., LeCun, Y., & Deny, S. (2021). Barlow twins: Self-supervised learning via redundancy reduction. *arXiv preprint arXiv:2103.03230*.
        * [4] Assran, M., Caron, M., Misra, I., Bojanowski, P., Bordes, F., Vincent, P., ... & Ballas, N. (2022). Masked Siamese networks for label-efficient learning. *European Conference on Computer Vision*.
        * [5] Assran, M., Caron, M., Misra, I., Bojanowski, P., Joulin, A., Ballas, N., & Rabbat, M. (2021). Semi-supervised learning of visual features by non-parametrically predicting view assignments with support samples. *IEEE/CVF International Conference on Computer Vision*.
        * [18] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. *arXiv preprint arXiv:2104.14294*.
    * **Relevance:** This citation provides a comprehensive overview of techniques used to address the representation collapse problem in JEAs. This is important for understanding the context of I-JEPA's design choices, which also aim to prevent collapse.


* **Claim:** "Generative Architectures learn to directly reconstruct a signal y from a compatible signal x, using a decoder network that is conditioned on an additional (possibly latent) variable z to facilitate reconstruction."
    * **Citation:** [9] Bao, H., Dong, L., & Wei, F. (2021). Beit: Bert pre-training of image transformers. *arXiv preprint arXiv:2106.08254*.
    * **Relevance:** This citation introduces the concept of Generative Architectures, which are based on the idea of reconstructing the input signal. This is relevant to I-JEPA because it highlights a related approach to learning representations and helps to differentiate I-JEPA's approach.


* **Claim:** "Joint-Embedding Predictive Architectures [48] are conceptually similar to Generative Architectures; however, a key difference is that the loss function is applied in embedding space, not input space."
    * **Citation:** [48] LeCun, Y. (2022). A path towards autonomous machine intelligence version 0.9.
    * **Relevance:** This citation introduces the concept of JEPAs, which are the core inspiration for I-JEPA. It's crucial for understanding the paper's contribution because it highlights the key innovation of I-JEPA, which is to predict representations in a latent space rather than reconstructing the input signal.


### 2.3 Method

**Summary:** This section details the proposed I-JEPA architecture, explaining how it predicts the representations of target blocks in an image given a context block. It describes the process of generating targets, selecting the context, and making predictions using a Vision Transformer (ViT) based architecture. It also defines the loss function used for training.

**Significant Citations:**

* **Claim:** "We use a Vision Transformer [29, 63] (ViT) architecture for the context-encoder, target-encoder, and predictor."
    * **Citation:**
        * [29] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Gelly, S. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.
        * [63] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & Jégou, H. (2021). Training data-efficient image transformers & distillation through attention. *International Conference on Machine Learning*.
    * **Relevance:** This citation introduces the core building block of the I-JEPA architecture, the ViT. It's crucial for understanding the technical implementation of the proposed method.


* **Claim:** "Our encoder/predictor architecture is reminiscent of the generative masked autoencoders (MAE) [36] method. However, one key difference is that the I-JEPA method is non-generative and the predictions are made in representation space."
    * **Citation:** [36] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. *IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    * **Relevance:** This citation draws a connection between I-JEPA and MAE, highlighting similarities in the architectural design while emphasizing the key difference that I-JEPA operates in the representation space rather than the pixel space. This helps to clarify the novelty of I-JEPA.


* **Claim:** "The use of an exponential moving average target-encoder has proven essential for training JEAs with Vision Transformers [18, 25, 79], we find the same to be true for I-JEPA."
    * **Citation:**
        * [18] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. *arXiv preprint arXiv:2104.14294*.
        * [25] Chen, X., Xie, S., & He, K. (2021). An empirical study of training self-supervised vision transformers. *arXiv preprint arXiv:2104.02057*.
        * [79] Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., & Kong, T. (2022). Ibot: Image bert pre-training with online tokenizer. *International Conference on Learning Representations*.
    * **Relevance:** This citation justifies the use of an exponential moving average for updating the target encoder weights, a crucial aspect of the training process. It connects I-JEPA to existing practices in the field, demonstrating that the proposed method builds upon established techniques.


### 2.4 Related Work

**Summary:** This section reviews the relevant literature on self-supervised learning, focusing on denoising autoencoders, context encoders, masked image modeling, and joint-embedding architectures. It also discusses the relationship between I-JEPA and other related works, such as data2vec and Context Autoencoders.

**Significant Citations:**

* **Claim:** "A long line of work has explored visual representation learning by predicting the values of missing or corrupted sensory inputs. Denoising autoencoders use random noise as input corruption [67]."
    * **Citation:** [67] Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.-A., & Bottou, L. (2010). Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. *Journal of Machine Learning Research*, *11*(12).
    * **Relevance:** This citation introduces the concept of denoising autoencoders, a foundational approach in self-supervised learning. It's relevant to I-JEPA because it highlights a related approach to learning representations by predicting missing information.


* **Claim:** "The idea of image denoising has recently been revisited in the context of masked image modeling [9, 36, 71], where a Vision Transformer [29] is used to reconstruct missing input patches."
    * **Citation:**
        * [9] Bao, H., Dong, L., & Wei, F. (2021). Beit: Bert pre-training of image transformers. *arXiv preprint arXiv:2106.08254*.
        * [36] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. *IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
        * [71] Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., ... & Hu, H. (2021). Simmim: A simple framework for masked image modeling. *arXiv preprint arXiv:2111.09886*.
        * [29] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Gelly, S. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.
    * **Relevance:** This citation connects I-JEPA to the more recent work on masked image modeling, which has shown promising results in self-supervised learning. It's important for understanding the context of I-JEPA's contribution.


* **Claim:** "Closest to our work is data2vec [8] and Context Autoencoders [25]. The data2vec method learns to predict the representation of missing patches computed through an online target encoder; by avoiding handcrafted augmentations, the method can be applied to diverse modalities with promising results in vision, text and speech."
    * **Citation:**
        * [8] Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., & Auli, M. (2022). Data2vec: A general framework for self-supervised learning in speech, vision and language. *arXiv preprint arXiv:2202.03555*.
        * [25] Chen, X., Xie, S., & He, K. (2021). An empirical study of training self-supervised vision transformers. *arXiv preprint arXiv:2104.02057*.
    * **Relevance:** This citation highlights the most closely related works to I-JEPA, emphasizing the similarities and differences. It's crucial for understanding the novelty of I-JEPA and its contribution to the field.


### 2.5 Image Classification

**Summary:** This section presents the results of I-JEPA on the ImageNet-1K dataset, focusing on linear probing and semi-supervised learning. It compares I-JEPA's performance to other methods, including MAE, data2vec, and view-invariant methods.

**Significant Citations:**

* **Claim:** "Compared to popular methods such as Masked Autoencoders (MAE) [36], Context Autoencoders (CAE) [22], and data2vec [8], which also do not rely on extensive hand-crafted data augmentations during pretraining, we see that I-JEPA significantly improves linear probing performance, while using less computational effort."
    * **Citation:**
        * [36] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. *IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
        * [22] Chen, X., Ding, M., Wang, X., Xin, Y., Mo, S., Wang, Y., ... & Wang, J. (2022). Context autoencoder for self-supervised representation learning. *arXiv preprint arXiv:2202.03026*.
        * [8] Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., & Auli, M. (2022). Data2vec: A general framework for self-supervised learning in speech, vision and language. *arXiv preprint arXiv:2202.03555*.
    * **Relevance:** This citation provides the context for I-JEPA's performance on ImageNet-1K, comparing it to other methods that also avoid hand-crafted augmentations. It's important for understanding the significance of I-JEPA's improved performance.


* **Claim:** "I-JEPA outperforms MAE which also does not rely on hand-crafted data-augmentations during pretraining."
    * **Citation:** [36] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. *IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    * **Relevance:** This citation highlights a specific comparison between I-JEPA and MAE, demonstrating that I-JEPA achieves better performance on the low-shot ImageNet-1K task.


### 2.6 Local Prediction Tasks

**Summary:** This section demonstrates that I-JEPA can also learn low-level image features, achieving strong performance on tasks like object counting and depth prediction. It compares I-JEPA's performance to view-invariant methods on the CLEVR dataset.

**Significant Citations:**

* **Claim:** "Compared to view-invariance methods such as DINO and iBOT, the I-JEPA method effectively captures low-level image features during pretraining and outperforms them in object counting (Clevr/Count) and (by a large margin) depth prediction (Clevr/Dist)."
    * **Citation:**
        * [18] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. *arXiv preprint arXiv:2104.14294*.
        * [79] Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., & Kong, T. (2022). Ibot: Image bert pre-training with online tokenizer. *International Conference on Learning Representations*.
        * [42] Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C. L., & Girshick, R. (2017). Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
    * **Relevance:** This citation provides the context for I-JEPA's performance on low-level tasks, comparing it to view-invariant methods. It's important for understanding the broader impact of I-JEPA's ability to learn both high-level and low-level features.


### 2.7 Scalability

**Summary:** This section explores the scalability of I-JEPA, demonstrating its efficiency in terms of computational resources and its ability to benefit from larger datasets and model sizes.

**Significant Citations:**

* **Claim:** "I-JEPA requires less compute than previous methods and achieves strong performance without relying on hand-crafted data-augmentations."
    * **Citation:** [36] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. *IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    * **Relevance:** This citation provides a comparison of I-JEPA's computational efficiency to other methods, highlighting its advantage in terms of resource usage.


* **Claim:** "Compared to view-invariance based methods, such as iBOT, which rely on hand-crafted data augmentations to create and process multiple views of each image, I-JEPA also runs significantly faster."
    * **Citation:** [79] Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., & Kong, T. (2022). Ibot: Image bert pre-training with online tokenizer. *International Conference on Learning Representations*.
    * **Relevance:** This citation highlights another aspect of I-JEPA's scalability, comparing its speed to view-invariant methods. It's important for understanding the practical benefits of I-JEPA.


### 2.8 Predictor Visualizations

**Summary:** This section provides a qualitative analysis of the predictor's learned representations using the RCDM framework. It aims to understand how the predictor captures positional uncertainty and learns to predict target block representations.

**Significant Citations:**

* **Claim:** "To visualize the representations of a pretrained neural network in pixel space, we use the RCDM framework [13]."
    * **Citation:** [13] Bordes, F., Balestriero, R., & Vincent, P. (2022). High fidelity visualization of what your self-supervised representation knows about. *Transactions on Machine Learning Research*.
    * **Relevance:** This citation introduces the RCDM framework, a technique used to visualize the learned representations of a neural network. It's crucial for understanding the methodology used in this section.


### 2.9 Ablations

**Summary:** This section presents a series of ablation studies to investigate the impact of different design choices in I-JEPA on its performance. It explores the effect of target block size, context size, number of targets, and masking strategy.

**Significant Citations:**

* **Claim:** "We conjecture that a crucial component of I-JEPA is that the loss is computed entirely in representation space, thereby giving the target encoder the ability to produce abstract prediction targets, for which irrelevant pixel-level details are eliminated."
    * **Citation:** [8] Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., & Auli, M. (2022). Data2vec: A general framework for self-supervised learning in speech, vision and language. *arXiv preprint arXiv:2202.03555*.
    * **Relevance:** This claim connects the choice of predicting in representation space to the ability of I-JEPA to learn more semantic features. It's important for understanding the rationale behind this design choice.


### 2.10 Conclusion

**Summary:** The conclusion summarizes the key contributions of the paper, highlighting the simplicity and efficiency of I-JEPA, its ability to learn semantic representations without hand-crafted augmentations, and its faster convergence compared to pixel-reconstruction methods.

**Significant Citations:** (None directly in the conclusion, but the overall argument is supported by the previously cited works.)


### 2.11 Appendix A: Implementation Details

**Summary:** This section provides detailed information about the implementation of I-JEPA, including the architectures used, optimization techniques, masking strategy, and downstream task evaluation protocols.

**Significant Citations:**

* **Claim:** "For I-JEPA pretraining, we use Vision Transformer [29] (ViT) architectures for the context-encoder, target-encoder, and the predictor."
    * **Citation:** [29] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Gelly, S. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.
    * **Relevance:** This citation reiterates the use of ViT as the backbone architecture for I-JEPA, providing a crucial technical detail for understanding the implementation.


* **Claim:** "We use AdamW [51] to optimize the context-encoder and predictor weights."
    * **Citation:** [51] Loshchilov, I., & Hutter, F. (2017). Decoupled weight decay regularization. *arXiv preprint arXiv:1711.05101*.
    * **Relevance:** This citation specifies the optimizer used for training I-JEPA, providing a key detail for reproducibility.


* **Claim:** "When evaluating methods such as iBOT [79], DINO [18] or MAE [36], which leverage Vision Transformers [29] with an additional [cls] token, we use the default configurations of VISSL [34] to evaluate all the models on iNaturalist18 [65], CIFAR100 [45], Clevr/Count [42, 75], Clevr/Dist [42, 75], and Places205 [78]."
    * **Citation:**
        * [79] Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., & Kong, T. (2022). Ibot: Image bert pre-training with online tokenizer. *International Conference on Learning Representations*.
        * [18] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., & Joulin, A. (2021). Emerging properties in self-supervised vision transformers. *arXiv preprint arXiv:2104.14294*.
        * [36] He, K., Chen, X., Xie, S., Li, Y., Dollar, P., & Girshick, R. (2022). Masked autoencoders are scalable vision learners. *IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
        * [29] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Gelly, S. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.
        * [34] Goyal, P., Duval, Q., Reizenstein, J., Leavitt, M., Xu, M., Lefaudeux, B., ... & Misra, I. (2021). Vissl.
        * [65] Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., ... & Belongie, S. (2018). The iNaturalist species classification and detection dataset. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
        * [45] Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.
        * [42] Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C. L., & Girshick, R. (2017). Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*.
        * [78] Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., & Oliva, A. (2014). Learning deep features for scene recognition using places database. *Advances in Neural Information Processing Systems*.
    * **Relevance:** This citation details the evaluation protocols used for comparing I-JEPA to other methods, providing crucial information for understanding the experimental setup.


### 2.12 Appendix B: Broader Related Work

**Summary:** This section provides a more in-depth discussion of the broader context of self-supervised learning, focusing on joint-embedding architectures, regularization techniques, and the InfoMax principle. It also discusses the relationship between I-JEPA and other related works, such as contrastive predictive coding and iBOT.

**Significant Citations:**

* **Claim:** "Self-supervised learning of visual representations with joint-embedding architectures is an active line of research [3, 10, 12, 18, 23, 24, 35, 37, 54, 69, 79]."
    * **Citation:**
        * [3] Assran, M., Ballas, N., Castrejon, L., & Rabbat, M. (2020). Supervision accelerates pre-training in contrastive semi-supervised learning of visual representations. *NeurIPS Workshop on Self-Supervised Learning*.
        * [10] Bardes, A., Ponce, J., & LeCun, Y. (2021). Vicreg: Variance-invariance-covariance regularization for self-