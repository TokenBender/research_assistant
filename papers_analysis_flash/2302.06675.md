## Analysis of "Symbolic Discovery of Optimization Algorithms"

**1. Introduction:**

- **Title:** Symbolic Discovery of Optimization Algorithms
- **Authors:** Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Yifeng Lu, Xuanyi Dong, Quoc V. Le, Cho-Jui Hsieh, and Thang Luong
- **Publication Date:** 8 May 2023 (v4)
- **Objective:** The paper proposes a method to discover optimization algorithms for deep neural network training by formulating the process as program search.
- **Number of References:** 83

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:**
    - The authors highlight the importance of optimization algorithms in deep learning, particularly for training large language models, vision models, and multimodal models.
    - They mention the widespread use of AdamW and Adafactor as de facto standard optimizers.
    - The authors introduce the concept of automatically discovering optimization algorithms as a research direction.
    - They discuss the limitations of existing approaches like learning to optimize (L2O) and reinforcement learning-based methods.
    - They introduce their novel approach, which formulates algorithm discovery as program search.

- **Significant Citations:**

    - **Claim:** "Adam (Kingma and Ba, 2014) with decoupled weight decay (Loshchilov and Hutter, 2019), also referred to as AdamW, and Adafactor with factorized second moments (Shazeer and Stern, 2018), are still the de facto standard optimizers for training most deep neural networks, especially the recent state-of-the-art language (Brown et al., 2020; Devlin et al., 2019; Vaswani et al., 2017), vision (Dai et al., 2021; Dosovitskiy et al., 2021; Zhai et al., 2021) and multimodal (Radford et al., 2021; Saharia et al., 2022; Yu et al., 2022) models."
    - **Citation:** Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*.
    - **Explanation:** This citation introduces Adam, a widely used optimizer, which forms the basis for AdamW, a variant with decoupled weight decay.
    - **Citation:** Loshchilov, I., & Hutter, F. (2019). Decoupled weight decay regularization. *arXiv preprint arXiv:1711.05101*.
    - **Explanation:** This citation introduces the concept of decoupled weight decay, a common modification to Adam.
    - **Citation:** Shazeer, N., & Stern, M. (2018). Adafactor: Adaptive learning rates with sublinear memory cost. *arXiv preprint arXiv:1804.04235*.
    - **Explanation:** This citation introduces Adafactor, another widely used optimizer, which is often compared to AdamW.
    - **Citation:** Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Amodei, D. (2020). Language models are few-shot learners. *arXiv preprint arXiv:2005.14165*.
    - **Explanation:** This citation highlights the importance of optimization algorithms for training large language models, which are a key focus of the paper.
    - **Citation:** Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. *arXiv preprint arXiv:1810.04805*.
    - **Explanation:** This citation highlights the importance of optimization algorithms for training large language models, which are a key focus of the paper.
    - **Citation:** Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Kaiser, Ł. (2017). Attention is all you need. *arXiv preprint arXiv:1706.03762*.
    - **Explanation:** This citation highlights the importance of optimization algorithms for training large language models, which are a key focus of the paper.
    - **Citation:** Dai, Z., Liu, H., Le, Q. V., & Tan, M. (2021). Coatnet: Marrying convolution and attention for all data sizes. *arXiv preprint arXiv:2103.00112*.
    - **Explanation:** This citation highlights the importance of optimization algorithms for training large vision models, which are a key focus of the paper.
    - **Citation:** Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. *arXiv preprint arXiv:2010.11929*.
    - **Explanation:** This citation highlights the importance of optimization algorithms for training large vision models, which are a key focus of the paper.
    - **Citation:** Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., ... & Beyer, L. (2022). LiT: Zero-shot transfer with locked-image text tuning. *arXiv preprint arXiv:2112.13447*.
    - **Explanation:** This citation highlights the importance of optimization algorithms for training large vision models, which are a key focus of the paper.
    - **Citation:** Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). Learning transferable visual models from natural language supervision. *arXiv preprint arXiv:2103.00020*.
    - **Explanation:** This citation highlights the importance of optimization algorithms for training large multimodal models, which are a key focus of the paper.
    - **Citation:** Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., ... & Norouzi, M. (2022). Photorealistic text-to-image diffusion models with deep language understanding. *arXiv preprint arXiv:2112.09841*.
    - **Explanation:** This citation highlights the importance of optimization algorithms for training large multimodal models, which are a key focus of the paper.
    - **Citation:** Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., & Wu, Y. (2022). Coca: Contrastive captioners are image-text foundation models. *arXiv preprint arXiv:2112.09437*.
    - **Explanation:** This citation highlights the importance of optimization algorithms for training large multimodal models, which are a key focus of the paper.
    - **Citation:** Andrychowicz, M., Denil, M., Gómez, S., Hoffman, M. W., Pfau, D., Schaul, T., ... & Freitas, N. de. (2016). Learning to learn by gradient descent by gradient descent. *arXiv preprint arXiv:1606.04474*.
    - **Explanation:** This citation introduces the concept of learning to optimize (L2O), which is a common approach to automatically discovering optimizers.
    - **Citation:** Bello, I., Zoph, B., Vasudevan, V., & Le, Q. V. (2017). Neural optimizer search with reinforcement learning. *arXiv preprint arXiv:1709.07417*.
    - **Explanation:** This citation introduces a reinforcement learning-based approach to automatically discovering optimizers.
    - **Citation:** Metz, L., Maheswaranathan, N., Nixon, J., Freeman, D., & Sohl-Dickstein, J. (2019). Understanding and correcting pathologies in the training of learned optimizers. *arXiv preprint arXiv:1803.02892*.
    - **Citation:** Metz, L., Harrison, J., Freeman, C. D., Merchant, A., Beyer, L., Bradbury, J., ... & Sohl-Dickstein, J. (2022). Velo: Training versatile learned optimizers by scaling up. *arXiv preprint arXiv:2107.01131*.
    - **Citation:** Real, E., Aggarwal, A., Huang, Y., & Le, Q. V. (2019). Regularized evolution for image classifier architecture search. *arXiv preprint arXiv:1802.07071*.
    - **Citation:** Real, E., Liang, C., So, D., & Le, Q. V. (2020). AutoML-Zero: Evolving machine learning algorithms from scratch. *arXiv preprint arXiv:1909.11880*.
    - **Citation:** Wang, H., Ge, S., Lipton, Z., & Xing, E. P. (2019). Learning robust global representations by penalizing local predictive power. *arXiv preprint arXiv:1812.03427*.

**2.2 Symbolic Discovery of Algorithms:**

- **Key Points:**
    - The authors propose using a symbolic representation in the form of programs for algorithm discovery.
    - They argue that this approach aligns with the fact that algorithms need to be implemented as programs.
    - They highlight the advantages of symbolic representations for analysis, comprehension, and transferability.
    - They define the program search space, including the input/output signature, building blocks, and mutation operations.
    - They discuss the challenges of infinite and sparse search spaces.

- **Significant Citations:**

    - **Claim:** "We present an approach that formulates algorithm discovery as program search (Brameier et al., 2007; Koza, 1994; Real et al., 2020)."
    - **Citation:** Brameier, M., Banzhaf, W., & Banzhaf, W. (2007). *Linear genetic programming, volume 1*. Springer.
    - **Explanation:** This citation introduces the concept of genetic programming, a common approach to program search.
    - **Citation:** Koza, J. R. (1994). *Genetic programming as a means for programming computers by natural selection*. Statistics and computing, 4(1), 87-112.
    - **Explanation:** This citation introduces the concept of genetic programming, a common approach to program search.
    - **Citation:** Real, E., Liang, C., So, D., & Le, Q. V. (2020). AutoML-Zero: Evolving machine learning algorithms from scratch. *arXiv preprint arXiv:1909.11880*.
    - **Explanation:** This citation introduces AutoML-Zero, a recent work that attempts to search every component of a machine learning pipeline.
    - **Claim:** "Another direction is to automatically discover such optimization algorithms. The learning to optimize (L2O) approach proposes to discover optimizers by training parameterized models, e.g., neural networks, to output the updates (Andrychowicz et al., 2016; Li and Malik, 2017; Metz et al., 2019, 2022)."
    - **Citation:** Andrychowicz, M., Denil, M., Gómez, S., Hoffman, M. W., Pfau, D., Schaul, T., ... & Freitas, N. de. (2016). Learning to learn by gradient descent by gradient descent. *arXiv preprint arXiv:1606.04474*.
    - **Explanation:** This citation introduces the concept of learning to optimize (L2O), which is a common approach to automatically discovering optimizers.
    - **Citation:** Li, K., & Malik, J. (2017). Learning to optimize. *arXiv preprint arXiv:1606.01883*.
    - **Explanation:** This citation introduces another work on learning to optimize.
    - **Citation:** Metz, L., Maheswaranathan, N., Nixon, J., Freeman, D., & Sohl-Dickstein, J. (2019). Understanding and correcting pathologies in the training of learned optimizers. *arXiv preprint arXiv:1803.02892*.
    - **Explanation:** This citation introduces another work on learning to optimize.
    - **Citation:** Metz, L., Harrison, J., Freeman, C. D., Merchant, A., Beyer, L., Bradbury, J., ... & Sohl-Dickstein, J. (2022). Velo: Training versatile learned optimizers by scaling up. *arXiv preprint arXiv:2107.01131*.
    - **Explanation:** This citation introduces another work on learning to optimize.
    - **Claim:** "Another line of methods (Bello et al., 2017; Wang et al., 2022) apply reinforcement learning or Monte Carlo Sampling to discover new optimizers, where the search space is defined by trees composed from predefined operands (e.g., gradient and momentum) and operators (e.g., unary and binary math operations)."
    - **Citation:** Bello, I., Zoph, B., Vasudevan, V., & Le, Q. V. (2017). Neural optimizer search with reinforcement learning. *arXiv preprint arXiv:1709.07417*.
    - **Explanation:** This citation introduces a reinforcement learning-based approach to automatically discovering optimizers.
    - **Citation:** Wang, R., Xiong, Y., Cheng, M., & Hsieh, C. J. (2022). Efficient non-parametric optimizer search for diverse tasks. *arXiv preprint arXiv:2209.13575*.
    - **Explanation:** This citation introduces a recent work on using reinforcement learning to discover optimizers.
    - **Claim:** "AutoML-Zero (Real et al., 2020) is an ambitious effort that attempts to search every component of a machine learning pipeline while evaluating on toy tasks."
    - **Citation:** Real, E., Liang, C., So, D., & Le, Q. V. (2020). AutoML-Zero: Evolving machine learning algorithms from scratch. *arXiv preprint arXiv:1909.11880*.
    - **Explanation:** This citation introduces AutoML-Zero, a recent work that attempts to search every component of a machine learning pipeline.

**2.3 Program Search Space:**

- **Key Points:**
    - The authors describe the design criteria for the program search space, emphasizing flexibility, analyzability, and focus on high-level algorithmic design.
    - They define the program structure as a sequence of assignment statements using functions operating on n-dimensional arrays.
    - They detail the building blocks of the programs, including common math functions and linear algebra operations.
    - They explain the mutation operations used in the evolutionary search process.
    - They acknowledge the infinite and sparse nature of the search space and highlight the challenge of finding high-performing programs.

- **Significant Citations:**

    - **Claim:** "We adhere to the following three criteria while designing the program search space: (1) the search space should be flexible enough to enable the discovery of novel algorithms; (2) the programs should be easy to analyze and incorporate into a machine learning workflow; (3) the programs should focus on the high-level algorithmic design rather than low-level implementation details."
    - **Explanation:** This section outlines the design criteria for the program search space, which are crucial for ensuring the effectiveness of the search process.
    - **Claim:** "We define the programs to contain functions operating over n-dimensional arrays, including structures like lists and dictionaries containing such arrays, in an imperative language. They are similar to Python code using NumPy / JAX (Bradbury et al., 2018; Harris et al., 2020) as well as pseudo code of optimization algorithms."
    - **Citation:** Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., ... & Zhang, Q. (2018). JAX: composable transformations of Python+NumPy programs. *arXiv preprint arXiv:1803.02892*.
    - **Explanation:** This citation introduces JAX, a library for high-performance numerical computation in Python, which is relevant to the paper's discussion of program representation.
    - **Citation:** Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., ... & Oliphant, T. E. (2020). Array programming with NumPy. *Nature*, 585(7825), 357-362.
    - **Explanation:** This citation introduces NumPy, a library for numerical computation in Python, which is relevant to the paper's discussion of program representation.
    - **Claim:** "The design of mutations utilized in evolutionary search is tightly intertwined with the representation of the program. We include three types of mutations: (1) inserting a new statement at a random location with randomly chosen functions and arguments, (2) deleting a random chosen statement, and (3) modifying a random statement by randomly altering one of its function arguments, which may be either variables or constants."
    - **Explanation:** This section describes the mutation operations used in the evolutionary search process, which are crucial for exploring the search space and finding new solutions.
    - **Claim:** "To mutate an argument, we replace it with an existing variable or a newly generated constant obtained by sampling from a normal distribution X ~ N(0, 1). Additionally, we can mutate an existing constant by multiplying it by a random factor 2a, where a ~ N(0, 1)."
    - **Explanation:** This section provides further details on the mutation operations, specifically how constants are mutated.
    - **Claim:** "These constants serve as tunable hyperparameters in the optimization algorithm, such as the peak learning rate and weight decay in AdamW."
    - **Explanation:** This section highlights the importance of constants in the program representation, as they can serve as tunable hyperparameters.
    - **Claim:** "Note that we allow a program to include redundant statements during search, i.e., statements that do not impact the final program outputs. This is necessary as mutations are limited to only affecting a single statement. Redundant statements therefore serve as intermediate steps towards future substantial modifications in the program."
    - **Explanation:** This section discusses the role of redundant statements in the search process, which can be helpful for exploring the search space and finding new solutions.

**2.4 Efficient Search Techniques:**

- **Key Points:**
    - The authors describe the efficient search techniques they employ, including regularized evolution with warm-start and restart, and pruning through abstract execution.
    - They explain the benefits of regularized evolution for exploring the search space and finding high-performing programs.
    - They highlight the importance of restarting the search from the best program found so far to further optimize it.
    - They discuss the use of abstract execution for pruning redundant statements and detecting errors in programs.
    - They introduce the concept of proxy tasks for reducing the search cost.

- **Significant Citations:**

    - **Claim:** "We employ the following techniques to address the challenges posed by the infinite and sparse space."
    - **Explanation:** This section introduces the efficient search techniques used in the paper, which are crucial for overcoming the challenges of the infinite and sparse search space.
    - **Claim:** "We apply regularized evolution as it is simple, scalable, and has shown success on many AutoML search tasks (Holland, 1992; Real et al., 2019, 2020; So et al., 2019; Ying et al., 2019)."
    - **Citation:** Holland, J. H. (1992). *Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence*. MIT press.
    - **Explanation:** This citation introduces regularized evolution, a common approach to program search.
    - **Citation:** Real, E., Aggarwal, A., Huang, Y., & Le, Q. V. (2019). Regularized evolution for image classifier architecture search. *arXiv preprint arXiv:1802.07071*.
    - **Explanation:** This citation introduces a recent work on using regularized evolution for AutoML.
    - **Citation:** Real, E., Liang, C., So, D., & Le, Q. V. (2020). AutoML-Zero: Evolving machine learning algorithms from scratch. *arXiv preprint arXiv:1909.11880*.
    - **Explanation:** This citation introduces AutoML-Zero, a recent work that attempts to search every component of a machine learning pipeline.
    - **Citation:** So, D., Le, Q., & Liang, C. (2019). The evolved transformer. *arXiv preprint arXiv:1905.11665*.
    - **Explanation:** This citation introduces a recent work on using regularized evolution for AutoML.
    - **Citation:** Ying, C., Klein, A., Real, E., Christiansen, M., Murphy, K., & Hutter, F. (2019). Nas-bench-101: Towards reproducible neural architecture search. *arXiv preprint arXiv:1902.09665*.
    - **Explanation:** This citation introduces a recent work on using regularized evolution for AutoML.
    - **Claim:** "It keeps a population of P algorithms that are gradually improved through cycles. Each cycle picks T<P algorithms at random and the best performer is chosen as the parent, i.e., tournament selection (Goldberg and Deb, 1991)."
    - **Citation:** Goldberg, D. E., & Deb, K. (1991). A comparative analysis of selection schemes used in genetic algorithms. *Proceedings of the First International Conference on Genetic Algorithms*, 24-29.
    - **Explanation:** This citation introduces tournament selection, a common selection method used in evolutionary algorithms.
    - **Claim:** "To further improve the search efficiency, we apply two types of restart: (1) restarting from the initial program, which can lead to different local optima due to the randomness in evolution and encourage exploration. This can be done by running multiple searches in parallel. (2) restarting from the best algorithm found thus far to further optimize it, encouraging exploitation."
    - **Explanation:** This section discusses the use of restarting the search process, which can be helpful for exploring the search space and finding new solutions.
    - **Claim:** "We propose to prune the redundancies in the program space from three sources: programs with syntax or type / shape errors, functionally equivalent programs, and redundant statements in the programs."
    - **Explanation:** This section introduces the concept of pruning the search space, which can be helpful for reducing the search cost and finding high-performing programs.
    - **Claim:** "Before a program is actually executed, we perform an abstract execution step that (1) infers variable types and shapes to detect programs with errors, and keeps mutating the parent program until a valid child program is generated; (2) produces a hash that uniquely identifies how the outputs are computed from the inputs, allowing us to cache and look up semantically duplicate programs (Gillard et al., 2023); (3) identifies redundant statements that can be ignored during actual execution and analysis."
    - **Citation:** Gillard, R., Jonany, S., Miao, Y., Munn, M., de Souza, C., Dungay, J., ... & Real, E. (2023). Unified functional hashing in automatic machine learning. *arXiv preprint arXiv:2209.09665*.
    - **Explanation:** This citation introduces a recent work on using functional hashing for program analysis.
    - **Claim:** "To reduce search cost, we create low-cost proxies by decreasing the model size, number of training examples, and steps from the target tasks. Evaluation on the proxies can be completed on one TPU V2 chip within 20min. We use the accuracy or perplexity on the validation set as the fitness."
    - **Explanation:** This section introduces the concept of proxy tasks, which are used to reduce the search cost and find high-performing programs.

**2.5 Generalization: Program Selection and Simplification:**

- **Key Points:**
    - The authors discuss the challenge of generalizing programs discovered on proxy tasks to larger, more complex tasks.
    - They introduce the concept of meta-validation tasks for evaluating the generalization ability of programs.
    - They highlight the phenomenon of meta-overfitting and its impact on generalization.
    - They describe the funnel selection process for selecting programs that generalize well to larger tasks.
    - They explain the importance of program simplification for improving generalization.

- **Significant Citations:**

    - **Claim:** "The search experiments can discover promising programs on proxy tasks. We use performance on meta-validation tasks that are larger than the proxy tasks by increasing the model size and training steps, to select the programs that generalize beyond proxy tasks then further simplify them."
    - **Explanation:** This section introduces the concept of meta-validation tasks, which are used to evaluate the generalization ability of programs.
    - **Claim:** "The discovered algorithms face a significant challenge due to the substantial gap between the proxy tasks during search and the target tasks. While proxy tasks can typically be completed within 20min on one TPU V2 chip, target tasks can be > 10⁴x larger and require days of training on 512 TPU V4 chips."
    - **Explanation:** This section highlights the challenge of generalizing programs discovered on proxy tasks to larger, more complex tasks.
    - **Claim:** "Furthermore, we expect the optimizer to perform well on different architectures, datasets and even different domains, so the discovered algorithms need to show strong out-of-distribution generalization."
    - **Explanation:** This section emphasizes the importance of out-of-distribution generalization for optimization algorithms.
    - **Claim:** "The sparse search space and inherent noise in the evolution process further compound this challenge, leading to inconsistent generalization properties between different runs."
    - **Explanation:** This section discusses the challenges of the sparse search space and inherent noise in the evolution process, which can impact the generalization ability of programs.
    - **Claim:** "Our observation suggests that evolutionary search experiments that meta-overfit later tend to uncover optimization algorithms that generalize better."
    - **Explanation:** This section highlights the importance of meta-overfitting happening later in the search process for finding programs that generalize well.
    - **Claim:** "To save compute, we apply a funnel selection process that gradually increases the scale of the meta-validation tasks."
    - **Explanation:** This section introduces the funnel selection process, which is used to select programs that generalize well to larger tasks.
    - **Claim:** "Simpler programs are easier to understand and our intuition is that they are more likely to generalize, so we simplify the programs with the following steps."
    - **Explanation:** This section discusses the importance of program simplification for improving generalization.

**3. Derivation and Analysis of Lion:**

- **Key Points:**
    - The authors describe the derivation of Lion, a simple and effective optimization algorithm discovered through their program search process.
    - They highlight the key features of Lion, including its memory efficiency, sign operation for uniform update magnitude, and momentum tracking.
    - They analyze the benefits of the sign operation for regularization and smoother convergence.
    - They discuss the hyperparameter choices for Lion and its relationship to batch size.
    - They compare Lion to existing optimizers like AdamW, Adafactor, signSGD, and NAdam.

- **Significant Citations:**

    - **Claim:** "The search and funnel selection process lead to Program 4, which is obtained by automatically removing redundant statements from the raw Program 8 (in the Appendix). We further simplify it to get the final algorithm (Lion) in Program 1."
    - **Explanation:** This section describes the derivation of Lion, which is obtained through a simplification process from a more complex program discovered through the search process.
    - **Claim:** "Intuitively, the sign operation adds noise to the updates, which acts as a form of regularization and helps with generalization (Chen et al., 2022; Foret et al., 2021; Neelakantan et al., 2017)."
    - **Citation:** Chen, X., Hsieh, C. J., & Gong, B. (2022). When vision transformers outperform resnets without pre-training or strong data augmentations. *arXiv preprint arXiv:2106.01569*.
    - **Explanation:** This citation introduces the concept of using the sign operation for regularization, which is a common technique in deep learning.
    - **Citation:** Foret, P., Kleiner, A., Mobahi, H., & Neyshabur, B. (2021). Sharpness-aware minimization for efficiently improving generalization. *arXiv preprint arXiv:2010.01412*.
    - **Explanation:** This citation introduces the concept of sharpness-aware minimization, which is related to the idea of using the sign operation for regularization.
    - **Citation:** Neelakantan, A., Vilnis, L., Le, Q. V., Kaiser, L., Kurach, K., Sutskever, I., & Martens, J. (2017). Adding gradient noise improves learning for very deep networks. *arXiv preprint arXiv:1706.05154*.
    - **Explanation:** This citation introduces the concept of adding gradient noise, which is related to the idea of using the sign operation for regularization.
    - **Claim:** "The default EMA factor used to track the momentum in Lion is 0.99 (β2), compared to the commonly used 0.9 in AdamW and momentum SGD. The current gradient and momentum are interpolated with a factor of 0.9 (β1) before the sign operation is applied."
    - **Explanation:** This section describes the momentum tracking mechanism used in Lion, which is different from the commonly used approach in AdamW and momentum SGD.
    - **Claim:** "Lion is simpler and has fewer hyperparameters compared to AdamW and Adafactor as it does not require e and factorization-related ones. The update is an element-wise binary ±1 if we omit the weight decay term, with larger norm than those produced by other optimizers like SGD and adaptive algorithms."
    - **Explanation:** This section highlights the simplicity of Lion compared to other optimizers, which is a key advantage.
    - **Claim:** "As a result, Lion needs a smaller learning rate and in turn a larger decoupled weight decay to achieve a similar effective weight decay strength (lr * λ)."
    - **Explanation:** This section discusses the hyperparameter choices for Lion, which are different from those used for other optimizers.
    - **Claim:** "The advantage of Lion over AdamW enlarges as the batch size increases, which fits the common practice of scaling up model training through data parallelism (Section 4.6)."
    - **Explanation:** This section highlights the relationship between Lion's performance and batch size, which is an important consideration for training large models.
    - **Claim:** "Lion only saves the momentum thus has smaller memory footprint than popular adaptive optimizers like AdamW, which is beneficial when training large models and / or using a large batch size."
    - **Explanation:** This section highlights the memory efficiency of Lion compared to other optimizers, which is a key advantage.
    - **Claim:** "Another practical benefit is that Lion has faster runtime (steps / sec) in our experiments due to its simplicity, usually 2-15% speedup compared to AdamW and Adafactor depending on the task, codebase, and hardware."
    - **Explanation:** This section highlights the runtime efficiency of Lion compared to other optimizers, which is another key advantage.
    - **Claim:** "The sign operation has been explored in previous optimizers (Bernstein et al., 2018; Riedmiller and Braun, 1993)."
    - **Citation:** Bernstein, J., Wang, Y. X., Azizzadenesheli, K., & Anandkumar, A. (2018). signSGD: Compressed optimization for non-convex problems. *arXiv preprint arXiv:1802.09571*.
    - **Explanation:** This citation introduces signSGD, a previous optimizer that uses the sign operation.
    - **Citation:** Riedmiller, M., & Braun, H. (1993). A direct adaptive method for faster backpropagation learning: the rprop algorithm. *IEEE International Conference on Neural Networks*, 1, 586-591.
    - **Explanation:** This citation introduces another previous optimizer that uses the sign operation.
    - **Claim:** "The closest to ours is the handcrafted optimizer signSGD (Bernstein et al., 2018) (and its momentum variant) that also utilizes the sign operation to calculate the update but has a different momentum update rule from Lion."
    - **Citation:** Bernstein, J., Wang, Y. X., Azizzadenesheli, K., & Anandkumar, A. (2018). signSGD: Compressed optimization for non-convex problems. *arXiv preprint arXiv:1802.09571*.
    - **Explanation:** This citation introduces signSGD, a previous optimizer that uses the sign operation.
    - **Claim:** "Their focus is to mitigate communication costs between agents in distributed training, and they observe inferior performance when training ConvNets on image classification tasks."
    - **Explanation:** This section highlights the difference between Lion and signSGD, which is designed for distributed training.
    - **Claim:** "On the other hand, NAdam (Dozat, 2016) combines the updated first moment and the gradient to compute the update, but Lion decouples the momentum tracking and how it is applied to the update through β2."
    - **Citation:** Dozat, T. (2016). Incorporating Nesterov Momentum into Adam. *arXiv preprint arXiv:1606.01883*.
    - **Explanation:** This citation introduces NAdam, another previous optimizer that uses momentum.

**4. Evaluation of Lion:**

- **Key Points:**
    - The authors evaluate Lion on various benchmarks, including image classification, vision-language contrastive learning, diffusion models, language modeling, and fine-tuning.
    - They compare Lion to AdamW (or Adafactor) as the de facto standard optimizer.
    - They highlight the strong performance of Lion across different tasks and model architectures.
    - They analyze the properties and limitations of Lion, including its sensitivity to batch size and hyperparameters.

- **Significant Citations:**

    - **Claim:** "In this section, we present evaluations of Lion, on various benchmarks. We mainly compare it to AdamW (or Adafactor when memory is a bottleneck) as it is exceedingly popular and the de facto standard optimizer on a majority of learning tasks."
    - **Explanation:** This section introduces the evaluation setup for Lion, which involves comparing it to AdamW (or Adafactor) as the de facto standard optimizer.
    - **Claim:** "We also benchmark other popular optimizers in Section 4.5, including handcrafted and automatically discovered ones."
    - **Explanation:** This section mentions that the authors also evaluate Lion against other popular optimizers, both handcrafted and automatically discovered.
    - **Claim:** "We make sure that every optimizer is well-tuned for each task (see Section 5 for tuning details)."
    - **Explanation:** This section emphasizes the importance of proper hyperparameter tuning for all optimizers in the evaluation.
    - **Claim:** "By default, the learning rate schedule is cosine decay with 10K steps warmup, and the momentum is saved as bfloat16 to reduce the memory footprint."
    - **Explanation:** This section describes the default settings used for the learning rate schedule and momentum in the evaluation.

**4.1 Image Classification:**

- **Key Points:**
    - The authors evaluate Lion on the image classification task, training models from scratch on ImageNet, pre-training on ImageNet-21K, and pre-training on JFT.
    - They demonstrate that Lion consistently outperforms AdamW across different model architectures and training settings.
    - They highlight the significant performance gains of Lion on larger models and with fewer inductive biases.
    - They analyze the impact of strong augmentations and batch size on the performance of Lion.

- **Significant Citations:**

    - **Claim:** "Following previous works (Dosovitskiy et al., 20