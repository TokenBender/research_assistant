## Analysis of "Scaling Expert Language Models with Unsupervised Domain Discovery"

**1. Introduction:**

- **Title:** Scaling Expert Language Models with Unsupervised Domain Discovery
- **Authors:** Suchin Gururangan, Margaret Li, Mike Lewis, Weijia Shi, Tim Althoff, Noah A. Smith, Luke Zettlemoyer
- **Publication Date:** March 24, 2023
- **Objective:** The paper proposes a novel method, Cluster-Branch-Train-Merge (C-BTM), for asynchronously training large, sparse language models by automatically discovering domains within a corpus and training specialized expert models on each domain.
- **Number of References:** 67

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1. Introduction:**

- **Key Points:**
    - Large language models (LLMs) are typically trained densely, requiring synchronization across thousands of GPUs, which is expensive and inefficient.
    - Branch-Train-Merge (BTM) alleviates this cost by dividing compute among smaller expert language models (ELMs) trained on distinct subsets of the corpus.
    - BTM relies on document metadata to identify domains, which is not always available and limits its applicability.
    - The paper introduces C-BTM, a metadata-free algorithm that uses unsupervised clustering to discover domains and train specialized ELMs on each cluster.
- **Significant Citations:**
    - **Claim:** "Large language models (LLMs) are typically trained densely: all parameters are updated with respect to all inputs."
        - **Citation:** Zhang et al., 2022; Chowdhery et al., 2022
        - **Relevance:** This citation establishes the context of dense training and its associated computational cost, motivating the need for alternative approaches.
    - **Claim:** "Branch-Train-Merge (BTM; Li et al. 2022) alleviates this cost by dividing the total compute among a collection of smaller expert language models (ELMs), each independently trained on a distinct subset (or domain) of the training corpus and ensembled during inference."
        - **Citation:** Li et al., 2022
        - **Relevance:** This citation introduces BTM as a prior work that addresses the limitations of dense training, providing a foundation for the paper's proposed method.
    - **Claim:** "However, BTM relies on document metadata to identify domains, and such supervision is not always available (e.g., in large Internet crawls; Raffel et al., 2019; Rae et al., 2021; Gao et al., 2021)."
        - **Citations:** Raffel et al., 2019; Rae et al., 2021; Gao et al., 2021
        - **Relevance:** This citation highlights the limitations of BTM, specifically its reliance on metadata, which is often unavailable in large-scale text corpora. This limitation motivates the development of C-BTM.

**2.2. C-BTM Training Process:**

- **Key Points:**
    - C-BTM uses k-means clustering to discover domains within a corpus.
    - Each cluster is assigned to a separate ELM, which is initialized from a seed language model and trained on the cluster's data.
    - The trained ELMs are combined into a sparse ensemble for inference.
- **Significant Citations:**
    - **Claim:** "C-BTM begins with unsupervised domain discovery using k-means clustering."
        - **Citation:** None
        - **Relevance:** The paper does not explicitly cite a specific work for the k-means clustering algorithm, suggesting it is a standard technique in the field.
    - **Claim:** "We then initialize expert language models (ELMs) with a seed language model (e.g., OPT; Zhang et al. 2022) and train an ELM on each cluster."
        - **Citation:** Zhang et al., 2022
        - **Relevance:** This citation introduces OPT as the seed language model used in the experiments, highlighting the importance of using a pre-trained model for initializing ELMs.

**2.3. C-BTM Inference Process:**

- **Key Points:**
    - C-BTM uses a sparse ensemble of ELMs for inference, activating only the top-k experts based on the distance between the current context and each expert's cluster center.
    - This approach enables efficient sparse computation by retrieving only the top-k experts for each new token.
- **Significant Citations:**
    - **Claim:** "We use a sparse ensemble of the outputs of ELMs for incoming test contexts (Figure 3)."
        - **Citation:** None
        - **Relevance:** The paper does not explicitly cite a specific work for the concept of sparse ensembles, suggesting it is a common practice in the field.
    - **Claim:** "This formulation is reminiscent of nearest-neighbor retrieval mechanisms for language models (Khandelwal et al., 2019; Shi et al., 2022)."
        - **Citations:** Khandelwal et al., 2019; Shi et al., 2022
        - **Relevance:** This citation connects C-BTM's inference approach to existing work on nearest-neighbor retrieval in language models, highlighting the potential for further research in this area.

**2.4. Comparing to Dense Training:**

- **Key Points:**
    - Dense LLMs are typically trained using hundreds or thousands of GPUs, requiring significant communication overhead.
    - C-BTM reduces communication overhead by training ELMs asynchronously, only requiring communication between GPUs training the same ELM.
    - C-BTM improves the resiliency of distributed training by mitigating the impact of GPU failures.
    - C-BTM makes training large LLMs more feasible on shared GPU clusters by decomposing training into smaller jobs.
- **Significant Citations:**
    - **Claim:** "Dense LMs are typically trained using hundreds or thousands of concurrent GPUs, all of which synchronize gradients each update."
        - **Citations:** Zhang et al., 2022; Chowdhery et al., 2022
        - **Relevance:** This citation provides concrete examples of dense training setups, highlighting the scale and complexity of training large LLMs.
    - **Claim:** "C-BTM improves training efficiency by reducing communication overhead, as only GPUs training the same ELM must communicate."
        - **Citation:** None
        - **Relevance:** The paper does not explicitly cite a specific work for the concept of communication overhead in distributed training, suggesting it is a well-understood concept in the field.

**2.5. Comparing to BTM:**

- **Key Points:**
    - BTM is limited to training data with metadata, which is not always available in large-scale text corpora.
    - BTM relies on cached prior estimation for ensemble weights, which requires additional data and forward passes through experts.
    - C-BTM addresses these limitations by using unsupervised clustering to discover domains and by routing based only on the current context, eliminating the need for cached prior estimation.
- **Significant Citations:**
    - **Claim:** "First, BTM is limited to training data with metadata which can be used to determine its domains."
        - **Citation:** Li et al., 2022
        - **Relevance:** This citation highlights the limitation of BTM, specifically its reliance on metadata, which is often unavailable in large-scale text corpora.
    - **Claim:** "Moreover, BTM inference follows the cached prior method introduced by Gururangan et al. (2022), where the ensemble weights are estimated using Bayes' rule on additional held out data, and the prior P(D = j) is estimated with an exponential moving average over sequences of posterior estimates that require forward passes on experts."
        - **Citation:** Gururangan et al., 2022
        - **Relevance:** This citation introduces the cached prior method used by BTM, highlighting its limitations in terms of data requirements and computational cost.

**2.6. Comparing to Mixture-of-Experts (MoE):**

- **Key Points:**
    - MoE models route tokens to specialist parameters, requiring online load balancing and communication overhead.
    - C-BTM routes sequences instead of tokens using offline balanced clustering, eliminating the need for online load balancing and reducing communication overhead.
    - C-BTM avoids expensive all-to-all operations between experts and naturally leads to interpretable expert specialization.
- **Significant Citations:**
    - **Claim:** "Like MoE models (e.g., Fedus et al., 2022), C-BTM allows for efficient scaling of large LMs while keeping inference costs manageable."
        - **Citation:** Fedus et al., 2022
        - **Relevance:** This citation establishes MoE as a relevant comparison point, highlighting the shared goal of efficient scaling.
    - **Claim:** "c-BTM improves over sparse LMs by routing sequences (instead of tokens) using offline balanced clustering (instead of online load balancing) with no shared parameters between experts."
        - **Citation:** None
        - **Relevance:** The paper does not explicitly cite a specific work for the concept of online load balancing in MoE models, suggesting it is a common practice in the field.

**3. Experimental Setup:**

- **Key Points:**
    - The paper evaluates C-BTM on two large text corpora: C4 and S2ORC.
    - The corpora are selected to be distinct from the corpus used to train the seed OPT model.
    - The paper reports perplexity on held-out data from each corpus.
- **Significant Citations:**
    - **Claim:** "C4 (Raffel et al., 2019) C4 is a publicly available distribution of a Common Crawl snapshot on Huggingface datasets."
        - **Citation:** Raffel et al., 2019
        - **Relevance:** This citation introduces C4 as the first corpus used in the experiments, providing a description of its origin and availability.
    - **Claim:** "S2ORC (Lo et al., 2019) The Semantic Scholar Research Open Corpus (S2ORC) is a publicly available corpus of full-text academic papers from the Semantic Scholar."
        - **Citation:** Lo et al., 2019
        - **Relevance:** This citation introduces S2ORC as the second corpus used in the experiments, providing a description of its origin and content.

**4. Language Modeling Results:**

- **Key Points:**
    - C-BTM consistently outperforms dense baselines across a range of compute budgets.
    - Performance improvements increase as the total compute grows.
    - There exists an optimal cluster count for each compute budget, which increases as the compute budget increases.
    - Sparsifying C-BTM models by using only the top-k experts at inference time achieves comparable performance to using all experts, while significantly reducing inference costs.
    - C-BTM achieves the same perplexity as a larger dense model with significantly fewer FLOPs.
- **Significant Citations:**
    - **Claim:** "Figure 5 shows evaluation perplexity on C4 and S2ORC with up to 16 clusters. Training on more than one cluster always outperforms training with a single cluster (i.e., a dense model)."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Figure 5, demonstrating the superiority of C-BTM over dense models.
    - **Claim:** "However, Figure 6 shows that there exists an optimal cluster count for each token budget that we consider. Each number of clusters has a budget range in which they are optimal, and the optimum smoothly progresses from smaller to larger cluster counts as we increase the training data size."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Figure 6, highlighting the relationship between the optimal cluster count and the compute budget.
    - **Claim:** "Results (Figure 9) show that despite training many more parameters, training C-BTM with many clusters and then using only the top-1 expert still outperforms the dense model."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Figure 9, demonstrating the effectiveness of sparsification in C-BTM.
    - **Claim:** "Our results are presented in Figure 11. A smaller C-BTM model, exposed to 168B tokens of text, can achieve the same perplexity as the larger 6.7B dense model with 3.5ร speedup."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Figure 11, highlighting the efficiency gains of C-BTM compared to dense models.

**5. Downstream Task Results:**

- **Key Points:**
    - C-BTM models consistently outperform dense baselines and original OPT models on six downstream text classification tasks.
    - Top-k activation reduces inference costs with negligible effects on downstream task performance.
    - C-BTM models perform comparably to larger, 6.7B OPT and 1-cluster dense baseline models, despite being trained with 3.5x less compute.
    - Performance routing methods, which take into account the order of in-context demonstrations, further improve downstream task performance.
- **Significant Citations:**
    - **Claim:** "We demonstrate that, consistent with the language modeling results in ยง4.1, C-BTM improves downstream performance on a variety of few-shot text classification tasks."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Table 1, demonstrating the superiority of C-BTM on downstream tasks.
    - **Claim:** "We also find that top-k activation reduces inference costs with negligible effects on downstream task performance."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Table 1, highlighting the efficiency gains of sparsification in C-BTM.
    - **Claim:** "C-BTM models perform comparably to larger, 6.7B OPT and 1-cluster dense baseline models, despite being trained with 3.5x less compute."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Table 1, highlighting the efficiency gains of C-BTM compared to dense models.

**6. Comparing to Mixture-of-Experts:**

- **Key Points:**
    - C-BTM outperforms MoE models trained with the same budget.
    - MoE models are more complex and prone to instability, particularly when using a large number of experts.
    - C-BTM's simplicity and deterministic routing contribute to its superior performance.
- **Significant Citations:**
    - **Claim:** "Our results suggest that language models trained with C-BTM substantially outperform MoEs trained to the same budget."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Figure 12, demonstrating the superiority of C-BTM over MoE models.
    - **Claim:** "We use 32 experts in our MoE, a capacity factor of 2, and continue training without resetting the optimizer from that used during OPT pretraining."
        - **Citation:** Komatsuzaki et al., 2022
        - **Relevance:** This citation introduces the sparse upcycling technique used to initialize the MoE model, highlighting the importance of using a pre-trained model for initialization.

**7. Analysis:**

- **Key Points:**
    - Clustering is essential for C-BTM's performance, as random clusters significantly underperform.
    - Balancing clusters is important, particularly when using a large number of clusters.
    - C-BTM experts specialize to their clusters, contributing to the efficiency gains of sparse inference.
    - C-BTM's performance is not solely due to ensembling, but also to the quality of the learned clusters.
- **Significant Citations:**
    - **Claim:** "Results in Figure 13 demonstrate that using random clusters dramatically underperforms both our method and the dense baseline."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Figure 13, demonstrating the importance of using learned clusters in C-BTM.
    - **Claim:** "To assess the effect of balancing cluster size on the performance of C-BTM, we perform C-BTM with a k-means clustering model but remove the balancing constraint."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Figure 14, demonstrating the importance of balancing clusters in C-BTM.
    - **Claim:** "These results suggest that experts specialize to their cluster. We infer that the success of sparse C-BTM inference is a result of expert specialization, and that C-BTM performance gains may be partially due to the sample efficiency of specialized training."
        - **Citation:** None
        - **Relevance:** This claim is supported by the experimental results presented in Figure 15, demonstrating the specialization of C-BTM experts to their clusters.

**8. Related Work:**

- **Key Points:**
    - C-BTM is closely related to sparse models, which activate only a subset of parameters.
    - C-BTM is inspired by MoE but is simpler and more efficient to train.
    - C-BTM is also related to expert language models, particularly BTM.
    - C-BTM's cluster routing is similar to approaches used in other works for image classification and task-specific model ensembles.
    - C-BTM contributes to research on communication-efficient training algorithms for large models.
- **Significant Citations:**
    - **Claim:** "C-BTM is closely related to sparse models which activate only a subset of parameters (Evci et al., 2020; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019)."
        - **Citations:** Evci et al., 2020; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019
        - **Relevance:** This citation establishes the connection between C-BTM and sparse models, highlighting the shared goal of reducing computational cost.
    - **Claim:** "C-BTM is inspired by MoE, but is much simpler and more efficient to train."
        - **Citation:** None
        - **Relevance:** The paper does not explicitly cite a specific work for the concept of MoE models, suggesting it is a well-understood concept in the field.
    - **Claim:** "This work is most directly related to BTM (Li et al., 2022). BTM is in turn partially inspired by prior work on variations of MoE models (Jacobs et al., 1991), but especially DEMix layers (Gururangan et al., 2022), which replace transformer feedforward layers with metadata-defined domain experts."
        - **Citations:** Li et al., 2022; Jacobs et al., 1991; Gururangan et al., 2022
        - **Relevance:** This citation establishes the connection between C-BTM and expert language models, highlighting the lineage of ideas and the contributions of prior work.

**9. Conclusion:**

- **Key Points:**
    - C-BTM is a new technique for efficiently training sparse LLMs.
    - C-BTM outperforms dense and MoE baselines across a range of compute budgets.
    - C-BTM's performance is attributed to its simplicity, deterministic routing, and the quality of the learned clusters.
    - Future work could explore C-BTM in multitask or multilingual settings, investigate the use of multiple iterations of C-BTM, and explore combining metadata- and cluster-based routing.
- **Significant Citations:**
    - **Claim:** "We introduce c-BTM, a new technique to efficiently train sparse LMs."
        - **Citation:** None
        - **Relevance:** This claim is a summary of the paper's main contribution, highlighting the novelty of C-BTM.
    - **Claim:** "C-BTM outperforms dense and MoE baselines across a range of compute budgets."
        - **Citation:** None
        - **Relevance:** This claim is a summary of the paper's main findings, highlighting the superiority of C-BTM over existing methods.

**10. Critical Analysis of Citation Usage:**

- **Effectiveness:** The authors generally use citations effectively to support their arguments and findings. They cite relevant prior work to establish context, highlight limitations, and justify their proposed approach.
- **Areas for Improvement:**
    - The paper could benefit from citing more specific works for certain standard techniques, such as k-means clustering and sparse ensembles.
    - The paper could provide a more comprehensive overview of related work on expert language models, including a discussion of different routing mechanisms and approaches to expert specialization.
- **Potential Biases:**
    - The paper primarily cites works from the authors' own research group, which could suggest a potential bias in the selection of cited works.

**11. Final Summary:**

- **Contribution:** The paper presents C-BTM, a novel and efficient method for training large, sparse language models by automatically discovering domains within a corpus and training specialized expert models on each domain. C-BTM outperforms dense and MoE baselines across a range of compute budgets, demonstrating its potential for scaling language models efficiently.
- **Influential Works:**
    - Li et al., 2022 (BTM)
    - Zhang et al., 2022 (OPT)
    - Gururangan et al., 2022 (DEMix layers)
    - Fedus et al., 2022 (Sparse expert models)
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. It cites relevant prior work to establish context, highlight limitations, and justify its proposed approach. However, the paper could benefit from citing more specific works for certain standard techniques and providing a more comprehensive overview of related work on expert language models.

This analysis provides a comprehensive overview of the paper's content, its relationship to existing literature, and its potential impact on the field of deep learning and large language models. By extracting and presenting the citations used by the authors, this analysis helps readers understand the factual basis of the research, its relationship to existing literature, and the broader context of the work.