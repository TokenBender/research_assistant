## Analysis of "FP8 versus INT8 for efficient deep learning inference"

**1. Introduction:**

- **Title:** FP8 versus INT8 for efficient deep learning inference
- **Authors:** Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Tijmen Blankevoort, Markus Nagel, Joseph Soriaga
- **Publication Date:** June 15, 2023
- **Objective:** The paper aims to investigate the practical implications of using FP8 as a number format for deep learning inference on edge devices, comparing its efficiency and accuracy against the widely used INT8 format.
- **References:** The paper cites a total of 46 references.

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:** The paper introduces the recent interest in FP8 for deep learning training and discusses the potential benefits of using FP8 for inference on edge devices. It highlights the challenges associated with quantization and the potential for FP8 to offer similar efficiency without the need for quantization.
- **Citations:**
    - **Claim:** "Porting FP32 or FP16-trained models to the INT format is called quantization. This quantization conversion step is not always straightforward and sometimes requires a bit of effort."
    - **Citation:** Nagel et al. (2020b), "Up or down? Adaptive rounding for post-training quantization." Proceedings of the 37th International Conference on Machine Learning, PMLR, 2020.
    - **Relevance:** This citation highlights the existing challenges and complexities associated with quantization, setting the stage for the paper's investigation into FP8 as a potential alternative.

**2.2 Preliminaries:**

- **Key Points:** This section provides a brief overview of the mathematical representations of integer and floating-point numbers, highlighting the key differences between the two formats. It also introduces the specific FP8 formats with 4 or 5 exponent bits that are commonly proposed for deep learning.
- **Citations:**
    - **Claim:** "We will denote the number of exponent bits in the paper as FP8-E[X], such that the proposed formats with 4 and 5 exponent bits are referred to as, respectively, FP8-E4 and FP8-E5."
    - **Citation:** Andersch et al. (2022), "Nvidia Hopper architecture in-depth." NVIDIA blog, 2022.
    - **Relevance:** This citation introduces the specific FP8 formats that the paper will focus on, providing a clear context for the subsequent analysis.

**2.3 Hardware Considerations:**

- **Key Points:** This section delves into the hardware implications of using FP8 and INT8 for deep learning inference. It analyzes the different components of a deep learning accelerator, highlighting the impact of bit-width choices on latency, energy consumption, and overall efficiency. The authors argue that FP8 is significantly less efficient than INT8 in terms of area and power consumption, especially when considering the hardware required for floating-point accumulation.
- **Citations:**
    - **Claim:** "For FP8 implementations, there is no standard exactly defining what these choices are. Thus, anytime you see a result on networks for FP8 training, it could mean many things."
    - **Citation:** Micikevicius et al. (2022), "FP8 formats for deep learning." arXiv preprint arXiv:2209.05433, 2022.
    - **Relevance:** This citation acknowledges the lack of standardization in FP8 implementations, emphasizing the need for careful analysis and comparison across different hardware designs.
    - **Claim:** "The existence of this logic also indicates the necessity for the FP32 format for some layers during training."
    - **Citation:** Nvidia (2019), "Nvidia: Apex automatic mixed precision." GitHub repository, 2019.
    - **Relevance:** This citation highlights the practical reality of mixed-precision training, where FP32 is often used for certain layers, further complicating the comparison between FP8 and INT8.
    - **Claim:** "As a first-order approximation, this measure is strongly correlated with actual performance in practice."
    - **Citation:** Buyuksahin & Najm (2002), "High-level area estimation." ISLPED, 2002.
    - **Relevance:** This citation provides a theoretical justification for using gate count as a proxy for area and power consumption, supporting the authors' analysis of hardware efficiency.

**2.4 Deep Learning Network Accuracy Comparison:**

- **Key Points:** This section focuses on the accuracy comparison between FP8 and INT8 for deep learning inference. The authors present a theoretical framework to explain the differences in accuracy between the two formats, highlighting the role of outliers in the distribution of weights and activations. They then present experimental results from both post-training quantization (PTQ) and quantization-aware training (QAT) settings, demonstrating the theoretical predictions in practice.
- **Citations:**
    - **Claim:** "If we take a similar setup for the floating-point format, as is also done by e.g., Nvidia, we can see that there is only one difference between the two formats."
    - **Citation:** Andersch et al. (2022), "Nvidia Hopper architecture in-depth." NVIDIA blog, 2022.
    - **Relevance:** This citation highlights the specific implementation details of FP8 that are relevant to the accuracy comparison, providing a clear basis for the subsequent analysis.
    - **Claim:** "We consider the mean-squared error of these distributions, as this has been shown to correlate strongly, both mathematically and practically, with the effect of noise on neural networks."
    - **Citation:** Nagel et al. (2020a), "Up or down? Adaptive rounding for post-training quantization." Proceedings of the 37th International Conference on Machine Learning, PMLR, 2020.
    - **Relevance:** This citation provides a theoretical justification for using mean-squared error as a metric for comparing the accuracy of different number formats, supporting the authors' analysis of accuracy differences.
    - **Claim:** "This is important to note since many weights and activations in neural networks are well-regularized, either explicitly due to weight regularization or implicitly because of SGD."
    - **Citation:** Zhang et al. (2021), "Understanding deep learning (still) requires rethinking generalization." Communications of the ACM, 2021.
    - **Relevance:** This citation highlights the common practice of regularization in deep learning, providing a context for the authors' analysis of the distribution of weights and activations.

**2.5 Setup for Comparative Analysis:**

- **Key Points:** This section describes the experimental setup used for comparing FP8 and INT8 formats in both PTQ and QAT settings. It outlines the specific models, datasets, and metrics used in the experiments, ensuring a fair and consistent comparison between the two formats.
- **Citations:**
    - **Claim:** "We can use these results to compare the formats as fairly as possible."
    - **Citation:** Nagel et al. (2020a), "Up or down? Adaptive rounding for post-training quantization." Proceedings of the 37th International Conference on Machine Learning, PMLR, 2020.
    - **Relevance:** This citation highlights the importance of using a fair and consistent experimental setup for comparing different quantization methods, justifying the authors' approach.

**2.6 PTQ Results:**

- **Key Points:** This section presents the results of PTQ experiments for various deep learning models, comparing the accuracy of FP8 formats with different exponent bitwidths against INT8. The results show that INT8 generally outperforms FP8 formats for well-behaved networks, while FP8-E4/FP8-E5 can be more accurate for networks with significant outliers.
- **Citations:**
    - **Claim:** "Looking at these results, there is a clear pattern. For networks like ResNet18 (He et al. (2016)), MobileNetV2 (Sandler et al. (2018)), and DeeplabV3 (Chen et al. (2017)), we know that the layers are relatively well-behaved."
    - **Citation:** He et al. (2016), "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition, 2016.
    - **Citation:** Sandler et al. (2018), "Mobilenetv2: Inverted residuals and linear bottlenecks." Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.
    - **Citation:** Chen et al. (2017), "Rethinking atrous convolution for semantic image segmentation." 2017.
    - **Relevance:** These citations provide context for the authors' analysis of the PTQ results, highlighting the specific characteristics of the models that influence the accuracy of different quantization formats.

**2.7 Quantization-Aware Training:**

- **Key Points:** This section presents the results of QAT experiments, where deep learning models are trained with quantized operations in the loop. The results show that QAT generally improves the accuracy of both FP8 and INT8 formats, with INT8 often achieving better performance than FP8 formats. The authors attribute this improvement to the ability of QAT to train away outliers and learn more uniform weight distributions, which are better suited for INT8 quantization.
- **Citations:**
    - **Claim:** "We perform range-learning based on the LSQ method (Esser et al. (2020); Bhalgat et al. (2020)), so that the results are not affected by a difference in setting the quantization ranges."
    - **Citation:** Esser et al. (2020), "Learned step size quantization." International Conference on Learning Representations (ICLR), 2020.
    - **Citation:** Bhalgat et al. (2020), "LSQ+: Improving low-bit quantization through learnable offsets and better initialization." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020.
    - **Relevance:** This citation highlights the specific range learning method used in the QAT experiments, ensuring a fair comparison between different quantization formats.

**2.8 Delving Deeper into the QAT Networks:**

- **Key Points:** This section explores the underlying reasons for the observed accuracy improvements in QAT, particularly focusing on the distribution of weights and activations. The authors argue that the network's parameters do not necessarily adapt to the specific number format used during training, and that the distribution of weights and activations is more influenced by other training settings like regularization and initialization.
- **Citations:**
    - **Claim:** "This is especially the case for INT8, where the sensitivity to the quantization ranges is much larger than the floating-point formats with more exponent bits that are naturally more resistant to outliers."
    - **Citation:** Esser et al. (2020), "Learned step size quantization." International Conference on Learning Representations (ICLR), 2020.
    - **Relevance:** This citation provides a theoretical explanation for the observed accuracy improvements in INT8 QAT, highlighting the importance of outlier sensitivity and quantization range in achieving high accuracy.

**2.9 Transformers:**

- **Key Points:** This section focuses on the specific challenges of quantizing transformer networks, highlighting the presence of significant outliers in certain layers that can negatively impact accuracy. The authors discuss various solutions proposed in the literature to address these outliers, including mixed-precision quantization and specific techniques for handling outliers in transformer networks.
- **Citations:**
    - **Claim:** "The problems with transformer quantization occur in a very specific part of the network, highlighted in red in Figure 7."
    - **Citation:** Bondarenko et al. (2021), "Understanding and overcoming the challenges of efficient transformer quantization." EMNLP, 2021.
    - **Relevance:** This citation introduces the specific problem of outliers in transformer networks, providing a context for the subsequent discussion of solutions.
    - **Claim:** "These outliers force the attention mechanism in the next layer to pay attention to some meaningless tokens â€“ like sentence separator tokens, periods, or commas that occur in the text, causing that specific token to not update significantly."
    - **Citation:** Bondarenko et al. (2021), "Understanding and overcoming the challenges of efficient transformer quantization." EMNLP, 2021.
    - **Relevance:** This citation explains the negative impact of outliers on the performance of transformer networks, highlighting the need for specific solutions to address this issue.
    - **Claim:** "Luckily, these outliers are very particular. They occur only in some attention blocks, and in those blocks only in one layer, and in those layers only in a few output channels."
    - **Citation:** Bondarenko et al. (2021), "Understanding and overcoming the challenges of efficient transformer quantization." EMNLP, 2021.
    - **Relevance:** This citation highlights the specific nature of outliers in transformer networks, suggesting that targeted solutions can be developed to address this issue.

**2.10 Comparison to Other Work:**

- **Key Points:** This section compares the paper's findings with existing literature on FP8 quantization, highlighting the consistency of the results and the paper's contribution in providing a more comprehensive analysis of the trade-offs between FP8 and INT8 formats.
- **Citations:**
    - **Claim:** "The paper from Graphcore only compares INT8 with FP8-E4/FP8-E5 for a ResNet-32 model on CIFAR-100."
    - **Citation:** Noune et al. (2023), "8-bit numerical formats for deep neural networks." 2023.
    - **Relevance:** This citation highlights the limited scope of previous work on FP8 quantization, emphasizing the paper's contribution in providing a broader and more comprehensive analysis.
    - **Claim:** "The rest of this paper shows that one can get close to the original FP32 accuracy by training with the format."
    - **Citation:** Micikevicius et al. (2022), "FP8 formats for deep learning." arXiv preprint arXiv:2209.05433, 2022.
    - **Relevance:** This citation acknowledges the findings of previous work on FP8 training, highlighting the paper's focus on the implications of FP8 for inference.

**2.11 FP8 to INT8 Network Conversion:**

- **Key Points:** This section investigates the impact of converting FP8-E4 trained networks to INT8, analyzing the accuracy changes and the underlying reasons for these changes. The authors demonstrate that INT8 conversion can often improve accuracy for networks without significant outliers, while it can lead to a decrease in accuracy for networks with outliers.
- **Citations:**
    - **Claim:** "As shown in Figure 9, INT8 can exactly represent roughly 90% of the range covered by the FP8-E4 format without any quantization error."
    - **Citation:** Nagel et al. (2020b), "Up or down? Adaptive rounding for post-training quantization." Proceedings of the 37th International Conference on Machine Learning, PMLR, 2020.
    - **Relevance:** This citation highlights the theoretical basis for the observed accuracy changes during INT8 conversion, explaining the impact of range representation on accuracy.

**2.12 The INT Quantization Paradigm:**

- **Key Points:** This section summarizes the advantages of using INT quantization for deep learning inference, highlighting the wide range of networks that can be successfully quantized to INT8 or even INT4, achieving high accuracy and efficiency. The authors emphasize the maturity of INT quantization tools and the availability of dedicated libraries for optimizing INT quantization.
- **Citations:**
    - **Claim:** "Many networks can be quantized with PTQ techniques to INT8 without much of a drop in accuracy."
    - **Citation:** Siddegowda et al. (2022), "Neural network quantization with ai model efficiency toolkit (aimet)." 2022.
    - **Relevance:** This citation highlights the widespread adoption of INT quantization for deep learning inference, emphasizing the paper's focus on the practical implications of INT quantization.
    - **Claim:** "Even for transformers, recent papers show that INT4 is likely the best accuracy/efficiency trade-off for the weights."
    - **Citation:** Dettmers & Zettlemoyer (2022), "The case for 4-bit precision: k-bit inference scaling laws." 2022.
    - **Citation:** Frantar et al. (2023), "GPTQ: Accurate quantization for generative pre-trained transformers." ICLR, 2023.
    - **Relevance:** These citations highlight the recent advancements in INT4 quantization, demonstrating the continued evolution of INT quantization techniques.

**2.13 Conclusion:**

- **Key Points:** The paper concludes that FP8 formats are not a suitable replacement for INT8 for deep learning inference on edge devices. The authors argue that INT8 offers better accuracy and efficiency for most networks, while FP8 can be more accurate for networks with significant outliers. They recommend using INT quantization for achieving optimal accuracy and efficiency in deep learning inference.
- **Citations:**
    - **Claim:** "Because of these reasons, implementing floating point formats for edge use-case scenarios is sub-optimal compared to the standard stack of integer solutions available today."
    - **Citation:** Siddegowda et al. (2022), "Neural network quantization with ai model efficiency toolkit (aimet)." 2022.
    - **Relevance:** This citation reinforces the paper's conclusion by highlighting the maturity and effectiveness of existing INT quantization tools, further emphasizing the advantages of using INT quantization for deep learning inference.

**3. Key Insights and Supporting Literature:**

- **Insight:** FP8 formats are generally less efficient than INT8 in terms of hardware area and power consumption, especially when considering the hardware required for floating-point accumulation.
    - **Supporting Citations:**
        - Andersch et al. (2022), "Nvidia Hopper architecture in-depth." NVIDIA blog, 2022.
        - Buyuksahin & Najm (2002), "High-level area estimation." ISLPED, 2002.
        - Rouhani et al. (2023), "Shared microexponents: A little shifting goes a long way." 2023.
    - **Contribution:** These citations support the paper's claim that FP8 is less efficient than INT8, providing evidence from both theoretical analysis and experimental results.
- **Insight:** The accuracy of FP8 and INT8 formats for deep learning inference is largely determined by the distribution of weights and activations, with INT8 being more accurate for well-behaved distributions and FP8-E4/FP8-E5 being more accurate for distributions with significant outliers.
    - **Supporting Citations:**
        - Nagel et al. (2020a), "Up or down? Adaptive rounding for post-training quantization." Proceedings of the 37th International Conference on Machine Learning, PMLR, 2020.
        - Zhang et al. (2021), "Understanding deep learning (still) requires rethinking generalization." Communications of the ACM, 2021.
        - Bondarenko et al. (2021), "Understanding and overcoming the challenges of efficient transformer quantization." EMNLP, 2021.
    - **Contribution:** These citations provide a theoretical framework for understanding the accuracy differences between FP8 and INT8, highlighting the importance of outlier sensitivity and distribution characteristics in achieving high accuracy.
- **Insight:** Quantization-aware training (QAT) can significantly improve the accuracy of both FP8 and INT8 formats, with INT8 often achieving better performance than FP8 formats.
    - **Supporting Citations:**
        - Esser et al. (2020), "Learned step size quantization." International Conference on Learning Representations (ICLR), 2020.
        - Bhalgat et al. (2020), "LSQ+: Improving low-bit quantization through learnable offsets and better initialization." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2020.
    - **Contribution:** These citations highlight the effectiveness of QAT in improving the accuracy of quantized networks, demonstrating the potential of QAT for achieving high accuracy with both FP8 and INT8 formats.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:** The paper uses a comprehensive experimental setup to compare FP8 and INT8 formats for deep learning inference. It includes a wide range of models, datasets, and metrics, ensuring a fair and consistent comparison between the two formats. The experiments are conducted in both post-training quantization (PTQ) and quantization-aware training (QAT) settings.
- **Foundations:** The authors cite several works to justify their experimental methodology, including:
    - **For PTQ:** Nagel et al. (2020b), "Up or down? Adaptive rounding for post-training quantization." Proceedings of the 37th International Conference on Machine Learning, PMLR, 2020.
    - **For QAT:** Esser et al. (2020), "Learned step size quantization." International Conference on Learning Representations (ICLR), 2020.
    - **For range estimation:** Kuzmin et al. (2022), "Fp8 quantization: The power of the exponent." CVPR, 2022.
- **Novel Aspects:** The authors introduce a novel approach for comparing FP8 and INT8 formats by analyzing the hardware implications of using each format, highlighting the efficiency trade-offs between the two. They also provide a comprehensive analysis of the accuracy differences between the two formats, considering both well-behaved distributions and distributions with significant outliers.

**5. Results in Context:**

- **Main Results:** The paper's main results demonstrate that INT8 generally outperforms FP8 formats for deep learning inference on edge devices. While FP8-E4/FP8-E5 can be more accurate for networks with significant outliers, INT8 offers better accuracy and efficiency for most networks. QAT can significantly improve the accuracy of both FP8 and INT8 formats, with INT8 often achieving better performance than FP8 formats.
- **Comparison with Existing Literature:** The paper's results are consistent with previous findings on FP8 quantization, but they provide a more comprehensive analysis of the trade-offs between FP8 and INT8 formats. The authors highlight the lack of standardization in FP8 implementations and the need for careful analysis and comparison across different hardware designs. They also emphasize the importance of considering the distribution of weights and activations in evaluating the accuracy of different quantization formats.
- **Confirmation, Contradiction, or Extension:** The paper's results confirm the findings of previous work on FP8 quantization, but they also extend the analysis by considering the hardware implications of using FP8 and INT8 formats. The authors' analysis highlights the efficiency trade-offs between the two formats, providing a more comprehensive understanding of the practical implications of using FP8 for deep learning inference.

**6. Discussion and Related Work:**

- **Situating the Work:** The authors situate their work within the existing literature on FP8 quantization, acknowledging the recent interest in FP8 for deep learning training and the potential benefits of using FP8 for inference on edge devices. They highlight the challenges associated with quantization and the potential for FP8 to offer similar efficiency without the need for quantization.
- **Key Papers Cited:**
    - Micikevicius et al. (2022), "FP8 formats for deep learning." arXiv preprint arXiv:2209.05433, 2022.
    - Noune et al. (2023), "8-bit numerical formats for deep neural networks." 2023.
    - Bondarenko et al. (2021), "Understanding and overcoming the challenges of efficient transformer quantization." EMNLP, 2021.
    - Dettmers & Zettlemoyer (2022), "The case for 4-bit precision: k-bit inference scaling laws." 2022.
    - Frantar et al. (2023), "GPTQ: Accurate quantization for generative pre-trained transformers." ICLR, 2023.
- **Highlighting Novelty:** The authors highlight the novelty of their work in providing a more comprehensive analysis of the trade-offs between FP8 and INT8 formats, considering both hardware efficiency and accuracy differences. They also emphasize the importance of considering the distribution of weights and activations in evaluating the accuracy of different quantization formats.

**7. Future Work and Open Questions:**

- **Areas for Further Research:** The authors suggest several areas for further research, including:
    - Investigating the impact of FP8 quantization on the training process, particularly for gradients.
    - Exploring the potential of using FP8 for specific tasks or applications where its accuracy advantages might outweigh its efficiency drawbacks.
    - Developing more efficient hardware implementations for FP8, potentially addressing the limitations of current designs.
- **Citations:**
    - Sun et al. (2019), "Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks." Advances in Neural Information Processing Systems, 2019.
    - Gupta et al. (2015), "Deep learning with limited numerical precision." International Conference on Machine Learning, ICML, 2015.
    - Fournarakis & Nagel (2021), "In-hindsight quantization range estimation for quantized training." 2021.
    - Yanga et al. (2019), "Training high-performance and large-scale deep neural networks with full 8-bit integers." 2019.
    - Yao et al. (2022), "Zeroquant: Efficient and affordable post-training quantization for large-scale transformers." NeuRIPS, 2022.
    - Dettmers & Zettlemoyer (2022), "The case for 4-bit precision: k-bit inference scaling laws." 2022.
    - Frantar et al. (2023), "GPTQ: Accurate quantization for generative pre-trained transformers." ICLR, 2023.
- **Open Questions:** The paper raises several open questions, including:
    - Can FP8 be effectively used for training deep learning models, particularly for gradients?
    - Are there specific tasks or applications where the accuracy advantages of FP8 outweigh its efficiency drawbacks?
    - Can more efficient hardware implementations for FP8 be developed to address the limitations of current designs?

**8. Critical Analysis of Citation Usage:**

- **Effectiveness:** The authors generally use citations effectively to support their arguments and findings. They cite relevant works to introduce key concepts, provide theoretical justification for their analysis, and compare their findings with existing literature.
- **Areas for Improvement:** While the authors cite a wide range of relevant works, they could have included additional citations to provide a more comprehensive overview of the research landscape. For example, they could have cited more works on the hardware implications of using FP8 and INT8 formats, particularly those focusing on the design of dedicated hardware accelerators.
- **Potential Biases:** The authors primarily cite works from NVIDIA, Qualcomm, and other major technology companies, potentially reflecting a bias towards industry-driven research. They could have included more citations from academic research groups to provide a more balanced perspective on the field.

**9. Final Summary:**

- **Contribution:** The paper makes a significant contribution to the field of deep learning inference by providing a comprehensive analysis of the trade-offs between FP8 and INT8 formats. The authors' analysis highlights the efficiency and accuracy differences between the two formats, considering both hardware implications and distribution characteristics of weights and activations. They also demonstrate the effectiveness of quantization-aware training (QAT) in improving the accuracy of both FP8 and INT8 formats.
- **Influential Works:** The paper frequently cites works from NVIDIA, Qualcomm, and other major technology companies, reflecting the growing interest in FP8 quantization for deep learning training and inference.
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. The authors cite relevant works to introduce key concepts, provide theoretical justification for their analysis, and compare their findings with existing literature. However, they could have included additional citations to provide a more comprehensive overview of the research landscape and address potential biases in their selection of cited works.

Overall, the paper provides a valuable contribution to the field of deep learning inference by offering a comprehensive analysis of the trade-offs between FP8 and INT8 formats. The authors' analysis highlights the importance of considering both hardware efficiency and accuracy differences, as well as the distribution characteristics of weights and activations, in evaluating the effectiveness of different quantization formats. The paper also demonstrates the effectiveness of quantization-aware training (QAT) in improving the accuracy of both FP8 and INT8 formats. However, the authors could have included additional citations to provide a more comprehensive overview of the research landscape and address potential biases in their selection of cited works.