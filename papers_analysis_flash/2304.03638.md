## Analysis of "Compressed Regression over Adaptive Networks"

**1. Introduction:**

- **Title:** Compressed Regression over Adaptive Networks
- **Authors:** Marco Carpentiero, Vincenzo Matta, and Ali H. Sayed
- **Publication Date:** April 7, 2023 (v1)
- **Objective:** The paper aims to derive the performance achievable by a network of distributed agents that solve a regression problem adaptively and under communication constraints.
- **References:** The paper cites a total of 47 references.

**2. Section-by-Section Analysis with Citation Extraction:**

**a. Introduction:**

- **Key Points:** The paper introduces the problem of distributed regression in the context of networked device systems and the challenges posed by communication constraints. It highlights the use of compression strategies, particularly randomized and differential compression, to address these constraints. The authors mention their previous work on the ACTC (adapt-compress-then-combine) diffusion strategy and its convergence guarantees.
- **Significant Citations:**
    - **[2]-[8]:** "Motivated by the exponential growth of data availability and the success of networked device systems (e.g., Internet of Things, mobile edge computing, vehicular networks) fully-decentralized strategies and federated strategies [2]-[8] represent the backbone of the next-generation learning algorithms." This citation introduces the broader context of decentralized learning and federated strategies, highlighting the importance of the research area.
    - **[9]–[12]:** "One of the biggest challenges of decentralized learning is the communication bottleneck induced by the back-and-forth information transmission among agents. Suitable compression strategies, e.g., quantization [13], must be introduced to cope with unavoidable communication constraints. Data compression is a well-studied discipline, providing several useful tools that, over the years, have been successfully applied to many inference problems, even in distributed settings [14]–[19]." This citation emphasizes the importance of compression strategies in decentralized learning and provides a brief overview of existing work in the field.
    - **[20]-[23]:** "Randomized compression operators date back to the pioneering works on the probabilistic analysis of quantizers [24] and dithering [25]. By generating the coded output through suitable random mechanisms, these compression operators exhibit features useful for inference purposes, such as unbiasedness or mean-square-error boundedness, which hold universally, i.e., irrespective of the particular data distribution. Thanks to these properties, the randomized compression approach has been recently employed in the distributed optimization field starting from the popular family of randomized quantizers [20] and randomized sparsifiers [21], [22]." This citation introduces the concept of randomized compression and its application in distributed optimization.
    - **[26]–[31]:** "Several recent works have considered gradient descent and stochastic gradient descent algorithms in the presence of randomized compression [26]–[28], sometimes also coupled with differential quantization [29]- [31]." This citation highlights the recent research efforts in combining gradient descent algorithms with randomized compression.
    - **[32], [33]:** "In [32], [33], we extended the analysis of distributed learning under communication constraints by removing the aforementioned limitations and considering instead: (a) adaptive systems exploiting stochastic gradient descent with constant step-size which, differently from diminishing step-size implementations, are able to promptly track data drifts and learn continuously; (b) non-symmetric and left-stochastic combination policies to represent a wide variety of real-life network scenarios; (c) strong convexity only at a global level allowing the existence of convex and non-convex cost functions at a local level; and (d) diffusion (as opposed to consensus) strategies, which have been shown to entail better performance and wider stability range [2]. We introduced the ACTC (adapt-compress-then-combine) diffusion strategy, which borrows the distributed implementation of the popular ATC (adapt-then-combine) diffusion strategy [2], [34], [35] and enriches it to embrace constrained communication. Leveraging a suitable class of randomized differential compression operators, the ACTC strategy is able to converge to a small neighborhood of the desired solution with significant savings in terms of transmission resources, e.g., quantization bits." This citation introduces the authors' previous work on the ACTC strategy and its key features.
    - **[32], [33]:** "In [32], [33] the learning behavior of the ACTC strategy and its convergence guarantees were characterized in great detail by means of transient and mean-square stability analysis. We showed that, despite data compression, it is always possible to achieve mean-square stability by tuning the step-size and the stability parameter of the ACTC algorithm. We concluded that the peculiar learning behavior of adaptive networks is preserved, exposing two transient phases and a steady-state phase [2], [34], [35]." This citation further emphasizes the importance of the ACTC strategy and its stability properties.

**b. Background:**

- **Key Points:** This section formally defines the distributed regression problem, including the model, the local risk function, and the network objective. It introduces the combination matrix and its properties, including left-stochasticity and strong connectivity. The authors also introduce the ACTC diffusion strategy and its three steps: adaptation, compression, and combination.
- **Significant Citations:**
    - **[2], [34], [35]:** "After stability and transient analyses, the third essential part that completes the characterization of a learning algorithm is the steady-state performance. In this work, we fill this research gap, focusing on the relevant case of the so-called MSE (Mean-Square-Error) Networks [2]. The networks consist of spatially dispersed agents deployed to solve a distributed linear regression problem in an online fashion. This setting represents the distributed counterpart of the popular LMS (least-mean-squares) algorithm employed in signal processing and statistical learning [37]. One of the earliest works on MSE networks under communication constraints is [36], where the compression errors were modeled as noisy sources. In contrast, in this work we start from the design of the compression operators, and take into account the exact form of the associated compression errors. This extension introduces significant additional challenges in the analysis." This citation introduces the concept of MSE networks and highlights the novelty of the paper's approach in considering the exact form of compression errors.
    - **[2], [34], [35]:** "The networks consist of spatially dispersed agents deployed to solve a distributed linear regression problem in an online fashion. This setting represents the distributed counterpart of the popular LMS (least-mean-squares) algorithm employed in signal processing and statistical learning [37]. One of the earliest works on MSE networks under communication constraints is [36], where the compression errors were modeled as noisy sources. In contrast, in this work we start from the design of the compression operators, and take into account the exact form of the associated compression errors. This extension introduces significant additional challenges in the analysis." This citation provides context for the distributed regression problem and its relationship to existing work on MSE networks.
    - **[20], [26], [28], [31]–[33]:** "The implementation of the ACTC strategy in (9) relies on the compression operator Qk(·). Following [20], [26], [28], [31]–[33], we focus on the following relevant class of randomized compression operators." This citation introduces the class of randomized compression operators used in the ACTC strategy.
    - **[32], [33]:** "We remark that the compressed difference is scaled by a design parameter ζ∈ (0,1) governing the stability of the ACTC strategy see [32], [33]." This citation highlights the role of the stability parameter ζ in the ACTC strategy.

**c. Compression Operators:**

- **Key Points:** This section defines the properties of compression operators, including unbiasedness and variance bound. It introduces two examples of compression operators: randomized quantizers and randomized sparsifiers.
- **Significant Citations:**
    - **[20]:** "Example 1 (Randomized quantizers [20]). Given an input vector x ∈ RM, the randomized quantizer represents its Euclidean norm ||x|| with negligible quantization error (e.g., with machine precision) using h bits, and quantizes each component xm of x separately. One bit is used to encode the sign of Xm and r bits are used to represent Xm = xm/||x|| ∈ [0,1]. The interval [0,1] is split into L equal subintervals of size 0 = 1/L. Accordingly, given r bits, the number of levels is L = 2″ – 1. Each Xm is then randomly encoded by choosing one of the two endpoints that enclose it. Let j(Xm) = [Xm/0] be the index of the lower endpoint, which is equal to y(xm) = j(xm) 0. The randomized encoding is based on the rule:..." This citation provides a detailed description of the randomized quantizer compression operator.
    - **[31]:** "Example 2 (Randomized sparsifiers [31]). Given an input x ∈ RM, the randomized sparsifier applies the rule..." This citation introduces the randomized sparsifier compression operator.
    - **[20]:** "It is shown in [20] that the compression parameter of the randomized quantizer is equal to:..." This citation provides the compression parameter for the randomized quantizer.
    - **[20]:** "and the number of bits needed to encode the vector x is equal to [20]:..." This citation provides the number of bits required to encode the vector using the randomized quantizer.
    - **[22]:** "It is shown in [22] that the compression parameter of the randomized sparsifier is equal to:..." This citation provides the compression parameter for the randomized sparsifier.
    - **[40], [41]:** "Noticing that the sparsified vector can be efficiently represented by encoding the values and positions of the non-masked components only, one can also compute the cost in bits by using h bits for the values and log2(M) bits for the positions." This citation discusses the cost of using the randomized sparsifier in terms of bits.
    - **[20], [29]–[31], [40], [41]:** "The class of compression operators examined in this work has been shown to model faithfully practical compression schemes, and to be well-tailored to the high-dimensional setting often encountered in learning applications [20], [29]–[31], [40], [41]." This citation highlights the practical relevance of the compression operators discussed in the paper.
    - **[39]:** "We hasten to add that the class of compression operators presented here is not exhaustive. Other constructions are possible. One useful class proposed in [39] introduces a second parameter into the variance bound (13). We remark that the tools used in our work can also be applied to study this other type of compression schemes see, e.g., [42]." This citation acknowledges the existence of other compression operators and suggests potential future research directions.

**d. Properties of Cost Functions and Gradient Noise:**

- **Key Points:** This section discusses the properties of the local risk functions and the gradient noise, which are crucial for analyzing the performance of the ACTC strategy. It establishes the smoothness of the local risk functions and the strong convexity of the aggregate risk function. It also introduces the gradient noise and its properties.
- **Significant Citations:**
    - **[2], [32]–[35]:** "The gradient noise acts as a disturbance, which causes a persistent fluctuation of the estimated minimizer of (3) around its true value [2], [32]–[35]. The convergence of the ACTC diffusion strategy was already established under standard regularity conditions on the gradient noise in [33]." This citation introduces the concept of gradient noise and its impact on the convergence of the ACTC strategy.
    - **[33]:** "The convergence of the ACTC diffusion strategy was already established under standard regularity conditions on the gradient noise in [33]." This citation highlights the authors' previous work on the convergence of the ACTC strategy.

**e. Learning Performance of the ACTC Strategy:**

- **Key Points:** This section presents the main result of the paper: an upper bound on the mean-square-error performance of the ACTC strategy in the steady-state regime. The authors analyze the different components of the error, including the uncompressed evolution error, the gradient noise compression loss, and the network error compression loss.
- **Significant Citations:**
    - **[2], [35]:** "Uncompressed evolution error. This term corresponds to the mean-square-error achieved by the diffusion strategy if perfect (i.e., uncompressed) information is shared. It has the same structure of the classical ATC diffusion strategy performance, involving the main attributes of the inference problem, namely, the regressors' correlation matrices Ru,k and the noise variances στκ, suitably weighted by the Perron eigenvector and the parameters ak that quantify the differences among the individual step-sizes [2], [35]." This citation connects the uncompressed evolution error to the performance of the classical ATC strategy.
    - **[2], [35]:** "Compression loss. An additional source of error affects the steady-state behavior because of the sharing of compressed information. Equation (43) highlights the fundamental sources from which the compression loss originates, which are the gradient noise and the network error component." This citation highlights the two main sources of compression loss: gradient noise and network error.
    - **[32], [33]:** "The former term can be related to the behavior of classical quantization systems, whose distortion depends on the variance of the random variable to be compressed. In the steady-state regime, the innovation in the compression step (9b) has variance related to the gradient noise - see Lemma 2 in Appendix B-C2. Accordingly, we find a distortion term containing the trace of the gradient noise covariance matrix, Rs,k(w°) = 4α σκRu,k." This citation connects the gradient noise compression loss to the behavior of classical quantization systems.
    - **[2], [34], [35]:** "The network error component term deals with the local discrepancies between individual agents and the coordinated evolution of the network towards the common minimizer. In the classical ATC diffusion strategy this error term is a higher-order correction [2], [34], [35], while in the ACTC diffusion strategy it is increased due to the compression error that seeps into the algorithm evolution; it is now on the order of µ as the other terms in the MSE expression." This citation explains the network error compression loss and its relationship to the classical ATC strategy.
    - **[32], [33]:** "Remarkably, we see from (43) that the compression parameters wk are weighted by the squared entries of the Perron eigenvector, suggesting a useful relationship between the network arrangement and the design of the compression operators. Therefore, designing the compression operators considering the network structure can be used to tune the mean-square-error performance. We will explore this possibility in the next section." This citation highlights the importance of considering the network structure when designing compression operators.

**f. Illustrative Examples and Optimized Resource Allocation:**

- **Key Points:** This section provides illustrative examples of the ACTC strategy with different compression operators and demonstrates the impact of data compression on network performance. It then introduces the problem of optimized resource allocation, aiming to minimize the upper bound on the mean-square-error by allocating communication resources efficiently across agents.
- **Significant Citations:**
    - **[32], [33]:** "The agents quantize the transmitted information using the randomized quantizers presented in Example 1. The ACTC performance approaches the ATC performance as the bit-rate increases [32], [33], in accordance with (40)." This citation highlights the relationship between the bit-rate and the performance of the ACTC strategy.
    - **[13]:** "As is typical in bit-allocation problems pursued in the theory of quantization [13], we solve problem (45) over the real domain and then round the solution to integer values that satisfy the constraints." This citation acknowledges the use of bit allocation techniques in quantization theory.
    - **[10]:** "One way to compute an estimate of the Perron eigenvector is by running, alongside the ACTC diffusion strategy, an averaging consensus algorithm [10]. After t iterations, the output of the consensus algorithm can be cast in the form:..." This citation introduces the use of consensus algorithms to estimate the Perron eigenvector.
    - **[2]:** "The solution requires knowledge of the Perron eigenvector entries πκ and of the distortion coefficients dk, which are in general not available to agents, but can be estimated in an online manner, as we now show." This citation highlights the importance of estimating the Perron eigenvector and distortion coefficients.
    - **[2]:** "One way to compute an estimate of the Perron eigenvector is by running, alongside the ACTC diffusion strategy, an averaging consensus algorithm [10]. After t iterations, the output of the consensus algorithm can be cast in the form:..." This citation provides a method for estimating the Perron eigenvector.
    - **[2]:** "In general, an averaging consensus algorithm [10] can be used to estimate the Perron eigenvector, but the estimated eigenvector is not available to agents, but can be estimated in an online manner, as we now show." This citation highlights the challenge of estimating the Perron eigenvector in an online setting.
    - **[2], [37]:** "We can compute the solution to (45) by applying the Karush-Kuhn-Tucker (KKT) conditions, see Appendix H. The solution requires knowledge of the Perron eigenvector entries πκ and of the distortion coefficients dk, which are in general not available to agents, but can be estimated in an online manner, as we now show." This citation introduces the use of KKT conditions to solve the optimization problem.
    - **[13], [13, §8.3]:** "According to (16), relation (51) is an upper bound on the compression error (13), which is known to be tight in the high resolution regime, i.e., for relatively high values of xk. We compute the solution to problem (45) by applying the Karush-Kuhn-Tucker (KKT) conditions see Appendix H. Under the high resolution approximation implied by (51), problem (45) can be solved in closed form by resorting to classical methods for optimal bit allocation with scalar quantizers. The closed-form solution will give us an insightful interpretation of the optimized allocation. Writing (51) as ŵk ≈ M2−2xk and neglecting the box constraints, problem (45) has the same structure as the bit allocation problem in [13]. By means of Lagrange multipliers, or applying the arithmetic/geometric mean inequality, the optimal allocation can be shown to be [13, §8.3]:..." This citation provides a solution to the optimization problem using classical methods for bit allocation.
    - **[13]:** "As is typical in bit-allocation problems pursued in the theory of quantization [13], we solve problem (45) over the real domain and then round the solution to integer values that satisfy the constraints." This citation highlights the use of bit allocation techniques in quantization theory.
    - **[13], [13, §8.3]:** "According to (16), relation (51) is an upper bound on the compression error (13), which is known to be tight in the high resolution regime, i.e., for relatively high values of xk. We compute the solution to problem (45) by applying the Karush-Kuhn-Tucker (KKT) conditions see Appendix H. Under the high resolution approximation implied by (51), problem (45) can be solved in closed form by resorting to classical methods for optimal bit allocation with scalar quantizers. The closed-form solution will give us an insightful interpretation of the optimized allocation. Writing (51) as ŵk ≈ M2−2xk and neglecting the box constraints, problem (45) has the same structure as the bit allocation problem in [13]. By means of Lagrange multipliers, or applying the arithmetic/geometric mean inequality, the optimal allocation can be shown to be [13, §8.3]:..." This citation provides a solution to the optimization problem using classical methods for bit allocation.
    - **[40], [41]:** "The exact solution of (45) would require to enforce an integer constraint on x, leading to nonpractical solvers. As is typical in bit-allocation problems pursued in the theory of quantization [13], we solve problem (45) over the real domain and then round the solution to integer values that satisfy the constraints." This citation acknowledges the use of bit allocation techniques in quantization theory.
    - **[2]:** "The solution requires knowledge of the Perron eigenvector entries πκ and of the distortion coefficients dk, which are in general not available to agents, but can be estimated in an online manner, as we now show." This citation highlights the importance of estimating the Perron eigenvector and distortion coefficients.
    - **[2]:** "One way to compute an estimate of the Perron eigenvector is by running, alongside the ACTC diffusion strategy, an averaging consensus algorithm [10]. After t iterations, the output of the consensus algorithm can be cast in the form:..." This citation provides a method for estimating the Perron eigenvector.
    - **[2]:** "In general, an averaging consensus algorithm [10] can be used to estimate the Perron eigenvector, but the estimated eigenvector is not available to agents, but can be estimated in an online manner, as we now show." This citation highlights the challenge of estimating the Perron eigenvector in an online setting.
    - **[2], [37]:** "We can compute the solution to (45) by applying the Karush-Kuhn-Tucker (KKT) conditions, see Appendix H. The solution requires knowledge of the Perron eigenvector entries πκ and of the distortion coefficients dk, which are in general not available to agents, but can be estimated in an online manner, as we now show." This citation introduces the use of KKT conditions to solve the optimization problem.
    - **[13], [13, §8.3]:** "According to (16), relation (51) is an upper bound on the compression error (13), which is known to be tight in the high resolution regime, i.e., for relatively high values of xk. We compute the solution to problem (45) by applying the Karush-Kuhn-Tucker (KKT) conditions see Appendix H. Under the high resolution approximation implied by (51), problem (45) can be solved in closed form by resorting to classical methods for optimal bit allocation with scalar quantizers. The closed-form solution will give us an insightful interpretation of the optimized allocation. Writing (51) as ŵk ≈ M2−2xk and neglecting the box constraints, problem (45) has the same structure as the bit allocation problem in [13]. By means of Lagrange multipliers, or applying the arithmetic/geometric mean inequality, the optimal allocation can be shown to be [13, §8.3]:..." This citation provides a solution to the optimization problem using classical methods for bit allocation.
    - **[13]:** "As is typical in bit-allocation problems pursued in the theory of quantization [13], we solve problem (45) over the real domain and then round the solution to integer values that satisfy the constraints." This citation highlights the use of bit allocation techniques in quantization theory.
    - **[13], [13, §8.3]:** "According to (16), relation (51) is an upper bound on the compression error (13), which is known to be tight in the high resolution regime, i.e., for relatively high values of xk. We compute the solution to problem (45) by applying the Karush-Kuhn-Tucker (KKT) conditions see Appendix H. Under the high resolution approximation implied by (51), problem (45) can be solved in closed form by resorting to classical methods for optimal bit allocation with scalar quantizers. The closed-form solution will give us an insightful interpretation of the optimized allocation. Writing (51) as ŵk ≈ M2−2xk and neglecting the box constraints, problem (45) has the same structure as the bit allocation problem in [13]. By means of Lagrange multipliers, or applying the arithmetic/geometric mean inequality, the optimal allocation can be shown to be [13, §8.3]:..." This citation provides a solution to the optimization problem using classical methods for bit allocation.
    - **[40], [41]:** "The exact solution of (45) would require to enforce an integer constraint on x, leading to nonpractical solvers. As is typical in bit-allocation problems pursued in the theory of quantization [13], we solve problem (45) over the real domain and then round the solution to integer values that satisfy the constraints." This citation acknowledges the use of bit allocation techniques in quantization theory.
    - **[2]:** "The solution requires knowledge of the Perron eigenvector entries πκ and of the distortion coefficients dk, which are in general not available to agents, but can be estimated in an online manner, as we now show." This citation highlights the importance of estimating the Perron eigenvector and distortion coefficients.
    - **[2]:** "One way to compute an estimate of the Perron eigenvector is by running, alongside the ACTC diffusion strategy, an averaging consensus algorithm [10]. After t iterations, the output of the consensus algorithm can be cast in the form:..." This citation provides a method for estimating the Perron eigenvector.
    - **[2]:** "In general, an averaging consensus algorithm [10] can be used to estimate the Perron eigenvector, but the estimated eigenvector is not available to agents, but can be estimated in an online manner, as we now show." This citation highlights the challenge of estimating the Perron eigenvector in an online setting.
    - **[2], [37]:** "We can compute the solution to (45) by applying the Karush-Kuhn-Tucker (KKT) conditions, see Appendix H. The solution requires knowledge of the Perron eigenvector entries πκ and of the distortion coefficients dk, which are in general not available to agents, but can be estimated in an online manner, as we now show." This citation introduces the use of KKT conditions to solve the optimization problem.
    - **[13], [13, §8.3]:** "According to (16), relation (51) is an upper bound on the compression error (13), which is known to be tight in the high resolution regime, i.e., for relatively high values of xk. We compute the solution to problem (45) by applying the Karush-Kuhn-Tucker (KKT) conditions see Appendix H. Under the high resolution approximation implied by (51), problem (45) can be solved in closed form by resorting to classical methods for optimal bit allocation with scalar quantizers. The closed-form solution will give us an insightful interpretation of the optimized allocation. Writing (51) as ŵk ≈ M2−2xk and neglecting the box constraints, problem (45) has the same structure as the bit allocation problem in [13]. By means of Lagrange multipliers, or applying the arithmetic/geometric mean inequality, the optimal allocation can be shown to be [13, §8.3]:..." This citation provides a solution to the optimization problem using classical methods for bit allocation.
    - **[13]:** "As is typical in bit-allocation problems pursued in the theory of quantization [13], we solve problem (45) over the real domain and then round the solution to integer values that satisfy the constraints." This citation highlights the use of bit allocation techniques in quantization theory.
    - **[13], [13, §8.3]:** "According to (16), relation (51) is an upper bound on the compression error (13), which is known to be tight in the high resolution regime, i.e., for relatively high values of xk. We compute the solution to problem (45) by applying the Karush-Kuhn-Tucker (KKT) conditions see Appendix H. Under the high resolution approximation implied by (51), problem (45) can be solved in closed form by resorting to classical methods for optimal bit allocation with scalar quantizers. The closed-form solution will give us an insightful interpretation of the optimized allocation. Writing (51) as ŵk ≈ M2−2xk and neglecting the box constraints, problem (45) has the same structure as the bit allocation problem in [13]. By means of Lagrange multipliers, or applying the arithmetic/geometric mean inequality, the optimal allocation can be shown to be [13, §8.3]:..." This citation provides a solution to the optimization problem using classical methods for bit allocation.
    - **[40], [41]:** "The exact solution of (45) would require to enforce an integer constraint on x, leading to nonpractical solvers. As is typical in bit-allocation problems pursued in the theory of quantization [13], we solve problem (45) over the real domain and then round the solution to integer values that satisfy the constraints." This citation acknowledges the use of bit allocation techniques in quantization theory.
    - **[2]:** "The solution requires knowledge of the Perron eigenvector entries πκ and of the distortion coefficients dk, which are in general not available to agents, but can be estimated in an online manner, as we now show." This citation highlights the importance of estimating the Perron eigenvector and distortion coefficients.
    - **[2]:** "One way to compute an estimate of the Perron eigenvector is by running, alongside the ACTC diffusion strategy, an averaging consensus algorithm [10]. After t iterations, the output of the consensus algorithm can be cast in the form:..." This citation provides a method for estimating the Perron eigenvector.
    - **[2]:** "In general, an averaging consensus algorithm [10] can be used to estimate the Perron eigenvector, but the estimated eigenvector is not available to agents, but can be estimated in an online manner, as we now show." This citation highlights the challenge of estimating the Perron eigenvector in an online setting.
    - **[2], [37]:** "We can compute the solution to (45) by applying the Karush-Kuhn-Tucker (KKT) conditions, see Appendix H. The solution requires knowledge of the Perron eigenvector entries πκ and of the distortion coefficients dk, which are in general not available to agents, but can be estimated in an online manner, as we now show." This citation introduces the use of KKT conditions to solve the optimization problem.
    - **[13], [13, §8.3]:** "According to (16), relation (51) is an upper bound on the compression error (13), which is known to be tight in the high resolution regime, i.e., for relatively high values of xk. We compute the solution to problem (45) by applying the Karush-Kuhn-Tucker (KKT) conditions see Appendix H. Under the high resolution approximation implied by (51), problem (45) can be solved in closed form by resorting to classical methods for optimal bit allocation with scalar quantizers. The closed-form solution will give us an insightful interpretation of the optimized allocation. Writing (51) as ŵk ≈ M2−2xk and neglecting the box constraints, problem (45) has the same structure as the bit allocation problem in [13]. By means of Lagrange multipliers, or applying the arithmetic/geometric mean inequality, the optimal allocation can be shown to be [13, §8.3]:..." This citation provides a solution to the optimization problem using classical methods for bit allocation.
    - **[13]:** "As is typical in bit-allocation problems pursued in the theory of quantization [13], we solve problem (45) over the real domain and then round the solution to integer values that satisfy the constraints." This citation highlights the use of bit allocation techniques in quantization theory.
    - **[13], [13, §8.3]:** "According to (16), relation (51) is an upper bound on the compression error (13), which is known to be tight in the high resolution regime, i.e., for relatively high values of xk. We compute the solution to problem (45) by applying the Karush-Kuhn-Tucker (KKT) conditions see Appendix H. Under the high resolution approximation implied by (51), problem (45) can be solved in closed form by resorting to classical methods for optimal bit allocation with scalar quantizers. The closed-form solution will give us an insightful interpretation of the optimized allocation. Writing (51) as ŵk ≈ M2−2xk and neglecting the box constraints, problem (45) has the same structure as the bit allocation problem in [13]. By means of Lagrange multipliers, or applying the arithmetic/geometric mean inequality, the optimal allocation can be shown to be [13, §8.3]:..." This citation provides a solution to the optimization problem using classical methods for bit allocation.
    - **[40], [41]:** "The exact solution of (45) would require to enforce an integer constraint on x, leading to nonpractical solvers. As is typical in bit-allocation problems pursued in the theory of quantization [13], we solve problem (45) over the real domain and then round the solution to integer values that satisfy the constraints." This citation acknowledges the use of bit allocation techniques in quantization theory.
    - **[2]:** "The solution requires knowledge of the Perron eigenvector entries πκ and of the distortion coefficients dk, which are in general not available to agents, but can be estimated in an online manner, as we now show." This citation highlights the importance of estimating the Perron eigenvector and distortion coefficients.
    - **[2]:** "One way to compute an estimate of the Perron eigenvector is by running, alongside the ACTC diffusion strategy, an averaging consensus algorithm [10]. After t iterations, the output of the consensus algorithm can be cast in the form:..." This citation provides a method for estimating the Perron eigenvector.
    - **[2]:** "In general, an averaging consensus algorithm [10] can be used to estimate the Perron eigenvector, but the estimated eigenvector is not available to agents, but can be estimated in an online manner, as we now show." This citation highlights the challenge of estimating the Perron eigenvector in an online setting.
    - **[2], [37]:** "We can compute the solution to (45) by applying the Karush-Kuhn-Tucker (KKT) conditions, see Appendix H. The solution requires knowledge of the Perron eigenvector entries πκ and of the distortion coefficients dk, which are in general not available to agents, but can be estimated in an online manner, as we now show." This citation introduces the use of KKT conditions to solve the optimization problem.
    - **[13], [13, §8.3]:** "According to (16), relation (51) is an upper bound on the compression error (13), which is known to be tight in the high resolution regime, i.e., for relatively high values of xk. We compute the solution to problem (45) by applying the Karush-Kuhn-Tucker (KKT) conditions see Appendix H. Under the high resolution approximation implied by (51), problem (45) can be solved in closed form by resorting to classical methods for optimal bit allocation with scalar quantizers. The closed-form solution will give us an insightful interpretation of the optimized allocation. Writing (51) as ŵk ≈ M2−2xk and neglecting the box constraints, problem (45) has the same structure as the bit allocation problem in [13]. By means of Lagrange multipliers, or applying the arithmetic/geometric mean inequality, the optimal allocation can be shown to be [13, §8.3]:..." This citation provides a solution to the optimization problem using classical methods for bit allocation.
    - **[13]:** "As is typical in bit-allocation problems pursued in the theory of quantization [13], we solve problem (45) over the real domain and then round the solution to integer values that satisfy the constraints." This citation highlights the use of bit allocation techniques in quantization theory.
    - **[13], [13, §8.3]:** "According to (16), relation (51) is an upper bound on the compression error (13), which is known to be tight in the high resolution regime, i.e., for relatively high values of xk. We compute the solution to problem (45) by applying the Karush-Kuhn-Tucker (KKT) conditions see Appendix H. Under the high resolution approximation implied by (51), problem (45) can be solved in closed form by resorting to classical methods for optimal bit allocation with scalar quantizers. The closed-form solution will give us an insightful interpretation of the optimized allocation. Writing (51) as ŵk ≈ M2−2xk and neglecting the box constraints, problem (45) has the same structure as the bit allocation problem in [13]. By means of Lagrange multipliers, or applying the arithmetic/geometric mean inequality, the optimal allocation can be shown to be [13, §8