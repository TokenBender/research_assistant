## Analysis of "Active Retrieval Augmented Generation"

**1. Introduction:**

- **Title:** Active Retrieval Augmented Generation
- **Authors:** Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
- **Publication Date:** 22 October 2023 (v2)
- **Objective:** The paper proposes a novel approach called Forward-Looking Active Retrieval augmented generation (FLARE) to address the issue of hallucination in large language models (LLMs) by actively retrieving relevant information throughout the generation process.
- **Number of References:** 63

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:**
    - LLMs are prone to hallucination and generating factually inaccurate outputs.
    - Retrieval augmentation, where LLMs retrieve information from external knowledge resources, is a promising solution.
    - Existing retrieval-augmented LLMs typically use a retrieve-and-generate setup, retrieving information only once based on the input.
    - This approach is limiting for long-form generation tasks where continuous information gathering is crucial.
    - The paper introduces a generalized view of active retrieval augmented generation, where the model actively decides when and what to retrieve during the generation process.
- **Significant Citations:**
    - **Claim:** LLMs have a tendency to hallucinate and create factually inaccurate output.
        - **Citation:** (Maynez et al., 2020; Zhou et al., 2021)
        - **Explanation:** These citations highlight the problem of hallucination in LLMs, motivating the need for retrieval augmentation.
    - **Claim:** Augmenting LMs by retrieving information from external knowledge resources is a promising direction to address hallucination.
        - **Citation:** (Khandelwal et al., 2020; Izacard et al., 2022)
        - **Explanation:** These citations introduce the concept of retrieval augmentation as a solution to address hallucination in LLMs.
    - **Claim:** Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input.
        - **Citation:** (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023)
        - **Explanation:** This extensive list of citations provides a comprehensive overview of existing retrieval-augmented LLM approaches, highlighting their reliance on a single retrieval step.
    - **Claim:** These single-time retrieval augmented LMs outperform purely parametric LMs, particularly for short-form knowledge-intensive generation tasks such as factoid question answering (QA).
        - **Citation:** (Kwiatkowski et al., 2019; Joshi et al., 2017)
        - **Explanation:** These citations demonstrate the effectiveness of retrieval augmentation for specific tasks like QA, but highlight the limitations for more complex tasks.
    - **Claim:** Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output, such as long-form QA, open-domain summarization, and (chain-of-thought; CoT) reasoning.
        - **Citation:** (Fan et al., 2019; Stelmakh et al., 2022; Cohen et al., 2021; Hayashi et al., 2021; Giorgi et al., 2022; Wei et al., 2022; Ho et al., 2020; Geva et al., 2021; Hendrycks et al., 2020)
        - **Explanation:** These citations showcase the growing capabilities of LLMs in handling complex tasks, emphasizing the need for more sophisticated retrieval strategies for long-form generation.

**2.2 Retrieval Augmented Generation:**

- **Key Points:**
    - The paper formally defines single-time retrieval augmented generation and proposes a framework for active retrieval augmented generation.
    - The framework involves actively deciding when and what to retrieve during the generation process.
- **Significant Citations:**
    - **Claim:** The LM conditions on both the user input x and retrieved documents Dq to generate the answer.
        - **Citation:** (Ram et al., 2023; Trivedi et al., 2022)
        - **Explanation:** These citations introduce the common practice of prepending retrieved documents to the user input to aid future generation in retrieval-augmented LLMs.

**2.3 Active Retrieval Augmented Generation:**

- **Key Points:**
    - The paper proposes a generic framework for active retrieval augmented generation, actively deciding when and what to retrieve throughout the generation process.
    - The hypothesis is that LLMs should retrieve information only when they lack the required knowledge to avoid unnecessary or inappropriate retrieval.
    - The paper argues that low-probability/confidence tokens often indicate a lack of knowledge and proposes an active retrieval strategy that only retrieves when LLMs generate low-probability tokens.
    - The paper emphasizes the importance of considering what LMs intend to generate in the future when deciding what to retrieve.
- **Significant Citations:**
    - **Claim:** Large LMs tend to be well-calibrated and low probability/confidence often indicates a lack of knowledge.
        - **Citation:** (Kadavath et al., 2022)
        - **Explanation:** This citation provides evidence for the paper's hypothesis that low-probability tokens signal a need for retrieval.

**2.4 Forward-Looking Active Retrieval Augmented Generation (FLARE):**

- **Key Points:**
    - The paper proposes two methods for implementing FLARE:
        - FLAREinstruct: prompts the LLM to generate retrieval queries when necessary while generating the answer using retrieval-encouraging instructions.
        - FLAREdirect: directly uses the LLM's generation as search queries, iteratively generating the next sentence to gain insight into the future topic and retrieving relevant documents if uncertain tokens are present.
    - FLARE is applicable to any existing LMs at inference time without additional training.
- **Significant Citations:**
    - **Claim:** Inspired by Toolformer, a straightforward way of expressing information needs for retrieval is to generate â€œ[Search(query)]" when additional information is needed.
        - **Citation:** (Schick et al., 2023)
        - **Explanation:** This citation introduces the concept of using retrieval instructions to guide LLMs in generating queries, which is the basis for FLAREinstruct.

**2.5 Confidence-based Active Retrieval:**

- **Key Points:**
    - FLARE uses a confidence-based approach to trigger retrieval, retrieving only when the generated sentence contains low-probability tokens.
    - The paper argues that using sentences as the basis for retrieval is more effective than phrases or paragraphs.
- **Significant Citations:**
    - **Claim:** Large LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge.
        - **Citation:** (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022)
        - **Explanation:** These citations further support the paper's argument for using confidence scores to trigger retrieval.

**2.6 Confidence-based Query Formulation:**

- **Key Points:**
    - The paper proposes two methods for formulating retrieval queries based on the generated sentence:
        - Masked sentences as implicit queries: masks out low-confidence tokens in the generated sentence.
        - Generated questions as explicit queries: generates explicit questions targeting the low-confident span in the generated sentence.
- **Significant Citations:**
    - **Claim:** Methods that use generated hypothetical titles or paragraphs from LMs as retrieval queries or evidences have been proposed.
        - **Citation:** (Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021)
        - **Explanation:** These citations provide context for the paper's approach of using generated sentences as retrieval queries.
    - **Claim:** Self-ask achieved this by manually inserting follow-up questions into downstream task exemplars.
        - **Citation:** (Press et al., 2022)
        - **Explanation:** This citation highlights a previous approach to generating explicit queries, which the paper aims to improve upon with a more universal approach.

**2.7 Implementation Details:**

- **Key Points:**
    - The paper describes the implementation details of FLARE, including the base LLM used, the document corpus and retrievers, and the retrieved document formatting.
- **Significant Citations:**
    - **Claim:** The paper validates its method on one of the most advanced GPT-3.5 LMs, text-davinci-003.
        - **Citation:** (Ouyang et al., 2022)
        - **Explanation:** This citation introduces the specific LLM used in the experiments, providing context for the evaluation.
    - **Claim:** The paper uses off-the-shelf retrievers that take queries as inputs and return a list of relevant documents.
        - **Citation:** (Karpukhin et al., 2020; Robertson and Zaragoza, 2009)
        - **Explanation:** These citations introduce the retrievers used in the experiments, providing context for the retrieval process.

**2.8 Multi-time Retrieval Baselines:**

- **Key Points:**
    - The paper introduces three baseline categories for multi-time retrieval augmented LMs:
        - Previous-window approaches: trigger retrieval every l tokens, using the generated tokens from the previous window as the query.
        - Previous-sentence approaches: trigger retrieval every sentence, using the previous sentence as the query.
        - Question decomposition approaches: manually annotated task-specific exemplars to guide LMs to generate decomposed sub-questions.
- **Significant Citations:**
    - **Claim:** Existing passive multi-time retrieval augmented LMs can also be formulated using the paper's framework.
        - **Citation:** (Borgeaud et al., 2022; Ram et al., 2023; Khandelwal et al., 2020; Trivedi et al., 2022; Press et al., 2022; Yao et al., 2022)
        - **Explanation:** These citations introduce the baseline methods used for comparison, providing context for the evaluation.

**3. Key Insights and Supporting Literature:**

- **Key Insight:** FLARE outperforms all baselines on all tasks/datasets, demonstrating the effectiveness of active retrieval for long-form generation.
    - **Supporting Citations:** (Ho et al., 2020; Geva et al., 2021; Stelmakh et al., 2022; Hayashi et al., 2021)
    - **Explanation:** These citations introduce the datasets used for evaluation, providing context for the results.
- **Key Insight:** FLARE achieves the most significant improvement on multihop QA, highlighting the effectiveness of active retrieval for tasks with clear definitions and specific objectives.
    - **Supporting Citations:** (Ho et al., 2020)
    - **Explanation:** This citation introduces the multihop QA dataset, providing context for the observed improvement.
- **Key Insight:** FLARE outperforms question decomposition approaches, indicating that manual exemplar annotation is not necessary for effective future-aware retrieval.
    - **Supporting Citations:** (Press et al., 2022; Yao et al., 2022)
    - **Explanation:** These citations introduce the question decomposition approach, providing context for the comparison with FLARE.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:**
    - The paper evaluates FLARE on four diverse knowledge-intensive tasks: multihop QA, commonsense reasoning, long-form QA, and open-domain summarization.
    - The paper uses few-shot in-context learning with text-davinci-003 as the base LLM.
    - The paper compares FLARE with single-time and multi-time retrieval baselines.
- **Foundations:**
    - **Few-shot in-context learning:** (Radford et al., 2019; Brown et al., 2020; Liu et al., 2023)
    - **Multihop QA:** (Ho et al., 2020)
    - **Commonsense reasoning:** (Geva et al., 2021)
    - **Long-form QA:** (Stelmakh et al., 2022)
    - **Open-domain summarization:** (Hayashi et al., 2021)
- **Novel Aspects:**
    - The paper introduces a novel approach to active retrieval augmented generation, actively deciding when and what to retrieve during the generation process.
    - The paper proposes two methods for implementing FLARE: FLAREinstruct and FLAREdirect.
    - The paper uses a confidence-based approach to trigger retrieval, retrieving only when the generated sentence contains low-probability tokens.
    - The paper proposes two methods for formulating retrieval queries based on the generated sentence: masked sentences as implicit queries and generated questions as explicit queries.
- **Justification for Novel Approaches:**
    - The paper justifies its novel approach by highlighting the limitations of existing retrieval-augmented LLMs and by providing evidence for the effectiveness of active retrieval for long-form generation.

**5. Results in Context:**

- **Main Results:**
    - FLARE outperforms all baselines on all tasks/datasets, demonstrating the effectiveness of active retrieval for long-form generation.
    - FLARE achieves the most significant improvement on multihop QA, highlighting the effectiveness of active retrieval for tasks with clear definitions and specific objectives.
    - FLARE outperforms question decomposition approaches, indicating that manual exemplar annotation is not necessary for effective future-aware retrieval.
- **Comparison with Existing Literature:**
    - FLARE outperforms previous-window, previous-sentence, and question decomposition approaches, demonstrating its superiority over existing multi-time retrieval methods.
    - FLARE's performance on multihop QA is particularly noteworthy, surpassing even question decomposition approaches which require task-specific annotations.
- **Confirmation, Contradiction, or Extension:**
    - FLARE's results confirm the effectiveness of retrieval augmentation for long-form generation, extending previous work by demonstrating the benefits of active retrieval.
    - FLARE's results contradict the notion that manual exemplar annotation is necessary for effective future-aware retrieval, suggesting that a more general approach can be equally effective.

**6. Discussion and Related Work:**

- **Situating the Work:**
    - The authors situate their work within the broader context of retrieval augmented generation, highlighting the limitations of existing approaches and the need for more sophisticated methods.
    - The authors discuss the relationship between their work and previous research on iterative and adaptive retrieval, emphasizing the unique contributions of FLARE.
- **Key Papers Cited:**
    - (Peng et al., 2023; Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu et al., 2023; Mallen et al., 2022; Li et al., 2023; Nakano et al., 2021; Qin et al., 2023)
- **Highlighting Novelty:**
    - The authors highlight the novelty of FLARE by emphasizing its forward-looking nature, its ability to actively decide when and what to retrieve, and its applicability to any existing LMs at inference time without additional training.

**7. Future Work and Open Questions:**

- **Areas for Further Research:**
    - Developing better strategies for active retrieval, such as exploring different query formulation methods or incorporating external knowledge sources.
    - Designing efficient LM architectures for active information integration, addressing the computational overhead associated with interleaving generation and retrieval.
    - Investigating the effectiveness of FLARE for other knowledge-intensive tasks, such as dialogue generation or code generation.
- **Citations:**
    - The authors do not explicitly cite any specific works to support their suggestions for future work.

**8. Critical Analysis of Citation Usage:**

- **Effectiveness:**
    - The authors effectively use citations to support their arguments and findings, providing a comprehensive overview of existing literature and highlighting the novelty of their work.
- **Areas for Improvement:**
    - The authors could have provided more specific citations to support their suggestions for future work, further contextualizing their research direction.
- **Potential Biases:**
    - The authors primarily cite works from top-tier conferences and journals, potentially overlooking relevant research from other sources.

**9. Final Summary:**

- **Contribution:** The paper makes a significant contribution to the field of retrieval augmented generation by introducing FLARE, a novel approach that actively retrieves relevant information throughout the generation process. FLARE outperforms existing methods on a variety of knowledge-intensive tasks, demonstrating the effectiveness of active retrieval for long-form generation.
- **Influential Works:**
    - (Khandelwal et al., 2020; Izacard et al., 2022; Ram et al., 2023; Trivedi et al., 2022; Press et al., 2022; Yao et al., 2022; Schick et al., 2023)
- **Integration of Literature:**
    - The paper effectively integrates existing literature to support its claims and findings, providing a comprehensive overview of the field and highlighting the novelty of its work. The authors clearly demonstrate the limitations of existing approaches and the need for more sophisticated methods, setting the stage for their proposed solution.

Overall, the paper presents a compelling argument for the effectiveness of active retrieval augmented generation and introduces a novel approach, FLARE, that shows promising results on a variety of knowledge-intensive tasks. The paper effectively integrates existing literature to support its claims and findings, providing a valuable contribution to the field. However, the authors could have provided more specific citations to support their suggestions for future work, further contextualizing their research direction. Additionally, the authors could have explored a wider range of citations, potentially including relevant research from other sources.
