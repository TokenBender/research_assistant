## Interpretability at Scale: Identifying Causal Mechanisms in Alpaca

**1. Introduction**

- **Title:** Interpretability at Scale: Identifying Causal Mechanisms in Alpaca
- **Authors:** Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah D. Goodman
- **Publication Date:** 6 February 2024 (v3)
- **Objective:** The paper aims to scale interpretability methods to large language models (LLMs) by identifying the causal mechanisms underlying their behavior and demonstrating their robustness to unseen inputs and instructions.
- **References:** 60

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - The authors highlight the importance of interpretability for AI safety and the need for methods that are faithful to the causal dynamics of LLMs.
    - They introduce Distributed Alignment Search (DAS) [23] as a promising approach for uncovering interpretable causal structures in LLMs.
    - The paper proposes Boundless DAS, a scalable extension of DAS, to address the limitations of brute-force search in large models.
- **Significant Citations:**
    - **[23] Geiger, Wu, Potts, Icard, & Goodman (2023). Finding alignments between interpretable causal variables and distributed neural representations.** This citation introduces DAS, a key method used in the paper.
    - **[5, 22] Beckers & Halpern (2019); Geiger, Potts, & Icard (2023).** These citations introduce the theory of causal abstraction, which provides the theoretical framework for the paper's approach.

**2.2 Related Work**

- **Key Points:**
    - The authors review existing interpretability methods for deep learning models, highlighting their limitations in capturing causal mechanisms.
    - They discuss the theory of causal abstraction [41, 4, 5] as a unifying framework for interpretability methods.
    - They emphasize the lack of prior work scaling these methods to large, general-purpose LLMs.
- **Significant Citations:**
    - **[12, 1, 13, 45, 44, 60, 46, 52, 6, 14, 48, 24, 42, 12, 32, 8, 40, 34, 27, 39, 31, 29, 20, 50, 53].** These citations provide a broad overview of existing interpretability methods, highlighting their limitations in capturing causal mechanisms.
    - **[41, 4, 5, 19, 28, 20, 58, 54, 23, 21, 59, 58, 25, 38, 15, 30, 51, 33, 18, 16, 2, 35, 17, 36, 7, 54].** These citations discuss the theory of causal abstraction and its potential for mechanistic interpretability.

**2.3 Methods**

- **Key Points:**
    - The authors provide a background on causal models and interventions, focusing on interchange interventions.
    - They introduce Boundless DAS, a scalable extension of DAS that replaces brute-force search with learned parameters.
    - They describe the objective function and time complexity of Boundless DAS.
- **Significant Citations:**
    - **[23] Geiger, Wu, Potts, Icard, & Goodman (2023). Finding alignments between interpretable causal variables and distributed neural representations.** This citation provides the foundation for Boundless DAS.
    - **[57] Wu, Maruyama, & Leskovec (2022).** This citation inspires the use of learned boundary indices in Boundless DAS, drawing from work on neural PDEs.
    - **[33, 54] Meng, Bau, Andonian, & Belinkov (2022); Wang, Variengien, Conmy, Shlegeris, & Steinhardt (2022).** These citations highlight the limitations of prior work on causal abstraction, which focused on all-zero or mean value representation replacement.

**2.4 Experiment**

- **Key Points:**
    - The authors apply Boundless DAS to the Alpaca model (7B parameters) to understand its ability to solve a simple numerical reasoning task.
    - They propose four hypothesized causal models that could explain Alpaca's behavior.
    - They evaluate the performance of Boundless DAS using Interchange Intervention Accuracy (IIA).
- **Significant Citations:**
    - **[47] Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, & Hashimoto (2023). Stanford Alpaca: An instruction-following LLaMA model.** This citation introduces the Alpaca model, the subject of the paper's experiments.
    - **[26] Kingma & Ba (2015). Adam: A method for stochastic optimization.** This citation describes the optimization algorithm used in the experiments.

**2.5 Results**

- **Key Points:**
    - The authors find strong support for two of the hypothesized causal models, "Left Boundary" and "Left and Right Boundary," with high IIA scores.
    - They observe that the alignments are robust to changes in instructions, inputs, and output formats.
    - They discuss the limitations of IIA as a metric and provide evidence for its calibration.
- **Significant Citations:**
    - **[43] Shi, Chen, Misra, Scales, Dohan, Chi, Schärli, & Zhou (2023). Large language models can be easily distracted by irrelevant context.** This citation highlights the importance of testing the robustness of alignments to irrelevant context.

**2.6 Discussion**

- **Key Points:**
    - The authors discuss the implications of their findings for understanding the internal mechanisms of LLMs.
    - They acknowledge the limitations of Boundless DAS and suggest areas for future work.
    - They emphasize the importance of developing interpretability tools that are robust and generalizable.
- **Significant Citations:**
    - **[22] Geiger, Potts, & Icard (2023). Causal abstraction for faithful model interpretation.** This citation provides the theoretical foundation for the paper's claims about the implications of high IIA scores.

**3. Key Insights and Supporting Literature**

- **Key Insight 1:** Boundless DAS can effectively identify interpretable causal mechanisms in large LLMs, demonstrating the potential for scaling interpretability methods.
    - **Supporting Citations:**
        - **[23] Geiger, Wu, Potts, Icard, & Goodman (2023). Finding alignments between interpretable causal variables and distributed neural representations.** This citation introduces DAS, the foundation for Boundless DAS.
        - **[5, 22] Beckers & Halpern (2019); Geiger, Potts, & Icard (2023).** These citations introduce the theory of causal abstraction, which provides the theoretical framework for the paper's approach.
- **Key Insight 2:** The identified causal mechanisms in Alpaca are robust to changes in instructions, inputs, and output formats, suggesting that LLMs may implement interpretable algorithms that generalize across different contexts.
    - **Supporting Citations:**
        - **[43] Shi, Chen, Misra, Scales, Dohan, Chi, Schärli, & Zhou (2023). Large language models can be easily distracted by irrelevant context.** This citation highlights the importance of testing the robustness of alignments to irrelevant context.
        - **[22] Geiger, Potts, & Icard (2023). Causal abstraction for faithful model interpretation.** This citation provides the theoretical foundation for the paper's claims about the implications of high IIA scores.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The authors use the Alpaca model (7B parameters) trained on a simple numerical reasoning task.
    - They apply Boundless DAS to identify interpretable causal models that explain Alpaca's behavior.
    - They evaluate the performance of Boundless DAS using Interchange Intervention Accuracy (IIA).
- **Foundations:**
    - **[23] Geiger, Wu, Potts, Icard, & Goodman (2023). Finding alignments between interpretable causal variables and distributed neural representations.** This citation introduces DAS, the foundation for Boundless DAS.
    - **[57] Wu, Maruyama, & Leskovec (2022).** This citation inspires the use of learned boundary indices in Boundless DAS, drawing from work on neural PDEs.
- **Novel Aspects:**
    - The authors introduce Boundless DAS, a scalable extension of DAS that replaces brute-force search with learned parameters.
    - They use a novel approach to learn the dimensionality of the orthogonal linear subspaces in Boundless DAS.
    - **Justification:**
        - **[57] Wu, Maruyama, & Leskovec (2022).** This citation inspires the use of learned boundary indices in Boundless DAS, drawing from work on neural PDEs.

**5. Results in Context**

- **Main Results:**
    - Boundless DAS identifies two interpretable causal models, "Left Boundary" and "Left and Right Boundary," that explain Alpaca's behavior with high IIA scores.
    - The alignments are robust to changes in instructions, inputs, and output formats.
- **Comparison with Existing Literature:**
    - The authors compare their results to prior work on causal abstraction, highlighting the limitations of existing methods in scaling to large LLMs.
    - **[33, 54] Meng, Bau, Andonian, & Belinkov (2022); Wang, Variengien, Conmy, Shlegeris, & Steinhardt (2022).** These citations highlight the limitations of prior work on causal abstraction, which focused on all-zero or mean value representation replacement.
- **Confirmation, Contradiction, or Extension:**
    - The authors' results extend prior work on causal abstraction by demonstrating its scalability to large LLMs.
    - They also provide evidence for the robustness of causal alignments, which contradicts the concerns raised by some prior work.

**6. Discussion and Related Work**

- **Situating the Work:**
    - The authors situate their work within the broader context of interpretability research, highlighting the importance of developing methods that are faithful to the causal mechanisms of LLMs.
    - They acknowledge the limitations of Boundless DAS and suggest areas for future work.
- **Key Papers Cited:**
    - **[22] Geiger, Potts, & Icard (2023). Causal abstraction for faithful model interpretation.** This citation provides the theoretical foundation for the paper's claims about the implications of high IIA scores.
    - **[43] Shi, Chen, Misra, Scales, Dohan, Chi, Schärli, & Zhou (2023). Large language models can be easily distracted by irrelevant context.** This citation highlights the importance of testing the robustness of alignments to irrelevant context.
- **Novelty and Importance:**
    - The authors highlight the novelty of Boundless DAS as a scalable method for identifying causal mechanisms in LLMs.
    - They emphasize the importance of their findings for understanding the internal workings of LLMs and for developing robust and generalizable interpretability tools.

**7. Future Work and Open Questions**

- **Areas for Further Research:**
    - The authors suggest exploring the use of Boundless DAS to identify circuits in LLMs.
    - They also propose investigating the limitations of IIA as a metric and developing more robust and generalizable interpretability methods.
- **Citations:**
    - **[22] Geiger, Potts, & Icard (2023). Causal abstraction for faithful model interpretation.** This citation provides the theoretical foundation for the paper's claims about the implications of high IIA scores.

**8. Critical Analysis of Citation Usage**

- **Effectiveness:**
    - The authors effectively use citations to support their arguments and findings.
    - They provide a comprehensive overview of existing literature, highlighting both the strengths and limitations of prior work.
- **Areas for Improvement:**
    - The authors could have provided more citations to support their claims about the robustness of causal alignments.
    - They could have also discussed the potential biases in the selection of cited works, such as over-reliance on certain authors or publications.
- **Potential Biases:**
    - The authors primarily cite their own work, which may suggest a potential bias.
    - They also tend to cite work from Stanford University, which may reflect their institutional affiliation.

**9. Final Summary**

- **Contribution:** The paper makes a significant contribution to the field of interpretability by introducing Boundless DAS, a scalable method for identifying causal mechanisms in LLMs.
- **Influential Works:**
    - **[23] Geiger, Wu, Potts, Icard, & Goodman (2023). Finding alignments between interpretable causal variables and distributed neural representations.** This citation introduces DAS, the foundation for Boundless DAS.
    - **[5, 22] Beckers & Halpern (2019); Geiger, Potts, & Icard (2023).** These citations introduce the theory of causal abstraction, which provides the theoretical framework for the paper's approach.
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. It provides a comprehensive overview of prior work, highlighting both the strengths and limitations of existing methods. However, the authors could have provided more citations to support their claims about the robustness of causal alignments and discussed potential biases in the selection of cited works.

Overall, the paper makes a valuable contribution to the field of interpretability by introducing a scalable method for identifying causal mechanisms in LLMs and demonstrating its robustness to unseen inputs and instructions. The authors effectively use citations to support their arguments and findings, providing a comprehensive overview of existing literature. However, the paper could have benefited from a more balanced selection of cited works and a more detailed discussion of potential biases.
