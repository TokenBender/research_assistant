Okay, here's the markdown-formatted analysis of the provided academic paper, "Accelerating Transformer Inference for Translation via Parallel Decoding":


# Accelerating Transformer Inference for Translation via Parallel Decoding: A Citation-Based Analysis


## 1. Introduction

**Title:** Accelerating Transformer Inference for Translation via Parallel Decoding

**Authors:** Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, Emanuele Rodolà

**Publication Date:** May 17, 2023

**Main Objective:** The research aims to accelerate transformer-based machine translation inference by proposing novel parallel decoding algorithms that can be applied to existing autoregressive models without requiring model retraining or modifications.

**Total Number of References:** 69


## 2. Section-by-Section Analysis with Citation Extraction


### 2.1 Introduction

This section introduces the problem of slow inference in transformer-based machine translation (MT) due to autoregressive decoding. It highlights the need for speed improvements, especially in production environments and on embedded devices. The authors also briefly discuss the existing approaches (Non-Autoregressive Machine Translation - NAT) and their limitations.

**Significant Citations:**

* **Claim:** "In recent years there have been dramatic improvements in Machine Translation (MT) (Edunov et al., 2018; Liu et al., 2020) thanks to the transition to neural models and the advent of the Transformer architecture (Vaswani et al., 2017)."
    * **Citation:** Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding back-translation at scale. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pp. 489-500.
    * **Citation:** Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.
    * **Citation:** Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In *Advances in neural information processing systems*, 30.
    * **Explanation:** These citations establish the context of recent advancements in MT, emphasizing the role of neural models and the Transformer architecture in achieving high-quality translations.


* **Claim:** "Transformers are used sequentially at inference time, generating one token per time (i.e., sending each token as input for the next autoregressive iteration)."
    * **Citation:** (None explicitly provided, but implied by the general discussion of autoregressive decoding)
    * **Explanation:** This claim is foundational to the paper's argument, highlighting the inherent sequential nature of autoregressive decoding in transformers, which leads to latency issues.


* **Claim:** "To address this issue, the community proposed ad-hoc trained models specific for parallel machine translation under the umbrella term of Non-Autoregressive Machine Translation models (NAT) (Gu et al., 2018)."
    * **Citation:** Gu, J., Bradbury, J., Xiong, C., Li, V. O. K., & Socher, R. (2018). Non-autoregressive neural machine translation. In *International Conference on Learning Representations*.
    * **Explanation:** This citation introduces the concept of NAT models, which are designed to produce translations in parallel, as a potential solution to the speed limitations of autoregressive decoding.


* **Claim:** "These models produce the translation in parallel but require (i) a complete reengineering of the MT system, (ii) extensive training resources and (iii) complex design choices like distillation from larger autoregressive models."
    * **Citation:** (Implied by the discussion of NAT models and their limitations)
    * **Explanation:** This claim highlights the significant drawbacks of NAT models, which make them difficult to implement in practical settings.


* **Claim:** "These requirements are quite demanding and not easily satisfiable. For example, production systems are heavily optimized for hardware and software and even introducing a minimal modification requires non-trivial human effort (Wu et al., 2016; Kim et al., 2019)."
    * **Citation:** Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., ... & Chintala, S. (2016). Google's neural machine translation system: Bridging the gap between human and machine translation. *arXiv preprint arXiv:1609.08144*.
    * **Citation:** Kim, Y. J., Junczys-Dowmunt, M., Hassan, H., Aji, A. F., Heafield, K., Bogoychev, N., ... & Grundkiewicz, M. (2019). From research to production and back: Ludicrously fast neural machine translation. In *Proceedings of the 3rd Workshop on Neural Generation and Translation*, pp. 280-288.
    * **Explanation:** These citations provide concrete examples of the challenges associated with modifying existing production MT systems, emphasizing the difficulty of integrating NAT models into such systems.


### 2.2 Related Work

This section reviews the existing literature on parallel machine translation, focusing on Non-Autoregressive Machine Translation (NAT) models and other approaches that aim to improve inference speed. It highlights the trade-off between translation quality and speed, and the challenges associated with achieving high-quality parallel translations.

**Significant Citations:**

* **Claim:** "Gu et al. (2018) first introduced Non-Autoregressive Translation models (NAT) as ad-hoc trained models capable of producing the translation all at once in parallel."
    * **Citation:** Gu, J., Bradbury, J., Xiong, C., Li, V. O. K., & Socher, R. (2018). Non-autoregressive neural machine translation. In *International Conference on Learning Representations*.
    * **Explanation:** This citation establishes the origin of NAT models, which are a key focus of the related work discussion.


* **Claim:** "With NATs, it is possible to consistently reduce the latency and speed up the translation at the expense of a slightly worse translation quality due to the multimodality problem (i.e., we lose the dependency between tokens in the target output)."
    * **Citation:** (Implied by the discussion of NAT models and their limitations)
    * **Explanation:** This claim highlights the inherent trade-off in NAT models, where speed improvements come at the cost of potential degradation in translation quality due to the loss of token dependencies.


* **Claim:** "Finding a tradeoff between translation quality and speed is an active research direction, with current methods trying to fill the gap in terms of translation quality (Geng et al., 2021; Savinov et al., 2022)."
    * **Citation:** Geng, X., Feng, X., & Qin, B. (2021). Learning to rewrite for non-autoregressive neural machine translation. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pp. 3297-3308.
    * **Citation:** Savinov, N., Chung, J., Binkowski, M., Elsen, E., & Oord, A. v. d. (2022). Step-unrolled denoising autoencoders for text generation. In *International Conference on Learning Representations*.
    * **Explanation:** These citations demonstrate that researchers are actively working on improving the translation quality of NAT models, acknowledging the limitations of the initial NAT approaches.


* **Claim:** "The most common is the sequence-level knowledge distillation of large autoregressive models into parallel models (Kim and Rush, 2016)."
    * **Citation:** Kim, Y., & Rush, A. M. (2016). Sequence-level knowledge distillation. In *Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing*, pp. 1317-1327.
    * **Explanation:** This citation highlights a common technique used to improve the quality of NAT models by leveraging knowledge from larger autoregressive models.


* **Claim:** "Other approaches include defining alternative training objectives (Ghazvininejad et al., 2020a; Saharia et al., 2020; Du et al., 2021; Huang et al., 2021), architectures that model dependencies between output sentence tokens (Ghazvininejad et al., 2019; Qian et al., 2021; Song et al., 2021a; Gu and Kong, 2021; Song et al., 2022) or multi-iteration methods (Ghazvininejad et al., 2020b; Kasai et al., 2020; Hao et al., 2021; Geng et al., 2021; Savinov et al., 2022; Huang et al., 2022; Xia et al., 2022) that apply iterative refinements to a translation, trading some speed for greater quality."
    * **Citation:** Ghazvininejad, M., Karpukhin, V., Zettlemoyer, L., & Levy, O. (2020a). Aligned cross entropy for non-autoregressive machine translation. In *Proceedings of the 37th International Conference on Machine Learning*, pp. 3515-3523.
    * **Citation:** Saharia, C., Chan, W., Saxena, S., & Norouzi, M. (2020). Non-autoregressive machine translation with latent alignments. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing*, pp. 1098-1108.
    * **Citation:** Du, C., Tu, Z., & Jiang, J. (2021). Order-agnostic cross entropy for non-autoregressive machine translation. In *International Conference on Machine Learning*, pp. 2849-2859.
    * **Citation:** Huang, C., Zhou, H., Zaïane, O. R., Mou, L., & Li, L. (2021). Non-autoregressive translation with layer-wise prediction and deep supervision. *arXiv preprint arXiv:2110.07515*.
    * **Citation:** Ghazvininejad, M., Levy, O., Liu, Y., & Zettlemoyer, L. (2019). Mask-predict: Parallel decoding of conditional masked language models. In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pp. 6112-6121.
    * **Citation:** Qian, L., Zhou, H., Bao, Y., Wang, M., Qiu, L., Zhang, W., ... & Li, L. (2021). Glancing transformer for non-autoregressive neural machine translation. In *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pp. 1993-2003.
    * **Citation:** Song, J., Meng, C., Liao, R., & Ermon, S. (2021b). Accelerating feedforward computation via parallel nonlinear equation solving. In *International Conference on Machine Learning*, pp. 9791-9800.
    * **Citation:** Gu, J., & Kong, X. (2021). Fully non-autoregressive neural machine translation: Tricks of the trade. In *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*, pp. 120-133.
    * **Citation:** Song, Z., Zhou, H., Qian, L., Xu, J., Cheng, S., Wang, M., & Li, L. (2022). Switch-glat: Multilingual parallel machine translation via code-switch decoder. In *International Conference on Learning Representations*.
    * **Citation:** Ghazvininejad, M., Levy, O., & Zettlemoyer, L. (2020b). Semi-autoregressive training improves mask-predict decoding. *arXiv preprint arXiv:2001.08785*.
    * **Citation:** Kasai, J., Pappas, N., Peng, H., Cross, J., & Smith, N. (2021). Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation. In *International Conference on Learning Representations*.
    * **Citation:** Hao, Y., He, S., Jiao, W., Tu, Z., Lyu, M., & Wang, X. (2021). Multi-task learning with shared encoder for non-autoregressive machine translation. In *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pp. 3989-3996.
    * **Citation:** Geng, X., Feng, X., & Qin, B. (2021). Learning to rewrite for non-autoregressive neural machine translation. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pp. 3297-3308.
    * **Citation:** Savinov, N., Chung, J., Binkowski, M., Elsen, E., & Oord, A. v. d. (2022). Step-unrolled denoising autoencoders for text generation. In *International Conference on Learning Representations*.
    * **Citation:** Huang, X. S., Perez, F., & Volkovs, M. (2022). Improving non-autoregressive translation models without distillation. In *International Conference on Learning Representations*.
    * **Citation:** Xia, Y., Ge, T., Wei, F., & Sui, Z. (2022). Lossless speedup of autoregressive translation with generalized aggressive decoding. *arXiv preprint arXiv:2204.09269*.
    * **Explanation:** This extensive list of citations demonstrates the breadth of research on alternative approaches to improve MT inference speed, including various training objectives, model architectures, and iterative refinement techniques.


* **Claim:** "In our approach, we also employ iterative refinements of solutions to non-linear equations, but we do not perform any training or modification to the model."
    * **Citation:** Song, Y., Meng, C., Liao, R., & Ermon, S. (2021b). Accelerating feedforward computation via parallel nonlinear equation solving. In *International Conference on Machine Learning*, pp. 9791-9800.
    * **Explanation:** This citation connects the authors' approach to the broader field of iterative methods for solving non-linear equations, highlighting the novelty of applying these techniques to MT decoding.


* **Claim:** "Other works that require retraining or modifications to the model add additional decoding heads (Stern et al., 2018) or use shallow decoders (Kasai et al., 2021)."
    * **Citation:** Stern, M., Shazeer, N., & Uszkoreit, J. (2018). Blockwise parallel decoding for deep autoregressive models. In *Advances in Neural Information Processing Systems*, 31.
    * **Citation:** Kasai, J., Pappas, N., Peng, H., Cross, J., & Smith, N. (2021). Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation. In *International Conference on Learning Representations*.
    * **Explanation:** These citations highlight alternative approaches that require model modifications, contrasting them with the authors' approach, which aims to be model-agnostic.


* **Claim:** "We refer the reader to Xiao et al. (2022) for a thorough survey on NAT methods."
    * **Citation:** Xiao, Y., Wu, L., Guo, J., Li, J., Zhang, M., Qin, T., & Liu, T. (2022). A survey on non-autoregressive generation for neural machine translation and beyond. *arXiv preprint arXiv:2204.09269*.
    * **Explanation:** This citation directs readers to a comprehensive overview of NAT methods, providing a broader context for the authors' work.


* **Claim:** "In the context of Grammatical Error Correction, Sun et al. (2021) recently proposed aggressive parallel decoding, assuming that the model output is similar to the input."
    * **Citation:** Sun, X., Ge, T., Wei, F., & Wang, H. (2021). Instantaneous grammatical error correction with shallow aggressive decoding. In *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pp. 5937-5947.
    * **Explanation:** This citation introduces a related work in a different domain (Grammatical Error Correction) that also explores parallel decoding, providing further context for the authors' approach.


* **Claim:** "More recently, inspiring our work, Song et al. (2021b) showed that it is possible to parallelize feedforward computations by thinking of them as a system of non-linear equations."
    * **Citation:** Song, Y., Meng, C., Liao, R., & Ermon, S. (2021b). Accelerating feedforward computation via parallel nonlinear equation solving. In *International Conference on Machine Learning*, pp. 9791-9800.
    * **Explanation:** This citation highlights a key inspiration for the authors' work, demonstrating the feasibility of parallelizing feedforward computations in other domains, which they adapt to MT decoding.


* **Claim:** "While this work was under submission and anonymity period, Leviathan et al. (2022), Chen et al. (2023) and Kim et al. (2023) concurrently proposed decoding approaches that speed up inference of a large transformer model by using another smaller model to draft tokens."
    * **Citation:** Leviathan, Y., Kalman, M., & Matias, Y. (2022). Fast inference from transformers via speculative decoding.
    * **Citation:** Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., & Jumper, J. (2023). Accelerating large language model decoding with speculative sampling.
    * **Citation:** Kim, S., Mangalam, K., Malik, J., Mahoney, M. W., Gholami, A., & Keutzer, K. (2023). Big little transformer decoder.
    * **Explanation:** These citations acknowledge concurrent work in the field that also addresses the challenge of accelerating MT inference, but through different approaches (using a smaller model for token drafting).


* **Claim:** "Compared to these approaches our method requires just an existing autoregressive model (no matter the size) and mathematically guarantees the output quality."
    * **Citation:** (Implied by the comparison with concurrent work)
    * **Explanation:** This claim emphasizes the key advantage of the authors' approach: it's model-agnostic and provides mathematical guarantees on the output quality, unlike the concurrent work that relies on additional model training or modifications.


### 2.3 Method

This section details the proposed parallel decoding approach, including notation, the formulation of the decoding problem as a system of non-linear equations, and the three proposed algorithms (Parallel Jacobi, Parallel GS-Jacobi, and Hybrid GS-Jacobi). It also discusses initialization and stopping conditions, and quality guarantees.

**Significant Citations:**

* **Claim:** "The goal of MT is to translate a sentence x in a source language (e.g., Italian) with its translation y in the target language (e.g., English)."
    * **Citation:** (None explicitly provided, but implied by the general discussion of MT)
    * **Explanation:** This statement establishes the fundamental goal of MT, which provides the context for the subsequent discussion of decoding algorithms.


* **Claim:** "Source and target sentences are generally tokenized in words or subwords (Kudo and Richardson, 2018; Schuster and Nakajima, 2012; Sennrich et al., 2016; Kudo, 2018)."
    * **Citation:** Kudo, T., & Richardson, J. (2018). Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. *arXiv preprint arXiv:1808.06226*.
    * **Citation:** Schuster, M., & Nakajima, K. (2012). Japanese and korean voice search. In *2012 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2012*, pp. 5149-5152.
    * **Citation:** Sennrich, R., Haddow, B., & Birch, A. (2016). Neural machine translation of rare words with subword units. In *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 1715-1725.
    * **Citation:** Kudo, T. (2018). Subword regularization: Improving neural network translation models with multiple subword candidates. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pp. 66-75.
    * **Explanation:** These citations provide the background on tokenization techniques commonly used in MT, which are essential for the subsequent formulation of the decoding problem.


* **Claim:** "From a probabilistic perspective, an MT model estimates pe(y | x)."
    * **Citation:** (None explicitly provided, but implied by the general discussion of probabilistic modeling in MT)
    * **Explanation:** This statement introduces the probabilistic framework used in MT, where the model aims to estimate the probability of a target sentence given a source sentence.


* **Claim:** "The inference phase is traditionally performed by sampling tokens from the model probability conditioned on the input sequence x and previously generated tokens (y1, ………, Yi−1)."
    * **Citation:** (None explicitly provided, but implied by the general discussion of autoregressive decoding)
    * **Explanation:** This statement describes the standard autoregressive decoding process, where tokens are generated sequentially, conditioned on the input and previously generated tokens.


* **Claim:** "Given the formalization above, a standard autoregressive setting runs m inference steps sequentially to generate an output sequence of m elements."
    * **Citation:** (None explicitly provided, but implied by the general discussion of autoregressive decoding)
    * **Explanation:** This statement summarizes the standard autoregressive decoding process, which is the baseline against which the authors' parallel decoding methods are compared.


* **Claim:** "Parallel Decoding. Given Equation (2), it is possible to write the greedy decoding procedure on all tokens as:"
    * **Citation:** (None explicitly provided, but implied by the reformulation of the decoding problem)
    * **Explanation:** This statement introduces the core idea of parallel decoding, where the decoding process is reformulated as a system of equations that can be solved in parallel.


* **Claim:** "Defining f (yi, У1:і−1, x) = Yi arg max po (Yi | 1:i-1, x), we can rewrite the system of Equations (3) as:"
    * **Citation:** (None explicitly provided, but implied by the reformulation of the decoding problem)
    * **Explanation:** This statement shows how the decoding problem is mathematically formulated as a system of non-linear equations, which is the basis for the proposed parallel decoding algorithms.


* **Claim:** "This system has m non-linear equations (each equation employ a neural network) with m variables."
    * **Citation:** (None explicitly provided, but implied by the reformulation of the decoding problem)
    * **Explanation:** This statement emphasizes the complexity of the system of equations, highlighting the challenge of solving it efficiently.


* **Claim:** "The autoregressive decoding implicitly solves the system of Equations (4) by substitution, i.e., given the [BOS] token and the input sentence x, it solves equations from first to last, progressively replacing the resolved variables."
    * **Citation:** (None explicitly provided, but implied by the comparison with autoregressive decoding)
    * **Explanation:** This statement explains how the standard autoregressive decoding process implicitly solves the system of equations, providing a contrast with the proposed parallel decoding methods.


* **Claim:** "In this paper, we rely on Jacobi and Gauss-Seidel (GS) fixed-point iteration methods (Ortega and Rheinboldt, 1970) to solve in parallel system (4) until a stopping condition is reached."
    * **Citation:** Ortega, J. M., & Rheinboldt, W. C. (1970). *Iterative solution of nonlinear equations in several variables*. Society for Industrial and Applied Mathematics.
    * **Explanation:** This citation introduces the specific iterative methods (Jacobi and Gauss-Seidel) that the authors use to solve the system of equations in parallel, forming the core of their proposed algorithms.


* **Claim:** "This formulation is particularly flexible and has several advantages: Firstly, it is completely agnostic to the underlying MT model used; Secondly, it can be analyzed with analytical tools and has guarantees of convergence to the exact solution for system (4); Thirdly, it can be potentially extended by drawing from the numerical methods literature for non-linear equations solving methods (Saad, 2003)."
    * **Citation:** Saad, Y. (2003). *Iterative methods for sparse linear systems*. Society for Industrial and Applied Mathematics.
    * **Explanation:** This statement highlights the key advantages of the proposed parallel decoding formulation, including its model-agnostic nature, analytical tractability, and potential for further development using existing numerical methods.


* **Claim:** "We see that, with the proper stopping condition, it is possible to have quality guarantees over the output."
    * **Citation:** (Implied by the discussion of stopping conditions and quality guarantees)
    * **Explanation:** This claim emphasizes the importance of the stopping condition in ensuring the quality of the translated output.


* **Claim:** "We present here three algorithms (PJ, PGJ, HGJ) that leverage these fixed-point iteration methods to speedup decoding in MT."
    * **Citation:** (None explicitly provided, but implied by the introduction of the three algorithms)
    * **Explanation:** This statement introduces the three specific parallel decoding algorithms that are the core contribution of the paper.


* **Claim:** "Parallel Jacobi (PJ) Decoding. First, we propose Algorithm 1. This algorithm works by initializing a draft translation for the whole target sentence and then iteratively translating the whole sentence in parallel until the stopping condition is triggered."
    * **Citation:** (None explicitly provided, but implied by the description of Algorithm 1)
    * **Explanation:** This statement introduces the first parallel decoding algorithm, Parallel Jacobi, and describes its core functionality.


* **Claim:** "Parallel GS-Jacobi (PGJ) Decoding. Decoding the whole target sentence in parallel may introduce difficulties in inferring long dependencies between tokens since the underlying model is trained to model the conditional distribution of a token given the previous tokens."
    * **Citation:** (None explicitly provided, but implied by the description of Algorithm 2)
    * **Explanation:** This statement introduces the second parallel decoding algorithm, Parallel GS-Jacobi, and explains the motivation for its design, which addresses potential issues with long-range dependencies in the parallel decoding of the entire sentence.


* **Claim:** "Hybrid GS-Jacobi (HGJ) Decoding. Algorithms 1 and 2 assume to know beforehand the number of equations m (i.e., the target length)."
    * **Citation:** (None explicitly provided, but implied by the description of Algorithm 3)
    * **Explanation:** This statement introduces the third parallel decoding algorithm, Hybrid GS-Jacobi, and highlights a limitation of the previous two algorithms (requiring knowledge of the target length).


* **Claim:** "Compared to NAT methods which do not have any quality guarantee since a novel parallel model is trained from scratch, our formulation guarantees to have the same quality of using autoregressive decoding with the same MT model."
    * **Citation:** (Implied by the discussion of quality guarantees)
    * **Explanation:** This statement emphasizes a key advantage of the proposed approach: it provides a guarantee of translation quality that is equivalent to the standard autoregressive decoding, unlike NAT models, which require retraining and may sacrifice quality for speed.


* **Claim:** "System (4) is known in literature as a triangular system of m equations with m variables, this characterization allows to state an important property."
    * **Citation:** (None explicitly provided, but implied by the mathematical formulation of the decoding problem)
    * **Explanation:** This statement connects the mathematical formulation of the decoding problem to existing literature, highlighting its properties and providing a basis for the subsequent proposition.


* **Claim:** "Proposition 1. Algorithms 1, 2, 3 converge and yield the same results of greedy autoregressive decoding in at most m parallel iterations, for any initialization and providing stopping condition (5)."
    * **Citation:** Song, Y., Meng, C., Liao, R., & Ermon, S. (2021b). Accelerating feedforward computation via parallel nonlinear equation solving. In *International Conference on Machine Learning*, pp. 9791-9800.
    * **Explanation:** This proposition formally states a key property of the proposed algorithms: they converge to the same solution as the standard autoregressive decoding within a maximum number of iterations, providing a theoretical guarantee of their correctness.


* **Claim:** "Equation 1 models the dependency between tokens in the decoding phase."
    * **Citation:** (None explicitly provided, but implied by the discussion of token dependencies)
    * **Explanation:** This statement introduces the concept of token dependencies in the decoding process, which is the basis for the subsequent discussion of the DDGviz tool.


* **Claim:** "In the standard autoregressive decoding this graph is a fully-connected chain where the i-th token is connected to all the previous tokens, starting from the encoding x: to decode yi you need to decode first Y1,..., Yi−1."
    * **Citation:** (None explicitly provided, but implied by the discussion of autoregressive decoding)
    * **Explanation:** This statement describes the dependency graph in the standard autoregressive decoding, providing a contrast with the relaxed dependencies that can be observed in parallel decoding.


### 2.4 Experiments

This section describes the experimental setup, including the datasets used, evaluation metrics, and model configurations. It presents the results of the experiments, comparing the proposed parallel decoding algorithms with the standard autoregressive decoding and beam search, and analyzes the impact of scaling on parallel resources.

**Significant Citations:**

* **Claim:** "Datasets. We evaluate our approach using standard evaluation datasets proposed for parallel MT (Gu et al., 2018): WMT14 English-German [En-De], WMT16 English-Romanian [En-Ro] (Bojar et al., 2014, 2016)."
    * **Citation:** Gu, J., Bradbury, J., Xiong, C., Li, V. O. K., & Socher, R. (2018). Non-autoregressive neural machine translation. In *International Conference on Learning Representations*.
    * **Citation:** Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., ... & Tamchyna, A. (2014). Findings of the 2014 workshop on statistical machine translation. In *Proceedings of the Ninth Workshop on Statistical Machine Translation*, pp. 12-58.
    * **Citation:** Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., ... & Turchi, M. (2016). Findings of the 2016 conference on machine translation. In *Proceedings of the First Conference on Machine Translation*, pp. 131-198.
    * **Explanation:** These citations justify the selection of the WMT datasets for evaluating the proposed algorithms, highlighting their widespread use in the MT research community.


* **Claim:** "Additionally, we tested our method on different language pairs with varying (low-medium) resources: IWSLT15 (English-Vietnamese [En-Vi]) (Tran et al., 2015), IITB (English-Hindi [En-Hi]) (Kunchukuttan et al., 2018), WMT17 (English-Finnish [En-Fi]) (Bojar et al., 2017), FLORES-101 (English-Italian [En-It]; English-French [En-Fr]) (Goyal et al., 2022)."
    * **Citation:** Tran, Y., Vu, H. T., Vinh, N. V., & Tien, T. L. (2015). The English-Vietnamese machine translation system for IWSLT 2015. In *Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign*, pp. 80-83.
    * **Citation:** Kunchukuttan, A., Mehta, P., & Bhattacharyya, P. (2018). The IIT Bombay English-Hindi parallel corpus. In *Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)*.
    * **Citation:** Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., ... & Turchi, M. (2017). Findings of the 2017 conference on machine translation (wmt17). In *Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers*, pp. 169-214.
    * **Citation:** Goyal, N., Gao, C., Chaudhary, V., Chen, P.-J., Wenzek, G., Ju, D., ... & Fan, A. (2022). The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. *Transactions of the Association for Computational Linguistics*, 10, 522-538.
    * **Explanation:** These citations justify the inclusion of additional language pairs in the evaluation, demonstrating the broader applicability of the proposed algorithms to different language scenarios and resource levels.


* **Claim:** "Evaluation. All the evaluations are performed using the official test split for each dataset, downloaded using Huggingface dataset library (Lhoest et al., 2021)."
    * **Citation:** Lhoest, Q., Villanova del Moral, A., Jernite, Y., Thakur, A., von Platen, P., Tunstall, L., ... & Wolf, T. (2021). Datasets: A community library for natural language processing. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pp. 175-184.
    * **Explanation:** This citation justifies the use of the Huggingface library for accessing and managing the evaluation datasets, highlighting its role in facilitating reproducible research.


* **Claim:** "We use SacreBLEU to evaluate the translation quality (Papineni et al., 2002; Post, 2018)."
    * **Citation:** Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. In *Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics*, pp. 311-318.
    * **Citation:** Post, M. (2018). A call for clarity in reporting BLEU scores. In *Proceedings of the Third Conference on Machine Translation: Research Papers*, pp. 186-191.
    * **Explanation:** These citations justify the use of SacreBLEU as the primary evaluation metric for translation quality, highlighting its importance in the MT research community.


* **Claim:** "We tested transformer models in the two standard configurations: base (512 model dimension, 6 attention layers for both encoder and decoder) and big (1024 model dimension, 12 attention layers for both encoder and decoder)."
    * **Citation:** (None explicitly provided, but implied by the description of the model configurations)
    * **Explanation:** This statement describes the specific model configurations used in the experiments, providing details