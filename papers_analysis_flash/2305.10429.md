Okay, here's a comprehensive analysis of the paper "DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining" in Markdown format, following the structure you provided:


# DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining

## 1. Introduction

**Title:** DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining

**Authors:** Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V. Le, Tengyu Ma, and Adams Wei Yu

**Publication Date:** November 21, 2023 (arXiv preprint)

**Main Objective:** This research aims to improve the efficiency and performance of language model pretraining by optimizing the mixture proportions (domain weights) of the training data using a novel algorithm called DoReMi.

**Total Number of References:** 77


## 2. Section-by-Section Analysis with Citation Extraction

### 2.1 Introduction

**Summary:** The introduction highlights the impact of data domain mixtures on language model (LM) performance, emphasizing the challenge of determining optimal domain weights for diverse downstream tasks. Existing methods rely on intuition or tuning weights based on specific downstream tasks, which can be inefficient and lead to overfitting. DoReMi proposes a novel approach to optimize domain weights without prior knowledge of downstream tasks.

**Significant Citations:**

* **Claim:** "Datasets for training language models (LMs) are typically sampled from a mixture of many domains (Brown et al., 2020, Chowdhery et al., 2022, Du et al., 2021, Gao et al., 2020)."
    * **Citation:** Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Sutskever, I. (2020). Language models are few-shot learners. *arXiv preprint arXiv:2005.14165*.
    * **Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Barham, P., ... & Dean, J. (2022). PaLM: Scaling language modeling with pathways.* arXiv preprint arXiv:2204.02311*.
    * **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2021). GLaM: Efficient scaling of language models with mixture-of-experts.* arXiv preprint arXiv:2110.04497*.
    * **Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., ... & Leahy, C. (2020). The Pile: An 800GB dataset of diverse text for language modeling.* arXiv preprint arXiv:2001.02390*.
    * **Relevance:** These citations establish the common practice of using diverse data sources for LM pretraining and highlight the importance of data composition in achieving good performance.


* **Claim:** "The composition of the pretraining data greatly affects the effectiveness of an LM (Du et al., 2021, Hoffmann et al., 2022, Xie et al., 2023)."
    * **Citation:** Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2021). GLaM: Efficient scaling of language models with mixture-of-experts.* arXiv preprint arXiv:2110.04497*.
    * **Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., ... & Sifre, L. (2022). An empirical analysis of compute-optimal large language model training.* Advances in Neural Information Processing Systems*.
    * **Xie, S. M., Santurkar, S., Ma, T., & Liang, P. (2023). Data selection for language models via importance resampling.* arXiv preprint arXiv:2302.03169*.
    * **Relevance:** These citations emphasize the significant impact of data composition on LM performance, motivating the need for a systematic approach to optimize domain weights.


* **Claim:** "Existing works determine domain weights (the sampling probabilities for each domain) by using intuition or a set of downstream tasks."
    * **Citation:** Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Barham, P., ... & Dean, J. (2022). PaLM: Scaling language modeling with pathways.* arXiv preprint arXiv:2204.02311*.
    * **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2021). GLaM: Efficient scaling of language models with mixture-of-experts.* arXiv preprint arXiv:2110.04497*.
    * **Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., ... & Leahy, C. (2020). The Pile: An 800GB dataset of diverse text for language modeling.* arXiv preprint arXiv:2001.02390*.
    * **Relevance:** These citations illustrate the current practices of either relying on intuition or tuning domain weights based on downstream tasks, highlighting the limitations of these approaches.


### 2.2 Domain Reweighting with Minimax Optimization (DoReMi)

**Summary:** This section formally introduces the DoReMi algorithm, which leverages distributionally robust optimization (DRO) to optimize domain weights. It outlines the three main steps of DoReMi: training a small reference model, training a proxy model with Group DRO to obtain domain weights, and training a large model using the optimized domain weights.

**Significant Citations:**

* **Claim:** "Instead of optimizing domain weights based on a set of downstream tasks, our approach aims to find domain weights which lead to models that perform well on all domains by minimizing the worst-case excess loss over domains, following Mindermann et al. (2022), Oren et al. (2019)."
    * **Citation:** Mindermann, S., Brauner, J., Razzak, M., Sharma, M., Kirsch, A., Xu, W., ... & Gal, Y. (2022). Prioritized training on points that are learnable, worth learning, and not yet learnt. *International Conference on Machine Learning*.
    * **Oren, Y., Sagawa, S., Hashimoto, T., & Liang, P. (2019). Distributionally robust language modeling.* Empirical Methods in Natural Language Processing*.
    * **Relevance:** These citations establish the connection between DoReMi's objective and the concept of worst-case excess loss, which is a key aspect of DRO. They also highlight the related work that inspired the approach.


* **Claim:** "Our approach adapts the DRO-LM framework (Oren et al., 2019) to optimize domain weights instead of producing a robust model."
    * **Citation:** Oren, Y., Sagawa, S., Hashimoto, T., & Liang, P. (2019). Distributionally robust language modeling.* Empirical Methods in Natural Language Processing*.
    * **Relevance:** This citation explicitly connects DoReMi to the DRO-LM framework, emphasizing that DoReMi adapts this framework for a different purpose (optimizing domain weights rather than training a robust model).


* **Claim:** "DoReMi uses the online learning-based optimizer from Group DRO (Nemirovski et al., 2009, Sagawa et al., 2020), which dynamically updates domain weights according to the loss on each domain for rescaling the training objective."
    * **Citation:** Nemirovski, A., Juditsky, A., Lan, G., & Shapiro, A. (2009). Robust stochastic approximation approach to stochastic programming. *SIAM Journal on Optimization*.
    * **Sagawa, S., Koh, P. W., Hashimoto, T., & Liang, P. (2020). Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.* International Conference on Learning Representations*.
    * **Relevance:** These citations provide the foundation for the optimization method used in DoReMi, specifically the Group DRO optimizer, which is crucial for efficiently updating domain weights during training.


### 2.3 Algorithm 1: DoReMi Domain Reweighting (Step 2)

**Summary:** This section provides the pseudocode for the core part of DoReMi: training the proxy model with Group DRO to obtain the optimized domain weights. It details the steps involved in computing per-domain excess losses, updating domain weights, and updating the proxy model's parameters.

**Significant Citations:**

* **Claim:** "We follow Sagawa et al. (2020) and sample a minibatch with uniform domain weights (regardless of the reference domain weights aref, which only affects the reference model)."
    * **Citation:** Sagawa, S., Koh, P. W., Hashimoto, T., & Liang, P. (2020). Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.* International Conference on Learning Representations*.
    * **Relevance:** This citation acknowledges the inspiration for the minibatch sampling strategy used in DoReMi, which is based on the Group DRO approach.


* **Claim:** "Finally, we update the proxy model for the objective L(0t−1, at) using a standard optimizer such as Adam (Kingma and Ba, 2015) or Adafactor (Shazeer and Stern, 2018)."
    * **Citation:** Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. *International Conference on Learning Representations*.
    * **Shazeer, N., & Stern, M. (2018). Adafactor: Adaptive learning rates with sublinear memory cost.* arXiv preprint arXiv:1804.04235*.
    * **Relevance:** These citations indicate the optimization algorithms used to update the proxy model's parameters, which are standard techniques in deep learning.


### 2.4 Iterated DoReMi

**Summary:** This section describes an extension to DoReMi where the process is iterated multiple times, using the optimized domain weights from the previous round as the reference weights for the next round. This iterative approach helps the domain weights converge to a stable solution.

**Significant Citations:** (None explicitly cited in this section, but the concept builds upon the core DoReMi algorithm described in previous sections.)


### 2.5 DoReMi Improves LM Training Efficiency and Performance

**Summary:** This section presents the experimental setup and results of using DoReMi to optimize domain weights for training larger language models on The Pile and GLaM datasets. It highlights the improvements in perplexity, downstream accuracy, and training speed achieved by DoReMi.

**Significant Citations:**

* **Claim:** "In this section, we use DoReMi domain weights optimized with a 280M-parameter proxy model to train a 8B-parameter main model (30x larger)."
    * **Citation:** Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., ... & Leahy, C. (2020). The Pile: An 800GB dataset of diverse text for language modeling.* arXiv preprint arXiv:2001.02390*.
    * **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2021). GLaM: Efficient scaling of language models with mixture-of-experts.* arXiv preprint arXiv:2110.04497*.
    * **Relevance:** These citations introduce the datasets used in the experiments (The Pile and GLaM) and provide context for the scale of the models trained.


### 2.6 Experimental Setup

**Summary:** This section details the specific configurations used for the experiments, including the datasets, model architectures, training procedures, and evaluation metrics.

**Significant Citations:**

* **Claim:** "The Pile (Gao et al., 2020) is a 800GB text dataset with 22 domains."
    * **Citation:** Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., ... & Leahy, C. (2020). The Pile: An 800GB dataset of diverse text for language modeling.* arXiv preprint arXiv:2001.02390*.
    * **Relevance:** This citation introduces the primary dataset used in the experiments and provides essential information about its size and structure.


* **Claim:** "The GLaM dataset (Du et al., 2021) (also used in training PaLM (Chowdhery et al., 2022)) includes text from 8 domains."
    * **Citation:** Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2021). GLaM: Efficient scaling of language models with mixture-of-experts.* arXiv preprint arXiv:2110.04497*.
    * **Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Barham, P., ... & Dean, J. (2022). PaLM: Scaling language modeling with pathways.* arXiv preprint arXiv:2204.02311*.
    * **Relevance:** This citation introduces the second dataset used in the experiments and connects it to the PaLM model, which also utilized this dataset.


* **Claim:** "We train Transformer (Vaswani et al., 2017) decoder-only LMs with the standard next-token language modeling loss."
    * **Citation:** Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. *Advances in Neural Information Processing Systems*.
    * **Relevance:** This citation specifies the core model architecture used in the experiments, which is a standard Transformer model.


### 2.7 DoReMi Improves Perplexity and Downstream Accuracy

**Summary:** This section presents the main results of the experiments, demonstrating that DoReMi significantly improves both perplexity and downstream accuracy of large language models trained on The Pile and GLaM datasets.

**Significant Citations:**

* **Claim:** "DoReMi reduces perplexity on all domains over baseline domain weights, even when it downweights a domain."
    * **Citation:** Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., ... & Leahy, C. (2020). The Pile: An 800GB dataset of diverse text for language modeling.* arXiv preprint arXiv:2001.02390*.
    * **Relevance:** This claim highlights a key finding of the paper, that DoReMi can improve perplexity even when reducing the weight of certain domains, which is a counterintuitive but beneficial result.


* **Claim:** "DoReMi improves average downstream accuracy over a baseline model trained on The Pile's default domain weights by 6.5% points on generative few-shot tasks and achieves the baseline accuracy 2.6x faster."
    * **Citation:** Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Sutskever, I. (2020). Language models are few-shot learners. *arXiv preprint arXiv:2005.14165*.
    * **Relevance:** This claim presents a significant improvement in downstream performance achieved by DoReMi, demonstrating its effectiveness in improving the overall quality of the trained language model.


* **Claim:** "On the GLaM dataset where domain weights tuned on downstream tasks are available, DoReMi even performs comparably to tuning domain weights on downstream task performance."
    * **Citation:** Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2021). GLaM: Efficient scaling of language models with mixture-of-experts.* arXiv preprint arXiv:2110.04497*.
    * **Relevance:** This claim demonstrates that DoReMi, without any knowledge of downstream tasks, can achieve comparable performance to methods that explicitly optimize domain weights for specific downstream tasks.


### 2.8 Inspecting the DoReMi Domain Weights

**Summary:** This section analyzes the optimized domain weights produced by DoReMi on both The Pile and GLaM datasets, highlighting the patterns and trends observed in the weights.

**Significant Citations:** (The analysis is based on the results presented in Tables 1 and 2, which are not explicitly cited in this section.)


### 2.9 Ablations and Analysis Across Scales

**Summary:** This section investigates the impact of varying the proxy model size and ablating components of the excess loss objective on DoReMi's performance. It demonstrates that DoReMi consistently improves performance across different model scales and that the excess loss objective is crucial for achieving the observed gains.

**Significant Citations:**

* **Claim:** "DoReMi improves LMs consistently across scales."
    * **Citation:** (None explicitly cited in this section, but the results are based on the experiments described in Figure 5 and Appendix Table 6.)
    * **Relevance:** This claim highlights a key finding of the ablation study, that DoReMi's benefits are not limited to specific model sizes.


* **Claim:** "Proxy model underperforms main model, especially at larger sizes."
    * **Citation:** (None explicitly cited in this section, but the results are based on the analysis presented in Table 3b.)
    * **Relevance:** This observation suggests that the proxy model's quality is not directly correlated with the quality of the optimized domain weights, which is an important consideration for practical applications.


* **Claim:** "Effect of proxy model scale on larger main model's performance."
    * **Citation:** (None explicitly cited in this section, but the results are based on the experiments described in Figure 6.)
    * **Relevance:** This analysis explores the relationship between the proxy model size and the performance of the main model, providing insights into the optimal choice of proxy model size for different scenarios.


### 2.10 Simple Example Where Data Reweighting Has No Tradeoff

**Summary:** This section presents a simplified example to illustrate how DoReMi can improve perplexity on all domains without a tradeoff. It demonstrates that DoReMi can identify and downweight domains with extremely high or low entropy, leading to better overall model performance.

**Significant Citations:** (None explicitly cited in this section, but the example builds upon the core concepts of DoReMi and DRO.)


### 2.11 Related Work

**Summary:** This section provides a comprehensive overview of related work in the areas of data curation for LMs, general data selection methods, distributionally robust optimization, and data-centric AI. It highlights the novelty of DoReMi in addressing the challenge of optimizing domain weights for LM pretraining.

**Significant Citations:**

* **Claim:** "Most closely related is the GLaM dataset (Du et al., 2021) (also used for training PaLM (Chowdhery et al., 2022)), which has domain weights that are tuned using downstream data."
    * **Citation:** Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2021). GLaM: Efficient scaling of language models with mixture-of-experts.* arXiv preprint arXiv:2110.04497*.
    * **Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Barham, P., ... & Dean, J. (2022). PaLM: Scaling language modeling with pathways.* arXiv preprint arXiv:2204.02311*.
    * **Relevance:** This citation establishes the closest related work to DoReMi, highlighting the existing practice of tuning domain weights based on downstream tasks.


* **Claim:** "Optimizing domain weights for downstream tasks can be expensive and could require search/zero-order optimization (Snoek et al., 2012), RL (Zoph and Le, 2016), or heuristic assumptions on how positive/negative transfer between domains work."
    * **Citation:** Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. *Advances in Neural Information Processing Systems*.
    * **Zoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning.* arXiv preprint arXiv:1611.01578*.
    * **Relevance:** This citation highlights the challenges and limitations of existing methods for optimizing domain weights, emphasizing the need for a more efficient and principled approach like DoReMi.


* **Claim:** "Within DRO methods for deep learning (Ben-Tal et al., 2013, Oren et al., 2019, Sagawa et al., 2020, Sinha et al., 2018), we target a restricted form of shift called group shifts (Duchi et al., 2019, Oren et al., 2019, Sagawa et al., 2020), where the test distribution can be an unknown mixture of groups (domains)."
    * **Citation:** Ben-Tal, A., den Hertog, D., De Waegenaere, A., Melenberg, B., & Rennen, G. (2013). Robust solutions of optimization problems affected by uncertain probabilities. *Management Science*.
    * **Oren, Y., Sagawa, S., Hashimoto, T., & Liang, P. (2019). Distributionally robust language modeling.* Empirical Methods in Natural Language Processing*.
    * **Sagawa, S., Koh, P. W., Hashimoto, T., & Liang, P. (2020). Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.* International Conference on Learning Representations*.
    * **Sinha, A., Namkoong, H., & Duchi, J. (2018). Certifiable distributional robustness with principled adversarial training.* International Conference on Learning Representations*.
    * **Duchi, J., Hashimoto, T., & Namkoong, H. (2019). Distributionally robust losses against mixture covariate shifts.* arXiv preprint arXiv:1909.08077*.
    * **Relevance:** This citation provides a detailed overview of the DRO literature, highlighting the connection between DoReMi and the broader field of robust optimization.


### 2.12 Discussion and Limitations

**Summary:** This section discusses the potential benefits and limitations of DoReMi, including its ability to save compute through extrapolation, the impact of the choice of reference model, the concept of "domains" in the context of DoReMi, and the transferability of domain weights across model scales. It also explores the broader impacts of DoReMi on the efficiency and potential risks of large language models.

**Significant Citations:**

* **Claim:** "A future direction for saving compute would be to stop running DoReMi at an early step and extrapolate the domain weights for the desired number of steps, since we found that most of the variation in the domain weights during a DoReMi run seems to occur in the beginning of training."
    * **Citation:** (None explicitly cited in this section, but the claim is based on the analysis presented in Appendix Figure 8.)
    * **Relevance:** This suggestion for future work highlights a potential avenue for improving the computational efficiency of DoReMi.


* **Claim:** "The choice of reference model can affect the domain weights found by DoReMi."
    * **Citation:** (None explicitly cited in this section, but the claim is based on the results presented in Section 3.)
    * **Relevance:** This discussion highlights a limitation of DoReMi, emphasizing the importance of carefully selecting the reference model for optimal performance.


* **Claim:** "Large language models are We hope to improve training efficiency and reduce the environmental impact of training large LMs (Lacoste et al., 2019, Ligozat et al., 2021, Patterson et al., 2021, Strubell et al., 2019)."
    * **Citation:** Lacoste, A., Luccioni, A., Schmidt, V., & Dandres, T. (2019). Quantifying the carbon emissions of machine learning. *arXiv preprint arXiv:1910.09700*.
    * **Ligozat, A.-L., Lefèvre, J., Bugeau, A., & Combaz, J. (2021). Unraveling the hidden environmental impacts of AI solutions for environment.* arXiv preprint arXiv:2110.11822*.
    * **Patterson, D. A., Gonzalez, J., Le, Q. V., Liang, C., Munguia, L.-M., Rothchild, D., ... & Dean, J. (2021). Carbon emissions and large neural network training.* arXiv preprint arXiv:2104.10350*.
    * **Strubell, E., Ganesh, A., & McCallum, A. (2019). Energy and policy considerations for deep learning in NLP.* Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*.
    * **Relevance:** These citations highlight the broader societal impact of DoReMi, emphasizing its potential to reduce the environmental footprint of training large language models.


### 2.13 Conclusion

**Summary:** The conclusion summarizes the main contributions of the paper, emphasizing the significant speedup in training achieved by DoReMi and encouraging further research on data-centric approaches for improving language model training efficiency.

**Significant Citations:** (None explicitly cited in this section, but the conclusion summarizes the findings presented throughout the paper.)


## 3. Key Insights and Supporting Literature

**Key Insights:**

* **Optimizing domain weights can significantly improve LM performance:** DoReMi demonstrates that carefully selecting the mixture of data domains used for pretraining can lead to substantial improvements in perplexity and downstream accuracy. This is supported by the results presented in Sections 3.2 and 3.3, which show improvements across various metrics on both The Pile and GLaM datasets. The primary citations supporting this insight are:
    * Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., ... & Leahy, C. (2020). The Pile: An 800GB dataset of diverse text for language modeling.* arXiv preprint arXiv:2001.02390*.
    * Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2021). GLaM: Efficient scaling of language models with mixture-of-experts.* arXiv preprint arXiv:2110.04497*.
    * Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., ... & Sutskever, I. (2020). Language models are few-shot learners. *arXiv preprint arXiv:2005.14165*.


* **DoReMi can achieve comparable performance to downstream-tuned domain weights without explicit downstream task knowledge:** DoReMi demonstrates that it can achieve comparable performance to methods that explicitly tune domain weights based on downstream tasks, even without any knowledge of those tasks. This is supported by the results presented in Section 3.2 on the GLaM dataset. The primary citations supporting this insight are:
    * Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2021). GLaM: Efficient scaling of language models with mixture-of-experts.* arXiv preprint arXiv:2110.04497*.


* **DoReMi is computationally efficient:** DoReMi can achieve significant speedups in training large language models by optimizing domain weights using a much smaller proxy model. This is supported by the results presented in Sections 3.2 and 3.3, which show that DoReMi can achieve the same performance as baseline models with significantly fewer training steps. The primary citations supporting this insight are:
    * Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., ... & Leahy, C. (2020). The Pile: An 800GB dataset of diverse text for language modeling.* arXiv preprint arXiv:2001.02390*.
    * Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2021). GLaM: Efficient scaling of language models with mixture-of-experts.* arXiv preprint arXiv:2110.04497*.


* **DoReMi is robust across model scales:** The benefits of DoReMi are not limited to specific model sizes. This is supported by the ablation study presented in Section 4, which shows that DoReMi consistently improves performance across a range of model sizes. The primary citations supporting this insight are:
    * (None explicitly cited in this section, but the results are based on the experiments described in Figure 5 and Appendix Table 6.)


## 4. Experimental Methodology and Its Foundations

**Experimental Setup:**

The paper's experiments involve training language models on two datasets: The Pile and GLaM. The core methodology is DoReMi, which consists of three main steps:

1. **Training a small reference model:** A small language model is trained using initial domain weights (e.g., uniform or based on token counts).
2. **Training a proxy model with Group DRO:** A small proxy model is trained using Group DRO to optimize domain weights. The objective is to minimize the worst-case excess loss across domains.
3. **Training a large model with optimized domain weights:** A larger language model is trained using the optimized domain weights obtained from the proxy model.

**Foundations in Cited Works:**

* **Distributionally Robust Optimization (DRO):** The core of DoReMi's methodology is based on DRO, particularly the Group DRO optimizer. The authors cite works like Nemirovski et al. (2009) and Sagawa et al. (2020) to establish the theoretical foundation for DRO and its application in deep learning.
* **DRO-LM:** The authors adapt the DRO-LM framework (Oren et al., 2019) to optimize domain weights instead of training a robust model. This adaptation is a novel aspect of the methodology.
* **Transformer Models:** The authors use Transformer decoder-only models (Vaswani et al., 2017) as the base architecture for their language models.


**Novel Aspects of Methodology:**

* **Adapting DRO for Domain Weight Optimization:** The core novelty of DoReMi lies in adapting the DRO framework to optimize domain weights rather than training a robust model. This approach allows for efficient optimization of domain weights without requiring extensive training of numerous models with different weight configurations.
* **Using a Proxy Model for Domain Weight Optimization:** The use of a smaller proxy model to optimize domain weights is a computationally efficient approach compared to directly optimizing weights for the large target model.


## 5. Results in Context

**Main Results:**

* **Improved Perplexity:** DoReMi consistently reduces perplexity across all domains on The Pile, even when downweighting some domains.
* **Improved Downstream Accuracy:** DoReMi improves average downstream accuracy on The Pile by 6.5% and achieves the baseline accuracy 2.6x faster.
* **Comparable Performance on GLaM:** DoReMi achieves comparable performance to downstream-tuned domain weights on the GLaM dataset, demonstrating its ability to generalize across different datasets.
* **Robustness Across Scales:** DoReMi consistently improves performance across different model scales, suggesting its applicability to a wide range of model sizes.


**Comparison with Existing Literature:**

* **GLaM Dataset:** DoReMi's results on the GLaM dataset are compared to the performance of models trained with domain weights tuned on downstream tasks (Du et al., 2021). DoReMi achieves comparable performance, demonstrating its ability to achieve good results without explicit downstream task knowledge.
* **The Pile Dataset:** DoReMi's results on The Pile are compared to a baseline model trained with the default domain weights (Gao et al., 2020). DoReMi significantly improves both perplexity and downstream accuracy, showcasing its effectiveness in optimizing domain weights.
* **DRO-LM:** DoReMi builds upon the DRO-LM framework (Oren et al., 2019) but adapts it for domain weight optimization. The authors highlight the differences between their approach and DRO-LM, emphasizing the novelty of their method.


**Confirmation, Contradiction, or Extension of Cited Works:**

* **Confirmation:** DoReMi's results confirm the general observation that data composition significantly impacts LM performance (Du et al., 2021, Hoffmann et al., 2022).
* **Extension:** DoReMi extends the DRO-LM framework (Oren et al., 2019) by applying it to domain weight optimization, demonstrating a novel application of DRO in the context of LM pretraining.
* **Contradiction (in a way):** DoReMi's findings that downweighting certain domains can improve overall performance might seem to contradict some intuitive notions about data selection, but it highlights the complex interplay between data diversity and model performance.


## 6. Discussion and Related Work

**Situating the Work within Existing Literature:**

The authors situate their work within the broader context of data-centric AI and specifically within the research on data curation for LMs. They highlight the limitations of existing methods for optimizing domain weights, such as reliance on downstream tasks or heuristic assumptions. DoReMi is presented as a principled and computationally efficient approach that addresses these limitations.

**Key Papers Cited in Discussion/Related Work:**

* **GLaM (Du et al., 2021):** This work is presented as the most closely related to DoReMi, highlighting the existing practice of tuning domain weights based on downstream tasks.
* **PaLM (Chowdhery et al., 2022):** This work is mentioned in the context of the GLaM dataset, emphasizing the importance of data composition for large language models.
* **DRO-LM (Oren et al., 2019):** This work provides the foundation for DoReMi's optimization approach, but DoReMi adapts it for domain weight optimization.
* **Group DRO (Nemirovski et al., 2009, Sagawa et al., 2020):** These works provide the theoretical foundation for the optimization method used in DoReMi.
* **Data-Centric AI (Deng et al., 2009, Gadre et al., 2023):** The authors position DoReMi within the broader context of data-centric AI, emphasizing its potential to improve the efficiency of language model training.


**Highlighting Novelty/Importance:**

The authors use these citations to emphasize the following aspects of DoReMi:

* **Novelty:** DoReMi's approach of using DRO to optimize domain weights without explicit downstream task knowledge is novel.
* **Efficiency:** DoReMi is computationally efficient compared to existing methods for optimizing domain weights.
* **Generalizability:** DoReMi demonstrates the ability to generalize across different datasets and model scales.
* **Principled Approach:** DoReMi is presented as a principled approach to data selection for LM pretraining, in contrast to methods that rely on intuition or heuristics.


## 7. Future Work and Open Questions

**Areas for Further Research:**

* **Extrapolating Domain Weights:** The authors suggest exploring the possibility of stopping DoReMi early and extrapolating the domain weights to save compute.
* **Choice of Reference Model:** They propose investigating the impact of different reference model architectures and sizes on the optimized domain weights.
* **Fine-Grained Domains:** They suggest exploring the use of finer-grained domains to potentially improve the gains from DoReMi.
* **Transferability Across Scales:** They propose further research into understanding the limits of transferability of domain weights across different model scales.
* **Group DRO Optimizer:** They suggest exploring alternative Group