## Analysis of "Fine-Tuning Language Models with Just Forward Passes"

**1. Introduction:**

- **Title:** Fine-Tuning Language Models with Just Forward Passes
- **Authors:** Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, Sanjeev Arora
- **Publication Date:** 11 January 2024 (v3)
- **Objective:** The paper proposes a memory-efficient zeroth-order optimizer (MeZO) for fine-tuning large language models (LLMs) using only forward passes, thereby significantly reducing memory consumption compared to backpropagation-based methods.
- **References:** The paper cites 113 references.

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:**
    - Fine-tuning LLMs is crucial for various downstream tasks, but backpropagation becomes memory-intensive as models scale up.
    - Zeroth-order (ZO) methods, which estimate gradients using only forward passes, are theoretically slow for large models.
    - The paper proposes MeZO, a memory-efficient ZO optimizer that operates in-place, enabling fine-tuning with the same memory footprint as inference.
- **Significant Citations:**
    - **[28] Devlin et al., 2019, BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186.** This citation highlights the importance of fine-tuning LLMs for various tasks.
    - **[42] Gururangan et al., 2020, Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360.** This citation emphasizes the need for adapting LLMs to specialized domains.
    - **[73] Ouyang et al., 2022, Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.** This citation underscores the importance of incorporating human instructions and preferences into LLMs.
    - **[13, 72] Brown et al., 2020, Language models are few-shot learners. In Advances in neural information processing systems, volume 33, pages 1877–1901; OpenAI, 2023, Gpt-4 technical report. arXiv preprint arXiv:2303.08774.** These citations highlight the scaling trend of LLMs, emphasizing the memory challenges associated with larger models.
    - **[52] Kingma and Ba, 2015, Adam: A method for stochastic optimization. In International Conference on Learning Representations.** This citation mentions Adam, a popular optimizer used for fine-tuning, which contributes to the memory overhead.
    - **[46, 57, 54] Hu et al., 2022, LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations; Li and Liang, 2021, Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597; Lester et al., 2021, The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059.** These citations introduce parameter-efficient fine-tuning (PEFT) methods, which are compared to MeZO in terms of memory efficiency.

**2.2 Zeroth-Order Optimization:**

- **Key Points:**
    - The paper introduces the classical ZO gradient estimator, SPSA (Simultaneous Perturbation Stochastic Approximation), and its corresponding SGD algorithm, ZO-SGD.
    - MeZO is presented as an in-place implementation of ZO-SGD, requiring the same memory as inference.
- **Significant Citations:**
    - **[88] Spall, 1992, Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE Transactions on Automatic Control, 37(3):332-341.** This citation introduces the SPSA gradient estimator, a key component of MeZO.
    - **[69, 32] Agarwal et al., 2012, Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235–3249; Duchi et al., 2015, Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788–2806.** These citations discuss classical lower bounds for ZO methods, which are contrasted with the paper's findings.
    - **[91, 90] Sun et al., 2022, Black-box tuning for language-model-as-a-service. In International Conference on Machine Learning, pages 20841-20855; Sun et al., 2022, BBTv2: Towards a gradient-free future with large language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3916–3930.** These citations mention previous work on gradient-free optimization of LLMs, which are compared to MeZO.
    - **[61] Liu et al., 2020, Understanding the difficulty of training transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5747–5763.** This citation discusses the challenges of training Transformers, providing context for the paper's focus on memory efficiency.

**2.3 Memory-Efficient ZO-SGD (MeZO):**

- **Key Points:**
    - MeZO is presented as an in-place implementation of ZO-SGD, requiring the same memory as inference.
    - The algorithm is described in detail, highlighting its memory efficiency compared to standard ZO-SGD.
    - The paper discusses the storage efficiency of MeZO compared to PEFT methods.
- **Significant Citations:**
    - **[88] Spall, 1992, Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE Transactions on Automatic Control, 37(3):332-341.** This citation is referenced again to emphasize the use of SPSA in MeZO.
    - **[69, 32] Agarwal et al., 2012, Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235–3249; Duchi et al., 2015, Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788–2806.** These citations are referenced again to contrast the paper's findings with classical lower bounds.
    - **[46] Hu et al., 2022, LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.** This citation is referenced again to compare MeZO's storage efficiency to PEFT methods.
    - **[57] Li and Liang, 2021, Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597.** This citation is referenced again to compare MeZO's storage efficiency to PEFT methods.

**2.4 MeZO Extensions:**

- **Key Points:**
    - The paper discusses the use of other one-point gradient estimators in place of SPSA, but highlights that SPSA is more efficient in practice.
    - MeZO is shown to be compatible with other gradient-based optimizers, such as SGD with momentum and Adam.
    - The paper explores layerwise adaptive optimizers as potential extensions to MeZO.
    - The paper discusses the use of forward auto-differentiation for computing the gradient estimate, but notes its memory limitations.
- **Significant Citations:**
    - **[34, 87, 95] Flaxman et al., 2005, Online convex optimization in the bandit setting: Gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA '05, page 385–394, USA, 2005. Society for Industrial and Applied Mathematics. ISBN 0898715857; Bollapragada et al., 2018, Adaptive sampling strategies for stochastic optimization. SIAM Journal on Optimization, 28(4):331-3343 2018; Spall, 1997, A one-measurement form of simultaneous perturbation stochastic approximation. Automatica, 33(1):109-112, 1997.** These citations introduce various one-point gradient estimators.
    - **[113] Zhang et al., 2022, A new one-point residual-feedback oracle for black-box learning and control. Automatica, 136:110006, 2022.** This citation specifically mentions a promising one-point estimator.
    - **[40] Griewank and Walther, 2008, Evaluating derivatives: principles and techniques of algorithmic differentiation. SIAM.** This citation discusses forward auto-differentiation, a potential approach for computing the gradient estimate.
    - **[9] Baydin et al., 2022, Gradients without backpropagation.** This citation mentions previous work on JVP-based training, providing context for the paper's discussion.

**2.5 Experiments:**

- **Key Points:**
    - The paper conducts comprehensive experiments across various model types, scales, and downstream tasks.
    - MeZO consistently outperforms zero-shot, in-context learning (ICL), and linear probing.
    - MeZO achieves comparable or better performance than fine-tuning with Adam (FT) on several tasks, while using significantly less memory.
    - MeZO is shown to be compatible with full-parameter tuning and PEFT methods, such as LoRA and prefix tuning.
    - MeZO can effectively optimize non-differentiable objectives, such as accuracy or F1 score.
- **Significant Citations:**
    - **[13, 84, 35] Brown et al., 2020, Language models are few-shot learners. In Advances in neural information processing systems, volume 33, pages 1877–1901; Schick and Schütze, 2021, Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255–269; Gao et al., 2021, Making pre-trained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816–3830.** These citations are referenced to highlight the importance of prompts for MeZO's success.
    - **[83, 67] Saunshi et al., 2021, A mathematical exploration of why language models help solve downstream tasks. In International Conference on Learning Representations; Malladi et al., 2022, A kernel-based view of language model fine-tuning. arXiv preprint arXiv:2210.05643.** These citations explain the importance of prompt design for ensuring the fine-tuning objective is closely related to the pre-training objective.
    - **[46] Hu et al., 2022, LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.** This citation is referenced again to demonstrate MeZO's compatibility with LoRA.
    - **[57] Li and Liang, 2021, Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597.** This citation is referenced again to demonstrate MeZO's compatibility with prefix tuning.
    - **[90] Sun et al., 2022, BBTv2: Towards a gradient-free future with large language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3916–3930.** This citation is referenced to compare MeZO's performance to BBTv2.
    - **[89, 73] Stiennon et al., 2020, Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021; Ouyang et al., 2022, Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.** These citations highlight the growing importance of non-differentiable objectives in fine-tuning LLMs.

**2.6 Memory Usage and Wall-Clock Time Analysis:**

- **Key Points:**
    - The paper provides a detailed analysis of the memory consumption of various methods, including zero-shot, ICL, FT, FT (prefix), and MeZO.
    - MeZO is shown to be significantly more memory-efficient than FT and FT (prefix), enabling the training of larger models within a fixed hardware budget.
    - The paper compares the wall-clock time efficiency of MeZO and Adam fine-tuning, highlighting MeZO's per-step speedup but noting its higher step count.
- **Significant Citations:**
    - **[18] Chen et al., 2016, Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174.** This citation mentions gradient checkpointing, a technique for reducing memory consumption in backpropagation.
    - **[23] Dao et al., 2022, Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, volume 35, pages 16344–16359.** This citation discusses FlashAttention, another technique for reducing memory consumption in backpropagation.
    - **[26, 27] Dettmers et al., 2022, GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Advances in Neural Information Processing Systems; Dettmers et al., 2022, 8-bit optimizers via block-wise quantization. In International Conference on Learning Representations.** These citations mention quantization, a technique for reducing memory consumption in backpropagation.
    - **[40] Griewank and Walther, 2008, Evaluating derivatives: principles and techniques of algorithmic differentiation. SIAM.** This citation is referenced again to provide a theoretical framework for understanding the memory-time tradeoff in backpropagation.

**2.7 Theory:**

- **Key Points:**
    - The paper provides a theoretical analysis of why MeZO can effectively optimize large LLMs, despite classical results suggesting otherwise.
    - The paper highlights the importance of prompts in ensuring the fine-tuning objective is closely related to the pre-training objective.
    - The paper derives a convergence rate for MeZO that is independent of the number of parameters, depending instead on the local effective rank of the Hessian.
    - The paper shows that under stronger assumptions about the loss landscape, the global convergence rate of MeZO also slows by a factor proportional to the local effective rank.
- **Significant Citations:**
    - **[69, 47, 79, 3, 70] Agarwal et al., 2012, Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235–3249; Jamieson et al., 2012, Query complexity of derivative-free optimization. In Advances in Neural Information Processing Systems, volume 25; Raginsky and Rakhlin, 2011, Information-based complexity, feedback and dynamics in convex programming. IEEE Transactions on Information Theory, 57(10):7036-7056; Agarwal et al., 2012, Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235–3249; Nesterov and Spokoiny, 2017, Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17:527–566.** These citations discuss classical lower bounds for ZO methods, which are contrasted with the paper's findings.
    - **[83, 67] Saunshi et al., 2021, A mathematical exploration of why language models help solve downstream tasks. In International Conference on Learning Representations; Malladi et al., 2022, A kernel-based view of language model fine-tuning. arXiv preprint arXiv:2210.05643.** These citations are referenced again to explain the importance of prompt design for ensuring the fine-tuning objective is closely related to the pre-training objective.
    - **[74, 75, 36, 107, 105, 82] Papyan, 2018, The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size. arXiv preprint arXiv:1811.07062; Papyan, 2020, Traces of class/cross-class structure pervade deep learning spectra. Journal of Machine Learning Research, 21(252):1–64; Ghorbani et al., 2019, An investigation into neural net optimization via hessian eigenvalue density. In International Conference on Machine Learning, pages 2232-2241; Yao et al., 2020, Pyhessian: Neural networks through the lens of the hessian. In 2020 IEEE international conference on big data (Big data), pages 581-590; Sagun et al., 2017, Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454.** These citations discuss the low effective rank of the Hessian in deep neural networks, providing support for the paper's assumptions.
    - **[4, 56] Aghajanyan et al., 2021, Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7319–7328; Li et al., 2018, Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations.** These citations provide further support for the assumption of low effective rank in LLMs.
    - **[50] Karimi et al., 2020, Linear convergence of gradient and proximal-gradient methods under the polyak-łojasiewicz condition.** This citation introduces the Polyak-Łojasiewicz condition, which is used in the paper's analysis.

**2.8 Related Work:**

- **Key Points:**
    - The paper discusses related work on zeroth-order optimization, memory-efficient backpropagation, and gradient-free adaptation of LLMs.
    - The paper highlights the limitations of classical ZO methods and the recent progress in understanding their complexity in the context of low-dimensional structure.
    - The paper contrasts MeZO with existing memory-efficient backpropagation techniques and gradient-free LLM adaptation methods.
- **Significant Citations:**
    - **[47, 3, 79, 32, 85, 69] Jamieson et al., 2012, Query complexity of derivative-free optimization. In Advances in Neural Information Processing Systems, volume 25; Agarwal et al., 2012, Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235–3249; Raginsky and Rakhlin, 2011, Information-based complexity, feedback and dynamics in convex programming. IEEE Transactions on Information Theory, 57(10):7036-7056; Duchi et al., 2015, Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788–2806; Shamir, 2017, An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. The Journal of Machine Learning Research, 18(1):1703–1713; Agarwal et al., 2012, Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235–3249.** These citations discuss classical lower bounds for ZO methods.
    - **[100, 6, 15] Wang et al., 2018, Stochastic zeroth-order optimization in high dimensions. In Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84, pages 1356–1365; Balasubramanian and Ghadimi, 2018, Zeroth-order (non)-convex stochastic optimization via conditional gradient and gradient updates. In Advances in Neural Information Processing Systems, volume 31; Cai et al., 2022, Zeroth-order regularized optimization (zoro): Approximately sparse gradients and adaptive sampling. SIAM Journal on Optimization, 32(2):687–714.** These citations discuss recent work on ZO methods that consider low-dimensional structure.
    - **[92, 102, 1, 19]  Wang et al., 2017, Minimal effort back propagation for convolutional neural networks. arXiv preprint arXiv:1709.05804;  Abdel-Khalik et al., 2008, A low rank approach to automatic differentiation. In Advances in Automatic Differentiation, pages 55-65; Adelman et al., 2021, Faster neural network training with approximate tensor operations. Advances in Neural Information Processing Systems, 34:27877–27889; Choromanski and Sindhwani, 2017, On blackbox backpropagation and jacobian sensing. In Advances in Neural Information Processing Systems, volume 30.** These citations discuss various memory-efficient backpropagation techniques.
    - **[18] Chen et al., 2016, Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174.** This citation is referenced again to discuss gradient checkpointing.
    - **[23] Dao et al., 2022, Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neural Information Processing Systems, volume 35, pages 16344–16359.** This citation is referenced again to discuss FlashAttention.
    - **[26, 27] Dettmers et al., 2022, GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Advances in Neural Information Processing Systems; Dettmers et al., 2022, 8-bit optimizers via block-wise quantization. In International Conference on Learning Representations.** These citations are referenced again to discuss quantization.
    - **[91, 90] Sun et al., 2022, Black-box tuning for language-model-as-a-service. In International Conference on Machine Learning, pages 20841-20855; Sun et al., 2022, BBTv2: Towards a gradient-free future with large language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3916–3930.** These citations discuss previous work on gradient-free optimization of LLMs.
    - **[16, 25, 29] Chai et al., 2022, Clip-tuning: Towards derivative-free prompt learning with a mixture of rewards. arXiv preprint arXiv:2210.12050; Deng et al., 2022, RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3369-3391; Diao et al., 2022, Black-box prompt learning for pre-trained language models. arXiv preprint arXiv:2201.08531.** These citations discuss "black-box tuning" methods for LLMs.
    - **[45, 78, 106] Hou et al., 2022, Promptboosting: Black-box text classification with ten forward passes. arXiv preprint arXiv:2212.09257; Prasad et al., 2022, Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281; Yang et al., 2023, Iterative forward tuning boosts in-context learning in language models.** These citations discuss recent work on improving in-context learning performance.

**2.9 Conclusion:**

- **Key Points:**
    - The paper concludes that MeZO can effectively optimize large LLMs across various tasks and scales.
    - The paper highlights the potential of MeZO for optimizing non-differentiable objectives.
    - The paper acknowledges the limitations of MeZO, such as its high step count, and suggests future research directions.
- **Significant Citations:**
    - **[73] Ouyang et al., 2022, Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.** This citation is referenced again to highlight the importance of non-differentiable objectives in fine-tuning LLMs.

**3. Key Insights and Supporting Literature:**

- **Key Insight 1:** MeZO can effectively fine-tune large LLMs using only forward passes, significantly reducing memory consumption compared to backpropagation-based methods.
    - **Supporting Citations:**
        - **[69, 47, 79, 3, 70] Agarwal et al., 2012, Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235–3249; Jamieson et al., 2012, Query complexity of derivative-free optimization. In Advances in Neural Information Processing Systems, volume 25; Raginsky and Rakhlin, 2011, Information-based complexity, feedback and dynamics in convex programming. IEEE Transactions on Information Theory, 57(10):7036-7056; Agarwal et al., 2012, Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235–3249; Nesterov and Spokoiny, 2017, Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17:527–566.** These citations provide context for the paper's findings by highlighting classical lower bounds for ZO methods, which are contrasted with the paper's results.
        - **[88] Spall, 1992, Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE Transactions on Automatic Control, 37(3):332-341.** This citation introduces the SPSA gradient estimator, a key component of MeZO.
        - **[46, 57, 54] Hu et al., 2022, LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations; Li and Liang, 2021, Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597; Lester et al., 2021, The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059.** These citations introduce PEFT methods, which are compared to MeZO in terms of memory efficiency.
- **Key Insight 2:** MeZO's convergence rate is independent of the number of parameters, depending instead on the local effective rank of the Hessian.
    - **Supporting Citations:**
        - **[74, 75, 36, 107, 105, 82] Papyan, 2018, The full spectrum of deepnet hessians at scale: Dynamics with sgd training and sample size. arXiv preprint arXiv:1811.07062; Papyan, 2020, Traces of class/cross-class structure pervade deep learning spectra. Journal of Machine Learning Research, 21(252):1–64; Ghorbani et al., 2019, An investigation into neural net optimization via hessian eigenvalue density. In International Conference on Machine Learning, pages 2232-2241; Yao et al., 2020, Pyhessian: Neural networks through the lens of the hessian. In 2020 IEEE international conference on big data (Big data), pages 581-590; Sagun et al., 2017, Empirical analysis of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454.** These citations discuss the low effective rank of the Hessian in deep neural networks, providing support for the paper's assumptions.
        - **[4, 56] Aghajanyan et al., 2021, Intrinsic dimensionality explains the effectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7319–7328; Li et al., 2018, Measuring the intrinsic dimension of objective landscapes. In International Conference on Learning Representations.** These citations provide further support for the assumption of low effective rank in LLMs.
- **Key Insight 3:** MeZO can effectively optimize non-differentiable objectives, such as accuracy or F1 score.
    - **Supporting Citations:**
        - **[89, 73] Stiennon et al., 2020, Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021; Ouyang et al., 2022, Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.** These citations highlight the growing importance of non-differentiable objectives in fine-tuning LLMs.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:**
    - The paper conducts experiments on various model types (masked LMs and autoregressive LMs), model scales (from 350M to 66B), and downstream tasks (classification, multiple-choice, and generation).
    - The paper compares MeZO with zero-shot, ICL, linear probing, and FT with Adam.
    - The paper explores the use of prompts, full-parameter tuning, and PEFT methods (LoRA and prefix tuning).
    - The paper investigates the optimization of non-differentiable objectives (accuracy and F1 score).
    - The paper analyzes the memory consumption and wall-clock time efficiency of different methods.
- **Foundations:**
    - The paper builds upon the existing literature on ZO optimization, memory-efficient backpropagation, and gradient-free adaptation of LLMs.
    - The paper cites several works to justify its experimental design and choices, including the use of prompts, PEFT methods, and non-differentiable objectives.
- **Novel Aspects:**
    - The paper's primary contribution is the development of MeZO, a novel memory-efficient ZO optimizer.
    - The paper's theoretical analysis provides insights into why MeZO can effectively optimize large LLMs, despite classical results suggesting otherwise.
    - The paper's experimental results demonstrate the effectiveness of MeZO across various model types, scales, and tasks, highlighting its potential for practical applications.

**5. Results in Context:**

- **Main Results:**
    - MeZO consistently outperforms zero-shot, ICL, and linear probing across various model types, scales, and tasks.
    - MeZO achieves comparable or better performance than FT with Adam on several tasks, while using significantly less memory.
    - MeZO is compatible with full-parameter tuning and PEFT methods, such as LoRA and prefix tuning.
    - MeZO can effectively optimize non-differentiable objectives, such as accuracy or F1 score.
- **Comparison with Existing Literature:**
    - MeZO's performance surpasses previous gradient-free methods, such as BBTv2 [90], on mutually assessed tasks.
    - MeZO's memory efficiency significantly outperforms FT and FT (prefix), enabling the training of larger models within a fixed hardware budget.
    - MeZO's theoretical analysis provides insights into why it can effectively optimize large LLMs, despite classical results suggesting otherwise.
- **Confirmation, Contradiction, or Extension:**
    - MeZO's results confirm the importance of prompts for successful ZO optimization, as suggested by previous work [83, 67].
    - MeZO's results contradict classical lower bounds for ZO methods [69, 47, 79, 3, 70], demonstrating that ZO methods can be effective for optimizing large LLMs.
    - MeZO's results extend the existing literature on gradient-free optimization of LLMs by demonstrating its effectiveness across various model types, scales, and tasks, including non-differentiable objectives.

**6. Discussion and Related Work:**

- **Situating the Work:**
    - The authors situate their work within the broader context of ZO optimization, memory-efficient backpropagation, and gradient-free adaptation of LLMs.
    - The authors highlight the limitations of classical ZO methods and the recent progress in understanding their complexity in the context of low-dimensional structure.
    - The authors contrast MeZO with existing memory-efficient backpropagation techniques and gradient-free LLM adaptation methods.
- **Key Papers Cited:**
    - **[47, 3, 79, 32, 85, 69] Jamieson et al., 2012, Query complexity of derivative-free optimization. In Advances in Neural Information Processing Systems, volume 25; Agarwal et al., 2012, Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235–3249; Raginsky and Rakhlin, 2011, Information-based complexity, feedback and dynamics in convex programming. IEEE Transactions on Information Theory, 57(10):7036-7056; Duchi et al., 2015, Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):2788–2806; Shamir, 2017, An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. The Journal of Machine Learning Research, 18(1):1703–1713; Agarwal et al., 2012, Information-theoretic lower bounds