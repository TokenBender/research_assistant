## SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression - Citation Analysis

**1. Introduction**

- **Title:** SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression
- **Authors:** Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan Alistarh
- **Publication Date:** June 5, 2023
- **Objective:** To introduce a new compressed format and quantization technique called Sparse-Quantized Representation (SpQR) that enables near-lossless compression of LLMs across model scales while achieving similar compression levels to previous methods.
- **Total References:** 34

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - Recent advances in LLM pretraining have led to high-quality LLMs with impressive abilities.
    - Quantization to 3-4 bits per parameter allows LLMs to fit into memory-limited devices, but often leads to accuracy losses, especially for smaller models.
    - SpQR addresses this accuracy issue by identifying and isolating outlier weights and storing them in higher precision while compressing other weights to 3-4 bits.
    - SpQR achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLMs like LLaMA and Falcon.
    - SpQR enables running 33B parameter LLMs on a single 24 GB consumer GPU without performance degradation.
    - SpQR provides efficient algorithms for encoding and decoding weights, including a GPU inference algorithm that is faster than 16-bit baselines at similar accuracy.
- **Citations:**
    - **Claim:** Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities.
        - **Citation:** [BMR+20, WBZ+21, Ope23]
        - **Explanation:** These citations highlight the recent progress in LLM pretraining and their improved performance on various tasks.
    - **Claim:** By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use.
        - **Citation:** [KMH+20, CND+22]
        - **Explanation:** These citations emphasize the importance of scaling LLMs for deployment on resource-constrained devices.
    - **Claim:** However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments.
        - **Citation:** [HBM+22, BSA+23, TLI+23]
        - **Explanation:** These citations highlight the challenges of quantizing smaller LLMs for edge deployments due to accuracy degradation.

**2.2 Related Work**

- **Key Points:**
    - The paper focuses on post-training quantization (PTQ) methods for compressing LLMs.
    - Existing PTQ methods often struggle to scale to large LLMs due to computational constraints.
    - Recent work has focused on isolating "outlier features" to improve quantization accuracy.
    - GPTQ [FAHA22] is a popular PTQ method that achieves high accuracy but can be computationally expensive.
    - SparseGPT [FA23] jointly sparsifies and quantizes LLM weights, but still results in significant accuracy loss for smaller models.
    - The paper aims to address the accuracy loss issue for smaller models by introducing a new compression format that enables near-lossless quantization.
- **Citations:**
    - **Claim:** PTQ methods are a popular approach for one-shot compression of models with various sizes, based on a limited amount of calibration data, using accurate solvers, usually focused on layer- or group-wise compression sub-problems.
        - **Citation:** [NAVB+20, GKD+21]
        - **Explanation:** These citations provide background on PTQ methods and their common applications.
    - **Claim:** Recently, there has been significant interest in obtaining accurate post-training methods that scale to such massive models.
        - **Citation:** [YAZ+22, DLBZ22, PPK+22]
        - **Explanation:** These citations highlight the growing interest in developing scalable PTQ methods for large LLMs.
    - **Claim:** Dettmers et al. [DZ22] provided an in-depth overview of the accuracy-compression trade-offs underlying these methods, establishing that 4-bit quantization is an optimal point for round-to-nearest-based methods, whereas higher compression can be achieved via data-aware methods such as GPTQ.
        - **Citation:** [DZ22]
        - **Explanation:** This citation highlights the importance of understanding the trade-offs between accuracy and compression in quantization.
    - **Claim:** SparseGPT [FA23] presented an approach to jointly sparsify LLM weights to medium sparsities, together with quantization of the remaining weights to a fixed given bit-width.
        - **Citation:** [FA23]
        - **Explanation:** This citation introduces SparseGPT, a related work that combines sparsification and quantization.
    - **Claim:** One common drawback of existing methods is that the accuracy loss relative to the original model is still significant.
        - **Citation:** [DLBZ22, XLS+22, YAZ+22]
        - **Explanation:** This citation highlights the limitations of existing methods in achieving near-lossless compression.

**2.3 Quantization Sensitivity of LLM Weights**

- **Key Points:**
    - Not all parameters in a neural network are equally important for quantization.
    - The paper defines a sensitivity measure for each weight based on the minimum squared difference between the original predictions and those of a quantized weight matrix.
    - The sensitivity measure captures the correlation between weights and can be approximated efficiently by quantization solvers like GPTQ.
    - The paper analyzes the sensitivity of weights in LLaMA-65B and observes several patterns, including row outliers, column outliers, sensitive attention heads, rotary embedding patterns, and unstructured outliers.
- **Citations:**
    - **Claim:** Not all parameters in a neural network are equally important.
        - **Citation:** [FAHA22, YAZ+22]
        - **Explanation:** These citations highlight the importance of considering weight sensitivity in quantization.
    - **Claim:** For computational tractability, we assess sensitivity on a per-layer level using a small set of calibration inputs X, collected by running them through the model up to the particular layer.
        - **Citation:** [FSA22]
        - **Explanation:** This citation introduces the Optimal Brain Surgeon framework, which is used to compute the sensitivity measure.
    - **Claim:** This saliency measure can be approximated efficiently by quantization solvers, such as GPTQ [FAHA22].
        - **Citation:** [FAHA22]
        - **Explanation:** This citation highlights the efficiency of GPTQ in approximating the sensitivity measure.
    - **Claim:** The latter are correlated to the "outlier feature" phenomenon reported in Dettmers et al. [DLBZ22].
        - **Citation:** [DLBZ22]
        - **Explanation:** This citation connects the observed column outliers to the concept of outlier features.

**2.4 SpQR: A Sensitivity-aware Compressed Representation**

- **Key Points:**
    - SpQR is a new compressed representation that addresses the limitations of existing quantization methods by treating low- and high-sensitivity weights differently.
    - SpQR uses bilevel quantization to capture small groups of sensitive weights and stores individual outliers in higher precision.
    - SpQR uses groupwise quantization with small groups (B1 = 8-32 weights) to reduce the number of cases where sensitive weights are grouped together.
    - SpQR quantizes the groupwise statistics using the same quantization algorithm as for weights to ensure efficient storage.
    - SpQR identifies outliers based on a sensitivity threshold and stores them in a row-wise arrangement similar to CSR representation.
    - SpQR provides efficient algorithms for encoding and decoding weights, including a GPU inference algorithm that leverages the sparse structure of outliers.
- **Citations:**
    - **Claim:** Existing LLM quantization algorithms treat low- and high-sensitivity weights equally; however, our above discussion suggests that this may lead to sub-optimal quantization.
        - **Citation:** [YLW+23]
        - **Explanation:** This citation highlights the limitations of existing methods in treating all weights equally.
    - **Claim:** To circumvent this issue, we quantize the groupwise statistics themselves using the same quantization algorithm as for weights - asymmetric (min-max) quantization.
        - **Citation:** [FSA22]
        - **Explanation:** This citation introduces the use of asymmetric quantization for groupwise statistics.
    - **Claim:** The procedure for detecting the outliers is described in detail in Alg. 1.
        - **Citation:** [DZ22]
        - **Explanation:** This citation references the work of Dettmers et al. for outlier detection.
    - **Claim:** Specifically, a particular weight is considered an outlier if keeping the weight in 16-bit reduces the error in Eq. (2) by at least т.
        - **Citation:** [HABN+21]
        - **Explanation:** This citation introduces the use of CSR representation for storing outliers.

**2.5 Implementing and Leveraging the Sparse Quantized Representation**

- **Key Points:**
    - SpQR representation consists of quantized weights, first-level quantized quantization statistics, second-level quantized quantization statistics, and CSR outlier indices and values.
    - Non-outlier weights are encoded as a bw-bit individual weight, a bq-bit scale and zero point for each group of size B, and 16-bit statistics for quantizing groups of Bq quantization scales and zero-points.
    - Outliers are stored in a row-wise arrangement, with each outlier represented by a 16-bit weight value and a 16-bit column index.
    - SpQR provides an efficient GPU inference algorithm that leverages the sparse structure of outliers and performs load balancing to improve performance.
- **Citations:**
    - **Claim:** We keep the quantized values for outlier weights in place and adjust the 16-bit versions to compensate for that.
        - **Citation:** [Neu22, GFS+19]
        - **Explanation:** These citations highlight the use of hybrid sparse-quantized formats in efficient CPU inference engines.
    - **Claim:** This results in an average storage cost of 32.03 to 32.1 bits per sensitive weight.
        - **Citation:** [KHB+21]
        - **Explanation:** This citation introduces the FBGEMM package, which proposes a format for storing outlier weights separately.

**2.6 Experimental Validation**

- **Key Points:**
    - The paper evaluates SpQR on LLaMA and Falcon models using WikiText2, C4, Penn Treebank, and five zero-shot tasks.
    - SpQR achieves near-lossless compression with less than 4.75 bits per parameter on average.
    - SpQR outperforms GPTQ and RTN at similar model size, especially on smaller models.
    - SpQR achieves better accuracy-size trade-off than GPTQ and RTN when controlling for model size.
    - The paper analyzes the impact of different hyperparameters, including group size and outlier type, on SpQR performance.
    - The paper evaluates the inference speed of SpQR and shows that its custom sparse matrix multiplication algorithm is faster than standard sparse matrix multiplication in PyTorch.
- **Citations:**
    - **Claim:** We evaluate SpQR on LLaMA {7, 13, 30, 65}B model family [TLI+23] and Falcon{7, 40}B model family [UAE23a].
        - **Citation:** [TLI+23, UAE23a]
        - **Explanation:** These citations introduce the LLaMA and Falcon model families, which are used for evaluation.
    - **Claim:** We compare SpQR against two other post-training quantization schemes: GPTQ [FAHA22] and simple rounding-to-nearest (RTN) quantization, which is used by most other LLM compression methods [DLBZ22, YAZ+22].
        - **Citation:** [FAHA22, DLBZ22, YAZ+22]
        - **Explanation:** These citations introduce the baseline methods used for comparison.
    - **Claim:** We evaluate quantized model performance by two metrics. Firstly, we measure perplexity, measured on the WikiText2 [MXBS16], Penn Treebank [MKM+94] and C4 [RSR+20] datasets.
        - **Citation:** [MXBS16, MKM+94, RSR+20]
        - **Explanation:** These citations introduce the datasets used for evaluating perplexity.
    - **Claim:** Secondly, we measure zero-shot accuracy on five tasks: WinoGrande [SBBC21], PiQA [TP03], HellaSwag, ARC-easy and ARC-challenge [CCE+18].
        - **Citation:** [SBBC21, TP03, CCE+18]
        - **Explanation:** These citations introduce the zero-shot tasks used for evaluating accuracy.
    - **Claim:** We use the LM Evaluation Harness [GTB+21] with recommended parameters.
        - **Citation:** [GTB+21]
        - **Explanation:** This citation introduces the LM Evaluation Harness, which is used for evaluating model performance.
    - **Claim:** We observe that SpQR outperforms GPTQ (and correspondingly RTN) at similar model size by a significant margin, especially on smaller models.
        - **Citation:** [RCK+20]
        - **Explanation:** This citation introduces the MLCommons benchmark, which is used for defining near-lossless compression.
    - **Claim:** The second set of results, presented in Table 1 for LLaMa and Table 2 for Falcon family models, controls model size by comparing SpQR and baseline methods with 4 bits per parameter.
        - **Citation:** [DZ22]
        - **Explanation:** This citation highlights the importance of controlling for model size in evaluating quantization methods.
    - **Claim:** We report both in Table 3, the "3-bit statistics" entry corresponds to group size 16 with 3-bit statistics and "16-bit statistics" stands for group size 16 with 16-bit statistics.
        - **Citation:** [DZ22]
        - **Explanation:** This citation references the work of Dettmers et al. for outlier detection.
    - **Claim:** We measure inference speed in two setups: i) generating 100 tokens from scratch and ii) adding 100 tokens on top of a 1024-token prefix (prompt).
        - **Citation:** [PGM+19]
        - **Explanation:** This citation introduces PyTorch, which is used for implementing the sparse matrix multiplication algorithm.

**2.7 Discussion and Limitations**

- **Key Points:**
    - SpQR achieves near-lossless compression with less than 4.75 bits per parameter on average.
    - SpQR is an ideal method for compressing models for memory-limited devices.
    - The paper does not evaluate the generative quality of quantized LLMs, but only the predictive performance in terms of zero-shot accuracy and perplexity.
    - The paper does not fuse sparse matrix multiplication with regular quantized matrix multiplication.
- **Citations:**
    - **Claim:** We achieve even better quality-size-tradeoff when compressing to as little as 3.36 bits which makes SpQR an ideal method for compressing models for memory-limited devices.
        - **Citation:** [None]
        - **Explanation:** This claim is not directly supported by any specific citation in the paper.
    - **Claim:** While we believe that perplexity measurements and generation quality are strongly related, this is a hypothesis we aim to investigate in future work.
        - **Citation:** [None]
        - **Explanation:** This claim is not directly supported by any specific citation in the paper.

**2.8 Future Work and Open Questions**

- **Key Points:**
    - The paper suggests evaluating the generative quality of quantized LLMs.
    - The paper suggests fusing sparse matrix multiplication with regular quantized matrix multiplication to improve inference time performance.
- **Citations:**
    - **Claim:** We leave the implementation of such an algorithm to future work.
        - **Citation:** [None]
        - **Explanation:** This claim is not directly supported by any specific citation in the paper.

**3. Key Insights and Supporting Literature**

- **Key Insight:** Weight sensitivity plays a crucial role in LLM quantization, and treating low- and high-sensitivity weights differently can significantly improve accuracy and compression.
    - **Supporting Citations:** [FAHA22, YAZ+22, DLBZ22, YLW+23]
    - **Explanation:** These citations highlight the importance of considering weight sensitivity in quantization and the limitations of existing methods in treating all weights equally.
- **Key Insight:** SpQR is a novel compression format that enables near-lossless quantization of LLMs across model scales while achieving similar compression levels to previous methods.
    - **Supporting Citations:** [FA23, DLBZ22, XLS+22, YAZ+22]
    - **Explanation:** These citations highlight the challenges of achieving near-lossless compression with existing methods and the need for new approaches.
- **Key Insight:** SpQR provides efficient algorithms for encoding and decoding weights, including a GPU inference algorithm that is faster than 16-bit baselines at similar accuracy.
    - **Supporting Citations:** [Neu22, GFS+19, KHB+21]
    - **Explanation:** These citations highlight the importance of efficient algorithms for encoding and decoding weights and the use of hybrid sparse-quantized formats in efficient CPU inference engines.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The paper evaluates SpQR on LLaMA and Falcon models using WikiText2, C4, Penn Treebank, and five zero-shot tasks.
    - The paper compares SpQR with GPTQ and RTN quantization methods, controlling for model size.
    - The paper analyzes the impact of different hyperparameters, including group size and outlier type, on SpQR performance.
    - The paper evaluates the inference speed of SpQR and compares it with standard sparse matrix multiplication in PyTorch.
- **Foundations:**
    - The paper builds upon the existing literature on post-training quantization (PTQ) methods for LLMs.
    - The paper uses GPTQ [FAHA22] as a baseline method for comparison.
    - The paper uses the MLCommons benchmark [RCK+20] for defining near-lossless compression.
- **Novel Aspects:**
    - The paper introduces a novel compression format called Sparse-Quantized Representation (SpQR).
    - The paper proposes a new approach to outlier detection and treatment.
    - The paper develops a custom sparse matrix multiplication algorithm for efficient GPU inference.
    - **Citations:**
        - **Claim:** We use groupwise quantization with extremely small groups, typically of B₁=8 – 32 weights.
            - **Citation:** [YLW+23]
            - **Explanation:** This citation justifies the use of small group sizes for quantization.
        - **Claim:** The algorithm aims to pick a sensitivity threshold 7 to obtain the desired number of outliers across the whole model, usually around 1% of weights.
            - **Citation:** [DZ22]
            - **Explanation:** This citation references the work of Dettmers et al. for outlier detection.
        - **Claim:** We will show in Section 5 that this custom approach is faster than the sparse matrix algorithms in PyTorch.
            - **Citation:** [None]
            - **Explanation:** This claim is not directly supported by any specific citation in the paper.

**5. Results in Context**

- **Main Results:**
    - SpQR achieves near-lossless compression with less than 4.75 bits per parameter on average.
    - SpQR outperforms GPTQ and RTN at similar model size, especially on smaller models.
    - SpQR achieves better accuracy-size trade-off than GPTQ and RTN when controlling for model size.
    - SpQR's custom sparse matrix multiplication algorithm is faster than standard sparse matrix multiplication in PyTorch.
- **Comparison with Existing Literature:**
    - SpQR's performance surpasses existing methods like GPTQ and RTN, especially for smaller models.
    - SpQR's near-lossless compression capabilities are significantly better than those reported in previous works.
    - SpQR's custom sparse matrix multiplication algorithm outperforms standard sparse matrix multiplication in PyTorch.
- **Confirmation, Contradiction, or Extension:**
    - SpQR's results confirm the importance of weight sensitivity in quantization and the limitations of existing methods in treating all weights equally.
    - SpQR's results extend the existing literature by demonstrating the feasibility of near-lossless compression for LLMs across model scales.
    - SpQR's results contradict the common belief that sparse matrix multiplication is slower than dense matrix multiplication.

**6. Discussion and Related Work**

- **Situating the Work:**
    - The authors situate their work within the context of post-training quantization (PTQ) methods for LLMs.
    - They highlight the limitations of existing methods in achieving near-lossless compression and scaling to large LLMs.
    - They emphasize the importance of weight sensitivity in quantization and the need for new approaches that treat low- and high-sensitivity weights differently.
- **Key Papers Cited:**
    - [FAHA22, YAZ+22, DLBZ22, YLW+23, FA23, DZ22, XLS+22, NAVB+20, GKD+21, FSA22, Neu22, GFS+19, KHB+21, RCK+20, HABN+21]
- **Novelty and Importance:**
    - The authors argue that SpQR is a novel compression format that addresses the limitations of existing methods and enables near-lossless compression of LLMs across model scales.
    - They highlight the importance of SpQR for deploying LLMs on memory-limited devices and its potential to make LLMs more accessible to a wider audience.

**7. Future Work and Open Questions**

- **Areas for Further Research:**
    - Evaluating the generative quality of quantized LLMs.
    - Fusing sparse matrix multiplication with regular quantized matrix multiplication to improve inference time performance.
- **Citations:**
    - **Claim:** We leave the implementation of such an algorithm to future work.
        - **Citation:** [None]
        - **Explanation:** This claim is not directly supported by any specific citation in the paper.

**8. Critical Analysis of Citation Usage**

- **Effectiveness:**
    - The authors generally use citations effectively to support their arguments and findings.
    - They provide a comprehensive overview of related work and highlight the key papers that inform their research.
- **Areas for Improvement:**
    - The paper could benefit from additional citations to support some of its claims, particularly in the discussion and limitations section.
    - For example, the claim that SpQR is an ideal method for compressing models for memory-limited devices could be strengthened by citing relevant work on mobile LLM deployment.
- **Potential Biases:**
    - The authors primarily cite papers from top conferences and journals, which may reflect a bias towards certain research communities.
    - The paper could benefit from including citations to work from less prominent venues or emerging research areas.

**9. Final Summary**

- **Contribution:** SpQR is a novel compression format that enables near-lossless quantization of LLMs across model scales while achieving similar compression levels to previous methods. SpQR outperforms existing methods like GPTQ and RTN in terms of accuracy, compression, and inference speed.
- **Influential Works:** [FAHA22, YAZ+22, DLBZ22, YLW+23, FA23, DZ22, XLS+22, NAVB+20, GKD+21, FSA22, Neu22, GFS+19, KHB+21, RCK+20, HABN+21]
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. It provides a comprehensive overview of related work and highlights the key papers that inform its research. However, the paper could benefit from additional citations to support some of its claims, particularly in the discussion and limitations section.

Overall, SpQR is a significant contribution to the field of LLM compression. It addresses the limitations of existing methods and provides a new approach that enables near-lossless compression of LLMs across model scales. The paper is well-written and provides a comprehensive analysis of the problem, the proposed solution, and the experimental results. However, the paper could benefit from additional citations to support some of its claims and a more balanced representation of the broader research landscape.