## Rewarded Soups: Towards Pareto-Optimal Alignment by Interpolating Weights Fine-Tuned on Diverse Rewards

**1. Introduction**

- **Title:** Rewarded Soups: Towards Pareto-Optimal Alignment by Interpolating Weights Fine-Tuned on Diverse Rewards
- **Authors:** Alexandre Rame, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, Matthieu Cord
- **Publication Date:** 16 October 2023 (v2)
- **Objective:** The paper proposes a novel multi-policy strategy called "rewarded soup" to address the limitations of single-policy alignment in deep reinforcement learning (RLHF) by embracing the heterogeneity of diverse rewards and aiming for Pareto-optimal generalization across the entire space of preferences.
- **Number of References:** 187

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - Foundation models are pre-trained on vast unsupervised datasets and fine-tuned on labeled data. [1]
    - Reinforcement learning, particularly RLHF, can further align the network with intended usage. [12, 15, 16, 17]
    - Imperfections in proxy rewards can hinder training and lead to suboptimal results. [9, 10, 11]
    - Diversity of objectives in real-world tasks and human opinions exacerbate the issue. [36, 37, 38]
    - Existing works align towards a consensus-based user, relying on the "wisdom of the crowd" and prioritizing certain principles. [47, 48, 42, 50]
    - This can result in unfair representations of marginalized groups and lack of transparency and explainability. [51, 52, 13, 55]
    - Multi-objective reinforcement learning (MORL) literature suggests shifting from single-policy to multi-policy approaches. [45, 46, 57, 58, 59, 60, 61, 62, 54]
    - The goal is to learn a set of Pareto-optimal networks rather than a single network. [63]
- **Significant Citations:**
    - **[1] Bommasani et al., 2021:** This citation introduces the concept of foundation models and their potential benefits and risks.
    - **[12, 15, 16, 17] Stiennon et al., 2020; Christiano et al., 2017; Ziegler et al., 2019; Wu et al., 2021:** These citations highlight the use of RLHF in aligning language models with human preferences.
    - **[9, 10, 11] Amodei et al., 2016; Taylor et al., 2016; Ngo et al., 2022:** These citations discuss the challenges of reward misspecification and the alignment problem in deep learning.
    - **[36, 37, 38] Wildavsky, 1987; Coello, 2000; Schwartz et al., 2012:** These citations emphasize the diversity of human preferences and the need to consider multiple objectives.
    - **[47, 48, 42, 50] Bakker et al., 2022; Ovadya, 2023; Ganguli et al., 2022; Kovač et al., 2023:** These citations highlight the limitations of single-policy alignment strategies and the need for multi-policy approaches.
    - **[51, 52, 13, 55] Weidinger et al., 2021; Kirk et al., 2023; Ouyang et al., 2022; Santurkar et al., 2023:** These citations discuss the potential biases and lack of transparency in single-policy alignment strategies.
    - **[45, 46, 57, 58, 59, 60, 61, 62, 54] Barrett et al., 2008; Li et al., 2020; Van Moffaert et al., 2014; Roijers et al., 2013; Rădulescu et al., 2020; Marta et al., 2023; Wu et al., 2023; Hayes et al., 2022:** These citations introduce the concept of MORL and its potential benefits in addressing diverse rewards.
    - **[63] Pareto, 1964:** This citation introduces the concept of Pareto-optimality, which is central to the paper's argument.

**2.2 Rewarded Soups**

- **Key Points:**
    - The paper proposes "rewarded soup" (RS), a multi-policy strategy that fine-tunes multiple networks independently for each proxy reward and then combines them according to user preferences.
    - RS leverages linear mode connectivity (LMC) to achieve weight interpolation. [65, 66]
    - RS offers a continuous set of Pareto-optimal solutions, approximating the more costly MORL strategy. [45, 46]
    - RS is computationally efficient and flexible, allowing for a posteriori selection of weights based on user preferences.
- **Significant Citations:**
    - **[65, 66] Frankle et al., 2020; Neyshabur et al., 2020:** These citations introduce the concept of LMC and its implications for weight interpolation.
    - **[45, 46] Barrett et al., 2008; Li et al., 2020:** These citations highlight the limitations of MORL and the need for more efficient multi-policy strategies.

**2.3 RL Fine-Tuning with Diverse Rewards**

- **Key Points:**
    - The paper defines the problem of aligning a deep neural network with a desired reward function R, given a test distribution T of deployment. [73, 74, 75]
    - Reward misspecification between the proxy reward R' and the true reward R can hinder alignment. [9, 34]
    - The paper proposes using a family of N diverse proxy rewards {Ri}1 to address the diversity of human preferences. [45, 46]
    - The paper introduces the MORL baseline, which uses scalarization to linearize the problem by interpolating proxy rewards. [45, 46, 62]
    - MORL is inefficient due to the need for maintaining a large set of networks. [77]
- **Significant Citations:**
    - **[73, 74, 75] Ioffe et al., 2015; Agarap, 2018; Vaswani et al., 2017:** These citations describe the architecture of deep neural networks used in the paper.
    - **[9, 34] Amodei et al., 2016; Pan et al., 2022:** These citations discuss the challenges of reward misspecification.
    - **[45, 46] Barrett et al., 2008; Li et al., 2020:** These citations introduce the concept of MORL.
    - **[62] Wu et al., 2023:** This citation highlights the recent use of MORL in aligning LLMs.
    - **[77] Vamplew et al., 2008:** This citation discusses the limitations of MORL in terms of scalability.

**2.4 Exploring the Properties of the Rewarded Soups Set of Solutions**

- **Key Points:**
    - The paper defines linear mode connectivity (LMC) for multiple rewards and proposes Working Hypothesis 1, which states that LMC holds if all rewards for the interpolated weights exceed the interpolated rewards. [65, 66, 67, 72]
    - The paper defines Pareto optimality and proposes Working Hypothesis 2, which states that the set of interpolated weights is a Pareto coverage set (PCS). [45, 46]
    - The paper provides theoretical justification for Working Hypotheses 1 and 2 in a simplified setup with quadratic rewards and co-diagonalizable Hessians. [66, 78, 79, 80, 81]
    - The paper proves that RS mitigates reward misspecification for linear rewards under Hypothesis 2. [60, 77]
- **Significant Citations:**
    - **[65, 66, 67, 72] Frankle et al., 2020; Neyshabur et al., 2020; Wortsman et al., 2022; Ramé et al., 2023:** These citations introduce the concept of LMC and its applications in weight interpolation.
    - **[45, 46] Barrett et al., 2008; Li et al., 2020:** These citations introduce the concept of Pareto optimality.
    - **[66, 78, 79, 80, 81] Neyshabur et al., 2020; Hansen et al., 1990; Lakshminarayanan et al., 2017; Entezari et al., 2022; Ainsworth et al., 2023:** These citations provide theoretical justification for Working Hypotheses 1 and 2.
    - **[60, 77] Rădulescu et al., 2020; Vamplew et al., 2008:** These citations discuss the limitations of MORL and the need for more efficient multi-policy strategies.

**3. Key Insights and Supporting Literature**

- **Key Insight 1:** Rewarded soup (RS) is a computationally efficient and flexible multi-policy strategy that can achieve Pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards.
    - **Supporting Citations:** [65, 66, 67, 72, 45, 46, 77]
    - **Explanation:** The authors cite works on linear mode connectivity (LMC) to justify the use of weight interpolation in RS. They also cite works on MORL to highlight the limitations of existing multi-policy strategies and the need for more efficient approaches.
- **Key Insight 2:** RS leverages linear mode connectivity (LMC) to achieve weight interpolation, which is particularly well-suited for fine-tuning foundation models. [65, 66, 67, 72]
    - **Supporting Citations:** [65, 66, 67, 72]
    - **Explanation:** The authors cite works on LMC to demonstrate that weights fine-tuned from a shared pre-trained initialization remain linearly connected, enabling weight interpolation.
- **Key Insight 3:** RS mitigates reward misspecification for linear rewards under the assumption of Pareto optimality. [60, 77]
    - **Supporting Citations:** [60, 77]
    - **Explanation:** The authors cite works on MORL to highlight the limitations of existing multi-policy strategies and the need for more efficient approaches. They also provide theoretical justification for this insight in a simplified setup with quadratic rewards.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The paper evaluates RS across a variety of standard learning tasks, including text-to-text generation, image captioning, image generation, visual grounding, visual question answering, and locomotion.
    - The authors use either model or statistical rewards and follow a systematic procedure:
        1. Independent fine-tuning of diverse rewards on training samples.
        2. Evaluation of rewards on test samples.
        3. Verification of Working Hypothesis 1 by comparing RS's rewards with interpolated rewards.
        4. Empirical support for Working Hypothesis 2 by comparing RS's front with MORL's front.
- **Foundations:**
    - The authors use the trl package [85] and the setup from [86] with low-rank adapters (LoRA) [87] for RL training with PPO [84].
    - The authors use the ExpansionNetv2 [96] network and a Swin Transformer [97] visual encoder for image captioning.
    - The authors use the UnIVAL [106] model for visual grounding.
    - The authors use the OFA model [107] for visual question answering.
    - The authors use the Brax physics engine [113] for locomotion.
- **Novel Aspects:**
    - The authors introduce the novel concept of "rewarded soup" and demonstrate its effectiveness across a variety of tasks and rewards.
    - The authors extend the concept of LMC to multiple rewards and provide theoretical justification for its use in RS.
    - The authors demonstrate the scalability of RS to handle more than two rewards.

**5. Results in Context**

- **Main Results:**
    - RS consistently outperforms MORL in terms of Pareto-optimality across all tasks and rewards.
    - RS is computationally efficient and flexible, allowing for a posteriori selection of weights based on user preferences.
    - RS mitigates reward misspecification for linear rewards.
    - RS is robust to variations in the number of training steps and the number of rewards.
    - RS can be used to fine-tune foundation models with diverse rewards, including text-to-text generation, image captioning, image generation, visual grounding, visual question answering, and locomotion.
- **Comparison with Existing Literature:**
    - The authors compare RS with MORL and demonstrate that RS achieves similar or better performance with significantly lower computational cost.
    - The authors compare RS with model soups [67] and demonstrate that RS is more effective in addressing reward misspecification.
    - The authors compare RS with other multi-policy approaches [117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134] and demonstrate that RS is more efficient and scalable.
- **Confirmation, Contradiction, or Extension:**
    - The authors' results confirm the theoretical findings on LMC and Pareto optimality.
    - The authors' results demonstrate the effectiveness of RS in addressing reward misspecification, which is a significant challenge in RLHF.
    - The authors' results extend the concept of LMC to multiple rewards and demonstrate its applicability to a wider range of tasks.

**6. Discussion and Related Work**

- **Situating the Work:**
    - The authors situate their work within the broader context of RLHF and multi-objective reinforcement learning (MORL).
    - They discuss the limitations of existing single-policy and multi-policy approaches and highlight the need for more efficient and flexible strategies.
    - They argue that RS is a significant step towards better empirical alignment and addresses the challenges of reward misspecification and generalization.
- **Key Papers Cited:**
    - **[12, 13, 15, 16, 17, 45, 46, 57, 58, 59, 60, 61, 62, 54, 65, 66, 67, 72, 77, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]: These citations highlight the key works in RLHF, MORL, LMC, and weight interpolation.
- **Novelty and Importance:**
    - The authors highlight the novelty of RS in its ability to achieve Pareto-optimal alignment with diverse rewards while being computationally efficient and flexible.
    - They argue that RS is a significant step towards better empirical alignment and addresses the challenges of reward misspecification and generalization.

**7. Future Work and Open Questions**

- **Areas for Further Research:**
    - Exploring the theoretical guarantees for RS in more complex settings with non-linear rewards.
    - Investigating the use of RS in combination with other multi-policy approaches, such as MORL.
    - Exploring the use of RS in federated learning settings to address privacy concerns.
    - Developing methods for automatically selecting the optimal interpolation coefficient A based on user preferences.
    - Investigating the impact of RS on the robustness and generalization of foundation models.
- **Citations:**
    - **[146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]:** These citations highlight the key works in RLHF, MORL, LMC, and weight interpolation.

**8. Critical Analysis of Citation Usage**

- **Effectiveness:**
    - The authors effectively use citations to support their arguments and findings.
    - They provide a comprehensive overview of the relevant literature and clearly explain how cited works contribute to their own research.
- **Areas for Improvement:**
    - The authors could have provided more citations to support their claims about the limitations of existing single-policy and multi-policy approaches.
    - The authors could have provided more citations to support their claims about the societal impacts of AI alignment.
- **Potential Biases:**
    - The authors primarily cite works from the fields of RLHF, MORL, and LMC, which may reflect a bias towards these areas of research.
    - The authors could have included more citations from other relevant fields, such as ethics, social science, and philosophy.

**9. Final Summary**

- **Contribution:** The paper makes a significant contribution to the field of AI alignment by introducing a novel multi-policy strategy called "rewarded soup" that addresses the limitations of single-policy alignment in RLHF.
- **Influential Works:** [12, 13, 15, 16, 17, 45, 46, 57, 58, 59, 60, 61, 62, 54, 65, 66, 67, 72, 77]
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. The authors provide a comprehensive overview of the relevant literature and clearly explain how cited works contribute to their own research.

**Overall Assessment:** This paper is a valuable contribution to the field of AI alignment. The authors introduce a novel and promising multi-policy strategy that addresses the limitations of existing approaches. The paper is well-written, well-researched, and provides a comprehensive overview of the relevant literature. The authors' findings are supported by strong empirical evidence and theoretical justification. The paper raises important questions about the future of AI alignment and the societal impacts of this research.
