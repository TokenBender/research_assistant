## Anticipatory Music Transformer: A Comprehensive Analysis

This analysis focuses on the paper "Anticipatory Music Transformer" by John Thickstun, David Hall, Chris Donahue, and Percy Liang, published in Transactions on Machine Learning Research (04/2024). The paper introduces a novel method called "anticipation" for constructing controllable generative models of temporal point processes, specifically focusing on symbolic music generation. The paper cites a total of 108 references.

### 1. Introduction

The paper aims to develop a controllable generative model for symbolic music generation, specifically addressing the task of infilling accompaniment. This is achieved by introducing a new method called "anticipation," which interleaves sequences of events and controls to create a bidirectional context for prediction.

### 2. Section-by-Section Analysis with Citation Extraction

**2.1 Introduction:**

- **Key Point:** The paper highlights the challenge of generating music with user-specified controls, particularly in the context of accompaniment generation.
- **Citation:** Louie et al., 2020. "Generating an accompaniment to a given melody is an example of a control task: we seek the ability to generate an accompaniment (the events) conditioned on a given melody (the controls). Models that generate symbolic music (i.e., compose) subject to user-specified controls are of broad interest as tools for music co-creation."
- **Relevance:** This citation establishes the broader context of the research, highlighting the importance of controllable music generation in the field.

- **Key Point:** The paper emphasizes the use of infilling as a powerful control mechanism for music generation, citing previous work on musical infilling.
- **Citation:** Huang et al., 2017. "Infilling is a powerful control mechanism for music generation: previous work on musical infilling (Huang et al., 2017) powered the J.S. Bach Google Doodle (Huang et al., 2019), an interactive music experience with broad popular appeal."
- **Relevance:** This citation connects the paper's research to existing work on infilling, demonstrating its relevance to the field and highlighting the potential impact of the proposed method.

**2.2 The Dynamics of Temporal Point Processes:**

- **Key Point:** The paper discusses the limitations of traditional autoregressive models and sequence-to-sequence models for handling long-range dependencies in conditional generation.
- **Citation:** Du et al., 2016. "The dynamics of a temporal point process can be captured by a neural autoregressive model trained to predict the next event in a time-ordered sequence (Du et al., 2016)."
- **Relevance:** This citation introduces the concept of temporal point processes and their modeling using autoregressive models, providing a foundation for the paper's proposed approach.

- **Key Point:** The paper highlights the need for a structured approach to conditional generation that considers both recent event history and future controls.
- **Citation:** Sutskever et al., 2014. "Standard practice to efficiently train an autoregressive model relies on the observation that context for prediction at one index in the sequence is a prefix of the context for predictions at future indices. This allows us to update the model based on M - 1 predictions for each sequence of length M. Conditioning on asynchronous controls by constructing an ad-hoc context (e.g., the M/2 previous events and M/2 nearest controls) to predict each event would be computationally wasteful: we want to define a single, coherent interleaved sequence of events and controls."
- **Relevance:** This citation discusses the limitations of traditional sequence-to-sequence models for handling asynchronous controls, motivating the need for a more structured approach.

**2.3 Anticipation:**

- **Key Point:** The paper introduces the concept of anticipation, a method for interleaving asynchronous events and controls in a way that allows for tractable sampling and maintains proximity between events and controls.
- **Citation:** Billingsley, 1995. "For tractable sampling, we will see in Section 3 that the index in the interleaved sequence that immediately precedes a control must be a stopping time (Billingsley, 1995)."
- **Relevance:** This citation introduces the concept of stopping times, which is crucial for the proposed method of anticipation.

- **Key Point:** The paper describes the interleaved structure of anticipation and its relationship to traditional autoregressive and sequence-to-sequence models.
- **Citation:** Wiener et al., 1949. "Model predictions under the anticipatory ordering a1:N+K combine a filtering (i.e., causal) estimate based on the local history of events with a smoothing (i.e., bidirectional) estimate based on local controls (Wiener et al., 1949)."
- **Relevance:** This citation connects the proposed method of anticipation to existing concepts in signal processing, highlighting its theoretical foundation.

**2.4 Music as a Temporal Point Process:**

- **Key Point:** The paper defines a marked temporal point process for representing music, using a vocabulary of musical notes and other events.
- **Citation:** Daley & Vere-Jones, 2007. "A marked temporal point process is a probability distribution over events ei = (ti, mi), where ti ∈ R+ (ti < tj if i < j) is a point in time and m₁ ∈ V is a mark from a finite vocabulary V."
- **Relevance:** This citation introduces the formal definition of a marked temporal point process, providing a theoretical framework for modeling music as a stochastic process.

- **Key Point:** The paper discusses the limitations of piano-roll representations for modeling music, particularly for diverse and rhythmically intricate music.
- **Citation:** Boulanger-Lewandowski et al., 2012. "Older work on music generation typically rasterizes time, encoding music as a uniformly sampled matrix or tensor, i.e., a piano-roll (Boulanger-Lewandowski et al., 2012; Dong et al., 2018)."
- **Relevance:** This citation highlights the limitations of traditional piano-roll representations for modeling music, motivating the use of temporal point processes.

**2.5 Modeling Temporal Point Processes:**

- **Key Point:** The paper discusses the use of conditional intensity functions for modeling temporal point processes and contrasts it with the approach of modeling the probability distribution over the next event in a time-ordered sequence.
- **Citation:** Du et al., 2016. "Much of the machine learning literature on temporal point processes focuses on modeling conditional intensity functions (Du et al., 2016; Mei & Eisner, 2017; Omi et al., 2019; Zuo et al., 2020)."
- **Relevance:** This citation provides a broader context for the paper's approach, highlighting the different methods used for modeling temporal point processes.

- **Key Point:** The paper emphasizes the use of discrete distributions over quantized time values for modeling temporal point processes, allowing for the application of autoregressive transformers and large language models.
- **Citation:** Shchur et al., 2020. "In contrast, we will model the probability distribution over the next event in a time-ordered sequence. In this regard, our work is most comparable to Shchur et al. (2020); but whereas that work models continuous densities, we model discrete distributions over quantized time values."
- **Relevance:** This citation highlights the novelty of the paper's approach, which allows for the application of modern deep learning techniques to modeling temporal point processes.

**2.6 Modeling Arrival Times:**

- **Key Point:** The paper introduces a novel arrival-time encoding for representing events and controls, which is amenable to autoregressive sequence modeling and facilitates anticipation.
- **Citation:** Huang et al., 2018. "The quantization proposed by Music Transformer (Huang et al., 2018) with a maximum time of 10ms intervals. We encode arrival times as sequences using a finite vocabulary of 10,000 values, quantizing time in 10ms intervals. Between successive arrival times that exceed 100 seconds, we quantize time in 1s intervals. This results in a vocabulary of 16,512 values. We found that this quantization scheme results in a reduction of 1024 differences between successive arrival times during preprocessing. By having less than 0.1% MIDI realizations of examples we discard, these models described in this paper appear in length we reduce Lakh MIDI training examples by 0.1%."
- **Relevance:** This citation introduces the concept of arrival-time encoding and its application to music modeling, providing a foundation for the paper's proposed approach.

- **Key Point:** The paper discusses the use of interarrival-time encoding and its relationship to arrival-time encoding.
- **Citation:** Huang & Yang, 2020. "In contrast to encoding including arrival-time tokens are determined contextually by their positions in the music context sensitive: the timings of REMI (Huang & Yang, 2020) and Octuple-Midi (Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common music encoding schemes (Huang et al., 2018; Huang & Yang, 2020; Zeng et al., 2021) are context sensitive: the interrival-time tokenization described by Definition 2.3 and other common