## Evaluating the Zero-shot Robustness of Instruction-tuned Language Models: A Citation-Focused Analysis

**1. Introduction**

- **Title:** Evaluating the Zero-shot Robustness of Instruction-tuned Language Models
- **Authors:** Jiuding Sun, Chantal Shaib, Byron C. Wallace
- **Publication Date:** 2023 (Preprint, under review)
- **Objective:** The paper investigates the sensitivity of instruction-tuned large language models (LLMs) to variations in instruction phrasing and proposes a method to improve their robustness to such variations.
- **Total References:** 40

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - Instruction fine-tuning has emerged as a promising approach for improving LLMs' zero-shot capabilities on new tasks.
    - This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants.
    - The paper investigates the sensitivity of instruction-tuned models to variations in instruction phrasing and proposes a method to improve their robustness to such variations.
- **Significant Citations:**
    - **Claim:** Instruction fine-tuning has emerged as a promising approach for improving LLMs' zero-shot capabilities on new tasks.
    - **Citation:** [3; 4; 10; 37]
    - **Explanation:** These citations refer to works that demonstrate the promise of prompting and few-shot learning in LLMs, setting the stage for the discussion of instruction fine-tuning.
    - **Claim:** This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants.
    - **Citation:** [5; 22]
    - **Explanation:** These citations highlight the success of instruction fine-tuning in improving the performance of smaller LLMs, motivating the paper's investigation into their robustness.

**2.2 Related Work**

- **Key Points:**
    - The authors review prior work on multitask learning and instruction-tuning, highlighting the development of large-scale instruction-tuning datasets and the encouraging results achieved by instruction-tuned models.
    - They discuss existing research on evaluating prompting and instruction capabilities, including studies that question whether models truly "understand" prompts and investigate the effectiveness of in-context learning.
    - The authors also review previous efforts to improve instruction-tuning, such as using human feedback, automatically generating instruction-tuning datasets, and engineering prompts.
- **Significant Citations:**
    - **Claim:** Prior work has established that LLMs do not seem to intuitively “understand” prompts.
    - **Citation:** [32; 12; 38]
    - **Explanation:** These citations support the claim that LLMs may not fully understand the meaning of prompts, setting the stage for the paper's investigation into the robustness of instruction-tuned models.
    - **Claim:** Recent, contemporaneous work to ours investigated the robustness of instruction-tuned models.
    - **Citation:** [8]
    - **Explanation:** This citation highlights a recent study that investigated the robustness of instruction-tuned models, providing context for the paper's own contributions.
    - **Claim:** These meta-resources—collections of instructions, tasks, and samples—have facilitated the training of instruction-tuned model families such as Flan-T5, Flan-PaLM, and OPT-IML.
    - **Citation:** [33; 20; 24; 31; 11; 17]
    - **Explanation:** These citations showcase the development of large-scale instruction-tuning datasets, which have enabled the training of instruction-tuned models and motivated further research in this area.

**2.3 Instruction Datasets**

- **Key Points:**
    - The authors describe the two large benchmarks used for evaluation: MMLU and BIG-BENCH LITE.
    - They explain the process of collecting novel instructions from NLP researchers, emphasizing the semantic equivalence but superficial differences from the instructions used during training.
- **Significant Citations:**
    - **Claim:** We evaluate a set of instruction-tuned models on two large benchmarks: MMLU and BIG-BENCH LITE.
    - **Citation:** [9; 25]
    - **Explanation:** These citations introduce the benchmarks used for evaluating the performance of instruction-tuned models.

**2.4 Evaluating the Robustness of Instruction-tuned LLMs**

- **Key Points:**
    - The authors describe the experimental setup, including the models, data, and benchmarks used.
    - They define the three categories of tasks considered: multiple-choice question answering (QA), binary classification (BC), and multi-class classification (MC).
- **Significant Citations:**
    - **Claim:** We conduct experiments with model variants trained over three instruction collections (these provide observed task instructions): P3, Flan-2022, and Alpaca.
    - **Citation:** [24; 5; 26]
    - **Explanation:** These citations identify the instruction-tuning datasets used to train the models, providing context for the analysis of model robustness.

**2.5 Results**

- **Key Points:**
    - The authors present aggregated results showing that using novel instructions consistently degrades model performance, with classification tasks being most affected.
    - They observe that incorrect but observed instructions outperform appropriate but unobserved instructions, suggesting that models rely heavily on previously observed instructions.
    - They investigate the relationship between semantic distance and robustness, finding that models are more robust to unobserved instructions when those instructions are semantically similar to the observed ones.
    - They explore the impact of scaling on robustness, finding that larger models do not necessarily exhibit greater robustness.
- **Significant Citations:**
    - **Claim:** We observe that using instructions unobserved in training—but manually composed for the task at hand and so semantically appropriate—leads to considerable degradation in performance.
    - **Citation:** None
    - **Explanation:** This finding is novel and not directly supported by any specific citations.
    - **Claim:** Incorrect but observed instructions outperform appropriate but unobserved instructions.
    - **Citation:** None
    - **Explanation:** This finding is novel and not directly supported by any specific citations.
    - **Claim:** We plot mean performance degradation (as %) as a function of average similarity between the similarity of the first decoded tokens (following unobserved instructions) and the same for the most similar observed instruction.
    - **Citation:** [28]
    - **Explanation:** This citation refers to the t-SNE method used for visualizing the representations of instructions.

**2.6 A Closer Look at Instruction Robustness**

- **Key Points:**
    - The authors delve deeper into the performance degradation observed when using novel instructions, highlighting the curious finding that incorrect but observed instructions outperform appropriate but unobserved ones.
    - They suggest that this finding indicates that instruction-tuned models may overrely on previously observed instructions and do not generalize well to new instructions and phrasings.
- **Significant Citations:**
    - **Claim:** Incorrect but observed instructions outperform appropriate but unobserved instructions.
    - **Citation:** None
    - **Explanation:** This finding is novel and not directly supported by any specific citations.

**2.7 Robustness with Semantic Distance**

- **Key Points:**
    - The authors observe that performance on MMLU is less affected by using unobserved instructions, hypothesizing that this is due to the greater similarity between observed and unobserved instructions for this benchmark.
    - They provide empirical evidence supporting this hypothesis by visualizing the representations of observed and unobserved instructions using t-SNE and calculating the average ℓ2 distance between them.
- **Significant Citations:**
    - **Claim:** We empirically verify this in Figure 4 and Table 4.
    - **Citation:** [28]
    - **Explanation:** This citation refers to the t-SNE method used for visualizing the representations of instructions.

**2.8 Robustness Under In-Context Learning (ICL)**

- **Key Points:**
    - The authors briefly explore the robustness of instruction-tuned models under in-context learning (ICL), finding that ICL slightly decreases the sensitivity of models to unobserved instructions.
- **Significant Citations:**
    - **Claim:** Previous study has shown that the LLMs are less sensitive to prompt / instruction variation when few-shot examples are provided in context.
    - **Citation:** [8]
    - **Explanation:** This citation refers to a previous study that investigated the robustness of instruction-tuned models under ICL, providing context for the paper's own findings.

**2.9 Aligning Equivalent Instructions**

- **Key Points:**
    - The authors propose a simple method to improve the robustness of instruction-tuned models by introducing "soft prompt" embedding parameters and optimizing them to maximize the similarity between representations of semantically equivalent instructions.
    - They explain the intuition behind this method and describe the objective function used for optimization.
- **Significant Citations:**
    - **Claim:** We now introduce a simple method to improve the robustness of instruction-tuned LLMs.
    - **Citation:** [16]
    - **Explanation:** This citation refers to a previous work on prefix tuning, which inspired the authors' approach to introducing soft prompt parameters.

**2.10 Results**

- **Key Points:**
    - The authors evaluate the proposed method using two representative instruction-tuned LLMs: Flan-XL and Alpaca.
    - They compare the performance of these models with variants fine-tuned using different components of the proposed method, including fine-tuning all model parameters, introducing soft prompt parameters, and adding a KL loss term.
    - They find that the proposed method consistently improves performance, especially on unobserved instructions.
- **Significant Citations:**
    - **Claim:** We experiment with the proposed method using two representative instruction-tuned LLMs: Flan-XL and Alpaca.
    - **Citation:** [33; 26]
    - **Explanation:** These citations identify the models used for evaluating the proposed method.

**2.11 Conclusions**

- **Key Points:**
    - The authors conclude that instruction-tuned LLMs are promising for achieving zero-shot performance with smaller models, but they are unduly sensitive to instruction phrasings.
    - They highlight the importance of their proposed method for improving the robustness of instruction-tuned models.
- **Significant Citations:**
    - **Claim:** Instruction-tuned LLMs have emerged as a promising means of achieving zero-shot performance with smaller models that is competitive to, and sometimes even better than, that observed using much larger LLMs.
    - **Citation:** [17; 26]
    - **Explanation:** These citations highlight the success of instruction-tuning in achieving zero-shot performance with smaller models, providing context for the paper's findings.

**2.12 Limitations**

- **Key Points:**
    - The authors acknowledge limitations of their work, including the focus on "mid-sized" models, the restricted evaluation to three task types, and the potential for LLMs to encode problematic biases.
- **Significant Citations:**
    - **Claim:** This work has important limitations: For example we only evaluated “mid-sized” models (<20B parameters), it is unclear if our findings would generalize to much larger instruction-tuned models.
    - **Citation:** None
    - **Explanation:** This limitation is acknowledged but not directly supported by any specific citations.

**3. Key Insights and Supporting Literature**

- **Insight:** Instruction-tuned LLMs are not especially robust to instruction re-phrasings, even when those re-phrasings are semantically equivalent.
    - **Supporting Citations:** [32; 12; 38; 8]
    - **Explanation:** These citations highlight previous research that questioned whether LLMs truly "understand" prompts and investigated the robustness of instruction-tuned models, providing context for this key insight.
- **Insight:** A simple method for improving the robustness of instruction-tuned models involves introducing "soft prompt" embedding parameters and optimizing them to maximize the similarity between representations of semantically equivalent instructions.
    - **Supporting Citations:** [16]
    - **Explanation:** This citation refers to a previous work on prefix tuning, which inspired the authors' approach to introducing soft prompt parameters.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The authors evaluate three families of instruction-tuned models: Flan-T5, Alpaca, and T0.
    - They use two benchmarks: MMLU and BIG-BENCH LITE.
    - They collect a large set of new task instructions manually composed by researchers in NLP.
    - They conduct experiments using both observed and unobserved instructions.
    - They investigate the impact of scaling on robustness.
    - They propose a method to improve robustness by introducing "soft prompt" embedding parameters and optimizing them to maximize the similarity between representations of semantically equivalent instructions.
- **Foundations:**
    - The authors use existing benchmarks and instruction-tuning datasets as a basis for their methodology.
    - They cite previous work on evaluating prompting and instruction capabilities to justify their investigation into the robustness of instruction-tuned models.
    - They cite previous work on prefix tuning to support their novel approach to improving robustness.

**5. Results in Context**

- **Main Results:**
    - Using novel instructions consistently degrades model performance, with classification tasks being most affected.
    - Incorrect but observed instructions outperform appropriate but unobserved ones.
    - Models are more robust to unobserved instructions when those instructions are semantically similar to the observed ones.
    - Scaling does not necessarily fix the issue of performance degradation when using novel instructions.
    - The proposed method for improving robustness consistently improves performance, especially on unobserved instructions.
- **Comparison with Existing Literature:**
    - The authors' findings confirm previous research that questioned whether LLMs truly "understand" prompts and investigated the robustness of instruction-tuned models.
    - Their results extend previous work by providing a more comprehensive analysis of robustness across a wider range of models and benchmarks.
    - Their proposed method for improving robustness is novel and not directly compared to any specific cited works.

**6. Discussion and Related Work**

- **Key Papers Cited:**
    - [32; 12; 38; 8; 16]
- **Novelty and Importance:**
    - The authors highlight the novelty of their findings regarding the sensitivity of instruction-tuned models to instruction re-phrasings.
    - They emphasize the importance of their proposed method for improving the robustness of instruction-tuned models, particularly in zero-shot settings.

**7. Future Work and Open Questions**

- **Areas for Further Research:**
    - Investigating the robustness of larger instruction-tuned models.
    - Evaluating the proposed method on a wider range of task types.
    - Exploring the interaction between instruction-tuning and problematic biases encoded in LLMs.
- **Supporting Citations:**
    - **Claim:** It is unclear if our findings would generalize to much larger instruction-tuned models.
    - **Citation:** None
    - **Explanation:** This suggestion for future work is not directly supported by any specific citations.

**8. Critical Analysis of Citation Usage**

- **Effectiveness:**
    - The authors generally use citations effectively to support their arguments and findings.
    - They provide a comprehensive overview of related work, highlighting key papers and contextualizing their own contributions.
- **Areas for Improvement:**
    - The authors could have provided more specific citations to support their novel findings regarding the sensitivity of instruction-tuned models to instruction re-phrasings and the effectiveness of their proposed method for improving robustness.
- **Potential Biases:**
    - The authors primarily cite works from the same research group or related institutions, which may indicate a potential bias in the selection of cited works.

**9. Final Summary**

- **Contribution:** The paper makes a significant contribution to the field by providing a comprehensive analysis of the robustness of instruction-tuned LLMs to variations in instruction phrasing. It highlights the importance of this issue for the practical application of instruction-tuned models and proposes a novel method for improving their robustness.
- **Influential Works:**
    - [32; 12; 38; 8; 16]
- **Integration of Existing Literature:**
    - The paper effectively integrates existing literature to support its claims and findings, providing a clear overview of related work and contextualizing its own contributions. However, it could have provided more specific citations to support its novel findings and proposed method.

Overall, this paper provides valuable insights into the robustness of instruction-tuned LLMs and offers a promising solution for improving their performance in zero-shot settings. The authors' thorough analysis and clear presentation make this paper a valuable contribution to the field. However, the paper could benefit from more specific citations to support its novel findings and proposed method.