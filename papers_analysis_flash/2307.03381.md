## Analysis of "Teaching Arithmetic to Small Transformers"

This paper, "Teaching Arithmetic to Small Transformers" by Nayoung Lee, Kartik Sreenivasan, Jason D. Lee, Kangwook Lee, and Dimitris Papailiopoulos, published on arXiv on July 7, 2023, investigates how small transformer models can efficiently learn basic arithmetic operations using the next-token prediction objective. The paper cites a total of 47 references.

### 1. Introduction

The paper explores the emergence of arithmetic capabilities in large language models (LLMs) like GPT-3/4, PaLM, and LaMDA, which are not explicitly encoded in their training objective. The authors aim to understand the key factors that contribute to the emergence of these abilities, focusing on data format, model scale, and the presence of pre-training.

### 2. Section-by-Section Analysis with Citation Extraction

**2.1. Introduction**

- **Claim:** LLMs like GPT-3/4, PaLM, and LaMDA exhibit emergent abilities for tasks like language and code translation, compositional reasoning, and basic arithmetic operations.
    - **Citation:** (Brown et al., 2020; Chowdhery et al., 2022; Thoppilan et al., 2022; Wei et al., 2022b; Nye et al., 2021; Wei et al., 2022c; Shi et al., 2022; Wang et al., 2022; Srivastava et al., 2022; Chen et al., 2023)
    - **Explanation:** This citation lists several papers that demonstrate the emergent capabilities of LLMs across various tasks, including arithmetic.
- **Claim:** These capabilities are not explicitly encoded in the model's training objective, which is typically an auto-regressive, next-token-prediction loss.
    - **Citation:** (Wei et al., 2022b; Chung et al., 2022; Tay et al., 2022)
    - **Explanation:** This citation highlights research that explores the emergence of these capabilities as a function of training compute, data type, and model size.

**2.2. Related Works**

- **Claim:** The paper builds upon prior work on instructional data and chain-of-thought (CoT) prompting.
    - **Citation:** (Vaswani et al., 2017; Ling et al., 2017; Cobbe et al., 2021; Nye et al., 2021; Roy & Roth, 2016; Reed & De Freitas, 2015; Chen et al., 2017; Cai et al., 2017; Nogueira et al., 2021; Razeghi et al., 2022; Wei et al., 2022c; Zhou et al., 2022a; Anil et al., 2022; Zhou et al., 2022b)
    - **Explanation:** This citation lists several papers that explore the use of instructional data and CoT prompting to improve model performance on reasoning tasks, including arithmetic.
- **Claim:** The paper focuses on decoder-only models, which are well-suited for text generation and are widely used in LLMs.
    - **Citation:** (Brown et al., 2020; Touvron et al., 2023; MosaicML, 2023; Kim et al., 2021; Wang et al., 2021; Qian et al., 2022; Lightman et al., 2023; Uesato et al., 2022; Wallace et al., 2019; Yun et al., 2019; Pérez et al., 2021; Wei et al., 2022a; Giannou et al., 2023; Ontanón et al., 2021; Shaw et al., 2018; Charton, 2022, 2021; Hanna et al., 2023)
    - **Explanation:** This citation highlights the extensive research on both decoder-only and encoder-decoder models for learning arithmetic.
- **Claim:** The paper acknowledges the research on recurrent neural networks (RNNs) for learning arithmetic.
    - **Citation:** (Bowman, 2013; Bowman et al., 2014; Zaremba et al., 2014; Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Dehghani et al., 2018)
    - **Explanation:** This citation highlights the use of RNNs for learning arithmetic and their ability to execute simple programs.
- **Claim:** The paper acknowledges the growing interest in Data-Centric AI, which emphasizes techniques to improve datasets for better performance.
    - **Citation:** (Motamedi et al., 2021; Hajij et al., 2021; Gadre et al., 2023; Rajani et al., 2019; Talmor et al., 2020; Zelikman et al., 2022; Huang et al., 2022)
    - **Explanation:** This citation highlights the importance of data quality and construction for improving model performance on reasoning tasks.

**2.3. Preliminaries and Experimental Setup**

- **Claim:** The paper uses NanoGPT (Karpathy, 2022) as the primary model for its experiments due to its lightweight implementation and feasibility for training from scratch.
    - **Citation:** (Karpathy, 2022)
    - **Explanation:** This citation introduces NanoGPT, a lightweight implementation of the GPT family of models, which is used for the experiments due to its ease of training.
- **Claim:** The authors train NanoGPT from random initialization using the conventional next-token prediction objective.
    - **Citation:** (Brown et al., 2020; Touvron et al., 2023; MosaicML, 2023)
    - **Explanation:** This citation highlights the use of the next-token prediction objective for training language models.
- **Claim:** The authors extend their experiments to GPT-2 and GPT-3 (davinci) to investigate the effect of scale and pretraining.
    - **Citation:** (davinci)
    - **Explanation:** This citation introduces GPT-2 and GPT-3 (davinci), which are used to explore the impact of model scale and pretraining on arithmetic performance.

**2.4. Learning Addition in Small Models**

- **Claim:** The authors demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy.
    - **Citation:** (Liu et al., 2023)
    - **Explanation:** This citation introduces the concept of "attention glitches," which can lead to decreased accuracy when the model is forced to learn a more complex function than necessary.
- **Claim:** The authors propose a reverse format for training data, where the output is generated in increasing order of significance (LSB first), which simplifies the learning function.
    - **Citation:** (Sutskever et al., 2014)
    - **Explanation:** This citation highlights the observation that reversing the input sequence can improve performance on text-based tasks, which is related to the authors' findings on the reverse format for addition.

**2.5. Connection to Low-Rank Matrix Completion**

- **Claim:** The authors demonstrate that learning an addition map on n digits from random samples can be considered as completing a rank-2 matrix.
    - **Citation:** (Király et al., 2015; Recht, 2011)
    - **Explanation:** This citation introduces the concept of low-rank matrix completion (LRMC) and provides theoretical justification for the observed phase transitions in learning addition.
- **Claim:** The authors show that NanoGPT exhibits capabilities that surpass traditional LRMC algorithms, demonstrating its ability to generalize to unseen numbers and digits.
    - **Citation:** (Király et al., 2015)
    - **Explanation:** This citation highlights the limitations of traditional LRMC algorithms, which struggle to generalize when entire rows or columns are missing.

**2.6. The Power of Chain-of-Thought: Incorporating Intermediate Steps in Training Data**

- **Claim:** The authors explore the use of chain-of-thought (CoT) style data, which includes step-by-step operations and intermediate results, to further improve learning efficiency and accuracy.
    - **Citation:** (Ling et al., 2017; Nye et al., 2021; Wei et al., 2022c; Zhou et al., 2022a; Anil et al., 2022; Zhou et al., 2022b)
    - **Explanation:** This citation highlights the use of CoT-style training data in prior work, which has shown significant improvements in sample complexity and accuracy.
- **Claim:** The authors demonstrate that incorporating detailed scratchpad data, which provides comprehensive information on how to execute each step in the addition process, significantly improves sample efficiency and accuracy.
    - **Citation:** (Nye et al., 2021; Zhou et al., 2022b)
    - **Explanation:** This citation highlights the use of scratchpad data in prior work for improving model performance on arithmetic tasks.
- **Claim:** The authors emphasize the importance of meticulously designing the intermediate steps in CoT data, demonstrating that different versions of the detailed scratchpad for subtraction can lead to significant differences in performance.
    - **Citation:** (Min et al., 2022)
    - **Explanation:** This citation highlights the importance of providing accurate intermediate steps in CoT data, as using random labels can lead to minimal degradation in performance.

**2.7. Extending to Longer Digit Addition**

- **Claim:** The authors demonstrate that their findings on data format and sampling extend to longer digit addition, observing similar phase transitions and the importance of scratchpad data.
    - **Citation:** (Nogueira et al., 2021; Yuan et al., 2023)
    - **Explanation:** This citation highlights the importance of large numbers of samples with small digits for learning arithmetic tasks.
- **Claim:** The authors investigate the generalization ability of transformer models, specifically focusing on their capacity to learn higher-digit additions based on their knowledge of lower-digit additions.
    - **Citation:** (Wei et al., 2022c)
    - **Explanation:** This citation highlights the use of chain-of-thought prompting for improving model performance on complex tasks.
- **Claim:** The authors observe that fine-tuning a model trained on k-digit addition using only k + 1-digit addition data can lead to catastrophic forgetting of previously learned additions.
    - **Citation:** (Brown et al., 2020)
    - **Explanation:** This citation highlights the phenomenon of catastrophic forgetting, which can occur when models are fine-tuned on new tasks.
- **Claim:** The authors demonstrate that using the detailed scratchpad format during fine-tuning can mitigate catastrophic forgetting and improve sample efficiency for learning higher-digit additions.
    - **Citation:** (Zhou et al., 2022b)
    - **Explanation:** This citation highlights the use of detailed scratchpad data for improving model performance on arithmetic tasks.

**2.8. Teaching Arithmetic Operations Beyond Addition**

- **Claim:** The authors extend their analysis to other arithmetic operations, including subtraction, multiplication, sine, and square root, demonstrating the broader applicability of their findings on data format and sampling.
    - **Citation:** (Bowman, 2013; Bowman et al., 2014; Zaremba et al., 2014; Zaremba & Sutskever, 2014; Kaiser & Sutskever, 2015; Dehghani et al., 2018)
    - **Explanation:** This citation highlights the use of RNNs for learning arithmetic and their ability to execute simple programs.
- **Claim:** The authors demonstrate that the detailed scratchpad format significantly improves performance for subtraction and multiplication tasks, but exhibits reduced efficiency for sine and square root functions.
    - **Citation:** (Min et al., 2022)
    - **Explanation:** This citation highlights the importance of providing accurate intermediate steps in CoT data, as using random labels can lead to minimal degradation in performance.

**2.9. Mixing Shakespeare with Arithmetic Data**

- **Claim:** The authors investigate the performance of models trained on a combination of arithmetic and text data, demonstrating that few-shot prompting with arithmetic exemplars generally improves performance, while text prompting shows performance similar to zero-shot.
    - **Citation:** (Wei et al., 2022c; Min et al., 2022)
    - **Explanation:** This citation highlights the use of chain-of-thought prompting for improving model performance on complex tasks.

**2.10. Fine-tuning, Scaling, and Pretraining in Larger Models**

- **Claim:** The authors compare the performance of NanoGPT and GPT-2 models trained from scratch, demonstrating that larger models exhibit improved performance, especially in the zero-shot setting.
    - **Citation:** (Brown et al., 2020; Touvron et al., 2023; MosaicML, 2023)
    - **Explanation:** This citation highlights the use of the next-token prediction objective for training language models.
- **Claim:** The authors investigate the impact of tokenization methods and model pretraining in GPT-2 models, demonstrating that pretrained models and consistent tokenization of numbers are crucial for arithmetic tasks.
    - **Citation:** (Wallace et al., 2019)
    - **Explanation:** This citation highlights the importance of character-level tokenization for improving numeracy capabilities in language models.
- **Claim:** The authors extend their experiments to GPT-3 models, demonstrating that fine-tuning pretrained GPT-3 models significantly improves performance compared to training from scratch.
    - **Citation:** (Brown et al., 2020; Touvron et al., 2023; MosaicML, 2023)
    - **Explanation:** This citation highlights the use of the next-token prediction objective for training language models.

**2.11. Token Efficiency Across Data Formats**

- **Claim:** The authors conduct a cost analysis based on the number of unique tokens encountered during training, demonstrating that the reverse format is the most token-efficient approach, while the detailed scratchpad method requires considerably more tokens.
    - **Citation:** (Pope et al., 2023)
    - **Explanation:** This citation highlights the importance of considering the cost of training and inference when evaluating different data formats.

**2.12. Length Generalization**

- **Claim:** The authors demonstrate that the model is unable to generalize to unseen digit lengths, highlighting the challenge of length generalization in arithmetic tasks.
    - **Citation:** (Shaw et al., 2018; Sun et al., 2022; Anil et al., 2022; Nye et al., 2021)
    - **Explanation:** This citation highlights the challenges of length generalization in prior work and suggests that models can only perform out-of-distribution tasks by combining fine-tuning, prompting, and scratchpad techniques.

**2.13. Limitations**

- **Claim:** The authors acknowledge the limitations of their experiments, including the smaller scale of their models and the focus on character-level tokenization.
    - **Citation:** (Nye et al., 2021)
    - **Explanation:** This citation highlights the observation that length generalization was observed only for models with more than 108 parameters.

**2.14. Conclusion**

- **Claim:** The authors conclude that high-quality, instructive data is crucial for the emergence of arithmetic capabilities in transformers.
    - **Citation:** (Wei et al., 2022c; Min et al., 2022)
    - **Explanation:** This citation highlights the use of chain-of-thought prompting for improving model performance on complex tasks.

### 3. Key Insights and Supporting Literature

- **Key Insight:** Data format and sampling play a crucial role in learning arithmetic.
    - **Supporting Citations:** (Liu et al., 2023; Sutskever et al., 2014; Király et al., 2015; Recht, 2011; Nye et al., 2021; Zhou et al., 2022b; Nogueira et al., 2021; Yuan et al., 2023)
    - **Explanation:** These citations highlight the importance of data quality and construction for improving model performance on arithmetic tasks.
- **Key Insight:** Chain-of-thought (CoT) style data, which includes step-by-step operations and intermediate results, significantly improves learning efficiency and accuracy.
    - **Supporting Citations:** (Ling et al., 2017; Nye et al., 2021; Wei et al., 2022c; Zhou et al., 2022a; Anil et al., 2022; Zhou et al., 2022b; Min et al., 2022)
    - **Explanation:** These citations highlight the use of CoT-style training data in prior work, which has shown significant improvements in sample complexity and accuracy.
- **Key Insight:** Length generalization is a challenging task for arithmetic tasks.
    - **Supporting Citations:** (Shaw et al., 2018; Sun et al., 2022; Anil et al., 2022; Nye et al., 2021)
    - **Explanation:** This citation highlights the challenges of length generalization in prior work and suggests that models can only perform out-of-distribution tasks by combining fine-tuning, prompting, and scratchpad techniques.

### 4. Experimental Methodology and Its Foundations

The paper uses NanoGPT (Karpathy, 2022) as the primary model for its experiments, training it from scratch using the conventional next-token prediction objective. The authors explore different data formatting techniques, including plain, reverse, simplified scratchpad, and detailed scratchpad, which are largely adopted from the literature on chain-of-thought (CoT) training (Nye et al., 2021; Zhou et al., 2022b). The authors also investigate the impact of data sampling, specifically balancing the number of digits and carry-ons in the training data. The paper extends its experiments to GPT-2 and GPT-3 (davinci) to investigate the effect of scale and pretraining.

### 5. Results in Context

- **Result:** The authors demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy.
    - **Comparison with Existing Literature:** (Liu et al., 2023)
    - **Confirmation/Contradiction/Extension:** The authors' findings confirm the concept of "attention glitches" introduced by Liu et al. (2023).
- **Result:** The authors demonstrate that the reverse format, where the output is generated in increasing order of significance (LSB first), significantly improves performance and sample efficiency.
    - **Comparison with Existing Literature:** (Sutskever et al., 2014)
    - **Confirmation/Contradiction/Extension:** The authors' findings extend the observation that reversing the input sequence can improve performance on text-based tasks, as observed by Sutskever et al. (2014), to the domain of arithmetic.
- **Result:** The authors demonstrate that incorporating detailed scratchpad data, which provides comprehensive information on how to execute each step in the addition process, significantly improves sample efficiency and accuracy.
    - **Comparison with Existing Literature:** (Nye et al., 2021; Zhou et al., 2022b)
    - **Confirmation/Contradiction/Extension:** The authors' findings confirm the use of scratchpad data in prior work for improving model performance on arithmetic tasks.
- **Result:** The authors demonstrate that the model is unable to generalize to unseen digit lengths, highlighting the challenge of length generalization in arithmetic tasks.
    - **Comparison with Existing Literature:** (Shaw et al., 2018; Sun et al., 2022; Anil et al., 2022; Nye et al., 2021)
    - **Confirmation/Contradiction/Extension:** The authors' findings confirm the challenges of length generalization in prior work and suggest that models can only perform out-of-distribution tasks by combining fine-tuning, prompting, and scratchpad techniques.

### 6. Discussion and Related Work

The authors situate their work within the existing literature on instructional data and chain-of-thought (CoT) prompting, highlighting the novelty of their focus on randomly initialized models and their extensive ablation studies on various sampling/data formatting and model scale settings. They also emphasize the theoretical justifications for their findings, particularly the connection between learning addition and low-rank matrix completion.

### 7. Future Work and Open Questions

The authors suggest several areas for future research, including:

- Exploring the use of more complex arithmetic operations and investigating the impact of data formatting on these tasks.
- Investigating the potential for length generalization in arithmetic tasks.
- Exploring the use of different model architectures and tokenization methods for learning arithmetic.
- Investigating the interplay between arithmetic and text data during training and the impact of different prompting strategies.

### 8. Critical Analysis of Citation Usage

The authors effectively use citations to support their arguments and findings, providing a comprehensive overview of the relevant literature. They cite a wide range of papers, including both foundational works in the field of deep learning and more recent research on LLMs and arithmetic capabilities. The authors also provide clear explanations for why each citation is relevant to their argument.

However, there are a few areas where additional citations might have been beneficial:

- The authors could have provided more citations to support their claims about the limitations of traditional LRMC algorithms and the enhanced capabilities of transformer models.
- The authors could have provided more citations to support their claims about the impact of model scale and pretraining on arithmetic performance.

Overall, the authors demonstrate a strong understanding of the relevant literature and effectively use citations to support their arguments and findings.

### 9. Final Summary

The paper makes a significant contribution to the field by providing a comprehensive analysis of the factors that contribute to the emergence of arithmetic capabilities in small transformer models. The authors demonstrate the importance of data format, sampling, and the use of chain-of-thought (CoT) style data for improving learning efficiency and accuracy. They also highlight the challenges of length generalization and the need for further research in this area. The paper effectively integrates existing literature to support its claims and findings, providing a valuable resource for researchers interested in the development of LLMs with arithmetic capabilities.

The most influential or frequently cited works used throughout the paper include:

- (Brown et al., 2020)
- (Wei et al., 2022c)
- (Nye et al., 2021)
- (Zhou et al., 2022b)
- (Király et al., 2015)

These works provide a strong foundation for the paper's arguments and findings, highlighting the importance of data quality, instructional data, and chain-of-thought prompting for improving model performance on reasoning tasks.
