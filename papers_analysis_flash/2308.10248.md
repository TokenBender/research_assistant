## Analysis of "Activation Addition: Steering Language Models Without Optimization"

**1. Introduction:**

- **Title:** Activation Addition: Steering Language Models Without Optimization
- **Authors:** Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, Monte MacDiarmid
- **Publication Date:** 2024 (preprint)
- **Objective:** The paper aims to develop a method for controlling the behavior of large language models (LLMs) at inference time without requiring optimization or labeled data.
- **Number of References:** 68

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:** The authors highlight the limitations of existing methods for controlling LLMs, including supervised finetuning, reinforcement learning from human feedback (RLHF), prompt engineering, and guided decoding. They introduce the concept of "activation engineering" as a novel approach to control LLMs by modifying their activations during the forward pass.
- **Significant Citations:**
    - **Claim:** Existing methods for controlling LLMs are computationally expensive and only partially effective.
        - **Citation:** Peters et al. (2018), Devlin et al. (2018), Radford et al. (2019), Brown et al. (2020), Ziegler et al. (2019), Radford et al. (2019), Ghazvininejad et al. (2017), Gu et al. (2017).
        - **Relevance:** This citation establishes the context for the paper's research by highlighting the limitations of existing approaches and motivating the need for a new method.
    - **Claim:** The authors introduce "activation engineering" as a novel approach to control LLMs.
        - **Citation:** Subramani et al. (2022), Hernandez et al. (2023).
        - **Relevance:** This citation introduces the concept of activation engineering and positions the paper's work within this emerging research area.

**2.2 Related Work:**

- **Key Points:** The authors discuss related work in latent space arithmetic, LLM steering, and activation engineering. They highlight the differences between their approach and previous methods, emphasizing the use of forward passes, operation on activation space, and natural-language user interface.
- **Significant Citations:**
    - **Claim:** Latent space arithmetic has been used to steer image generation and capture semantic reasoning in word embeddings.
        - **Citation:** Larsen et al. (2016), White (2016), Mikolov et al. (2013b,a).
        - **Relevance:** This citation provides a historical context for the paper's approach by showing that manipulating latent spaces has been explored in other domains.
    - **Claim:** Various methods have been proposed to steer LLMs, including fine-tuning, RLHF, prompt engineering, and guided decoding.
        - **Citation:** Ranzato et al. (2016), Ziegler et al. (2019), Dathathri et al. (2020), Meng et al. (2023), Ilharco et al. (2023), Gu et al. (2017), Grover et al. (2019), Zhang et al. (2022a), Jin et al. (2022), Shin et al. (2020), Zhou et al. (2022), Lester et al. (2021), Li and Liang (2021), Khashabi et al. (2022), Subramani et al. (2022), Hernandez et al. (2023), Li et al. (2023b).
        - **Relevance:** This citation provides a comprehensive overview of existing LLM steering methods and highlights the differences between the authors' approach and previous work.
    - **Claim:** Activation engineering involves creating vectors of activations that cause desired changes in output text when added to the forward passes of a frozen LLM.
        - **Citation:** Dathathri et al. (2020), Zou et al. (2023).
        - **Relevance:** This citation defines the concept of activation engineering and provides a framework for understanding the authors' approach.

**2.3 Methods:**

- **Key Points:** The authors describe their proposed method, Activation Addition (ActAdd), which involves adding a "steering vector" to the residual stream of a Transformer layer at inference time. The steering vector is computed by taking the difference in activations between two prompts that represent the desired property and its opposite.
- **Significant Citations:**
    - **Claim:** The authors use decoder-only Transformer neural networks trained on a large text corpus.
        - **Citation:** Vaswani et al. (2017), Liu et al. (2018).
        - **Relevance:** This citation establishes the foundation for the paper's methodology by describing the architecture of the LLMs used in their experiments.
    - **Claim:** The authors manipulate the residual stream values input to a specific layer.
        - **Citation:** Elhage et al. (2021).
        - **Relevance:** This citation explains the specific mechanism of the Transformer architecture that the authors target for their intervention.
    - **Claim:** The authors use a pair of natural-language prompts to specify the desired property and its opposite.
        - **Citation:** Kaci and Patel (2014).
        - **Relevance:** This citation provides a theoretical foundation for the authors' approach by drawing a connection to the concept of "comparative preference statements" used in formal logics.

**2.4 Metrics:**

- **Key Points:** The authors describe the metrics used to evaluate the effectiveness of ActAdd, including perplexity ratio, shift in logprobs, inference time premium, relevance steering, and generation scoring.
- **Significant Citations:**
    - **Claim:** The authors use perplexity ratio to measure the change in perplexity under ActAdd for different wedding-word frequency bins.
        - **Citation:** Peterson et al. (2018), Strunk (2013).
        - **Relevance:** This citation explains the rationale for using perplexity ratio as a metric and provides details about the data source and pre-processing steps.
    - **Claim:** The authors use shift in logprobs to test whether the intervention is affecting relevant tokens or reducing perplexity in a spurious way.
        - **Citation:** Gnanadesikan and Wilk (1968).
        - **Relevance:** This citation explains the rationale for using shift in logprobs as a metric and provides details about the visualization technique used.
    - **Claim:** The authors use inference time premium to measure the percentage increase in time-to-complete one forward pass using ActAdd.
        - **Citation:** None.
        - **Relevance:** This citation is not explicitly provided in the paper, but the authors clearly define the metric and its purpose.
    - **Claim:** The authors use relevance steering to demonstrate general topic steering by evaluating model completions on whether they relate to the ActAdd topic.
        - **Citation:** None.
        - **Relevance:** This citation is not explicitly provided in the paper, but the authors clearly define the metric and its purpose.
    - **Claim:** The authors use generation scoring to show the effect of different injection layers and give a sense of the reliability of ActAdd by scoring the generations based on the number of related words and the fraction of completions with a related word.
        - **Citation:** None.
        - **Relevance:** This citation is not explicitly provided in the paper, but the authors clearly define the metric and its purpose.

**2.5 Results:**

- **Key Points:** The authors present results demonstrating the effectiveness of ActAdd in controlling sentiment, topic, and style. They also show that ActAdd preserves model performance, scales naturally with model size, and has a low computational overhead.
- **Significant Citations:**
    - **Claim:** ActAdd preserves model performance as measured by P@K on the ConceptNet benchmark.
        - **Citation:** Petroni et al. (2019).
        - **Relevance:** This citation provides a baseline for comparing the performance of ActAdd with other methods.
    - **Claim:** ActAdd achieves state-of-the-art results on sentiment control as measured by success rate on the Stanford IMDb dataset.
        - **Citation:** Maas et al. (2011), Hartmann et al. (2023b).
        - **Relevance:** This citation provides a baseline for comparing the performance of ActAdd with other methods.
    - **Claim:** ActAdd achieves competitive results on toxicity reduction as measured by the Perspective API score on the RealToxicityPrompts dataset.
        - **Citation:** Gehman et al. (2020), Pei et al. (2023), Zhong et al. (2023), Gu et al. (2022).
        - **Relevance:** This citation provides a baseline for comparing the performance of ActAdd with other methods.

**2.6 Discussion:**

- **Key Points:** The authors discuss the limitations of ActAdd, including the need for parameter search and the lack of understanding of how large injection coefficients affect capabilities. They also highlight the advantages of ActAdd over fine-tuning and prompt engineering, emphasizing its efficiency, ease of implementation, and potential for preserving model interpretability.
- **Significant Citations:**
    - **Claim:** ActAdd requires parameter search, which makes it less user-friendly than prompt engineering.
        - **Citation:** None.
        - **Relevance:** This citation is not explicitly provided in the paper, but the authors clearly acknowledge this limitation.
    - **Claim:** ActAdd is more efficient than fine-tuning because it only requires forward passes.
        - **Citation:** Fuller (2022).
        - **Relevance:** This citation provides a context for understanding the computational efficiency of ActAdd compared to fine-tuning.
    - **Claim:** ActAdd is easier to implement than fine-tuning because it does not require labeled data or backward passes.
        - **Citation:** None.
        - **Relevance:** This citation is not explicitly provided in the paper, but the authors clearly acknowledge this advantage.
    - **Claim:** ActAdd has the potential to preserve model interpretability because it does not modify weights.
        - **Citation:** Elhage et al. (2022), Burns et al. (2022), Moschella et al. (2023), Li et al. (2023a), Nanda (2023), Li et al. (2023b), Alain and Bengio (2018), Park et al. (2023).
        - **Relevance:** This citation provides a theoretical foundation for understanding the potential of ActAdd to preserve model interpretability.

**2.7 Conclusion:**

- **Key Points:** The authors conclude that ActAdd is a promising method for controlling LLMs at inference time. They highlight its advantages over existing methods, including its efficiency, ease of implementation, and potential for preserving model interpretability.
- **Significant Citations:**
    - **Claim:** ActAdd complements existing methods for controlling LLMs.
        - **Citation:** None.
        - **Relevance:** This citation is not explicitly provided in the paper, but the authors clearly state this conclusion.
    - **Claim:** ActAdd scales well with model size.
        - **Citation:** None.
        - **Relevance:** This citation is not explicitly provided in the paper, but the authors clearly state this conclusion.
    - **Claim:** ActAdd provides evidence about the computational structure of LLM representations.
        - **Citation:** None.
        - **Relevance:** This citation is not explicitly provided in the paper, but the authors clearly state this conclusion.

**3. Key Insights and Supporting Literature:**

- **Key Insight:** ActAdd is a novel method for controlling LLMs at inference time without requiring optimization or labeled data.
    - **Supporting Citations:** Subramani et al. (2022), Hernandez et al. (2023).
    - **Contribution:** This insight highlights the novelty of the paper's approach and its potential to address the limitations of existing methods.
- **Key Insight:** ActAdd can be used to control high-level properties of LLM output, such as sentiment, topic, and style.
    - **Supporting Citations:** None.
    - **Contribution:** This insight demonstrates the versatility of ActAdd and its potential for a wide range of applications.
- **Key Insight:** ActAdd preserves model performance on off-target tasks.
    - **Supporting Citations:** Petroni et al. (2019).
    - **Contribution:** This insight highlights the safety and reliability of ActAdd, suggesting that it can be used to control LLMs without compromising their general capabilities.
- **Key Insight:** ActAdd scales naturally with model size and has a low computational overhead.
    - **Supporting Citations:** None.
    - **Contribution:** This insight highlights the practical advantages of ActAdd, suggesting that it can be used with large LLMs without significant computational cost.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:** The authors conducted experiments on a range of LLMs, including GPT-2-XL, OPT, LLaMA-3, and GPT-J. They evaluated the effectiveness of ActAdd using various metrics, including perplexity ratio, shift in logprobs, inference time premium, relevance steering, and generation scoring.
- **Foundations:** The authors used the Transformer architecture as the foundation for their methodology.
    - **Citation:** Vaswani et al. (2017), Liu et al. (2018).
- **Novel Aspects:** The authors introduced the concept of "activation engineering" and developed the ActAdd method, which is a novel approach to controlling LLMs at inference time.
    - **Justification:** The authors do not explicitly cite any works to justify their novel approach, but they clearly demonstrate its effectiveness through their experiments.

**5. Results in Context:**

- **Main Results:** The authors demonstrated that ActAdd is effective in controlling sentiment, topic, and style. They also showed that ActAdd preserves model performance, scales naturally with model size, and has a low computational overhead.
- **Comparison with Existing Literature:** The authors compared their results with existing methods for controlling LLMs, including fine-tuning, RLHF, prompt engineering, and guided decoding.
    - **Citation:** Ranzato et al. (2016), Ziegler et al. (2019), Dathathri et al. (2020), Meng et al. (2023), Ilharco et al. (2023), Gu et al. (2017), Grover et al. (2019), Zhang et al. (2022a), Jin et al. (2022), Shin et al. (2020), Zhou et al. (2022), Lester et al. (2021), Li and Liang (2021), Khashabi et al. (2022), Subramani et al. (2022), Hernandez et al. (2023), Li et al. (2023b).
    - **Confirmation/Contradiction/Extension:** The authors' results demonstrate that ActAdd is a competitive alternative to existing methods, offering advantages in terms of efficiency, ease of implementation, and potential for preserving model interpretability.

**6. Discussion and Related Work:**

- **Situating Work within Literature:** The authors situate their work within the existing literature by discussing the limitations of previous methods for controlling LLMs and highlighting the advantages of their approach.
- **Key Papers Cited:**
    - **Citation:** Subramani et al. (2022), Hernandez et al. (2023), Dathathri et al. (2020), Zou et al. (2023), Lyu et al. (2024), Meng et al. (2023), Ilharco et al. (2023), Zhang et al. (2022a), Fuller (2022), Bloom and Nanda (2022), Kaci and Patel (2014), Elhage et al. (2021), Vaswani et al. (2017), Liu et al. (2018), Peterson et al. (2018), Strunk (2013), Gnanadesikan and Wilk (1968), Petroni et al. (2019), Maas et al. (2011), Gehman et al. (2020), Pei et al. (2023), Zhong et al. (2023), Gu et al. (2022), Hartmann et al. (2023b), Wu et al. (2023), Zhang et al. (2024), Reimers and Gurevych (2019), Olah (2023), Askell et al. (2021), Burns et al. (2022), Moschella et al. (2023), Li et al. (2023a), Nanda (2023), Li et al. (2023b), Alain and Bengio (2018), Park et al. (2023), Sloman (2002).
    - **Relevance:** The authors use these citations to highlight the novelty of their approach, its advantages over existing methods, and its potential for future research.

**7. Future Work and Open Questions:**

- **Areas for Further Research:** The authors suggest several areas for further research, including:
    - Developing more general steering methods.
    - Investigating the effects of large injection coefficients on model capabilities.
    - Exploring the use of ActAdd for reasoning tasks.
    - Understanding the relationship between activation addition and prompt engineering.
    - Investigating the computational structure of LLM representations.
- **Citations:** The authors do not explicitly cite any works to support these suggestions for future work, but they clearly identify these areas as promising directions for future research.

**8. Critical Analysis of Citation Usage:**

- **Effectiveness:** The authors effectively use citations to support their arguments and findings. They provide a comprehensive overview of related work and clearly distinguish their approach from previous methods.
- **Areas for Improvement:** The authors could have provided more citations to support their claims about the advantages of ActAdd over fine-tuning and prompt engineering. They could also have provided more citations to support their suggestions for future work.
- **Potential Biases:** The authors primarily cite works from the field of deep learning and natural language processing. They could have included more citations from other fields, such as cognitive science and philosophy, to provide a broader perspective on their work.

**9. Final Summary:**

- **Contribution:** The paper presents a novel method for controlling LLMs at inference time without requiring optimization or labeled data. ActAdd is shown to be effective in controlling sentiment, topic, and style, while preserving model performance and scaling naturally with model size.
- **Influential Works:** The paper draws heavily on the work of Subramani et al. (2022), Hernandez et al. (2023), Dathathri et al. (2020), and Zou et al. (2023), which introduce the concept of activation engineering and provide a framework for understanding the authors' approach.
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. The authors provide a comprehensive overview of related work and clearly distinguish their approach from previous methods. They also identify several areas for future research, suggesting that their work has the potential to contribute significantly to the field of LLM control.

Overall, the paper makes a significant contribution to the field of LLM control by introducing a novel and effective method for steering LLMs at inference time. The authors provide a strong theoretical foundation for their approach and demonstrate its effectiveness through a series of well-designed experiments. The paper is well-written and clearly structured, making it a valuable resource for researchers interested in LLM control.
