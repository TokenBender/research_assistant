## Analysis of "Baichuan 2: Open Large-scale Language Models"

**1. Introduction:**

- **Title:** Baichuan 2: Open Large-scale Language Models
- **Authors:** Aiyuan Yang, Bin Xiao, Bingning Wang, et al. (37 authors)
- **Publication Date:** 20 Sep 2023 (v2)
- **Objective:** The paper introduces Baichuan 2, a series of open-source, large-scale multilingual language models, aiming to address the limitations of existing open-source LLMs primarily focused on English.
- **Total References:** 74

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:**
    - LLMs have shown impressive performance on various natural language tasks with few-shot learning.
    - Most powerful LLMs are closed-source or limited to English.
    - Baichuan 2 is a series of open-source multilingual LLMs trained from scratch on 2.6 trillion tokens.
    - Baichuan 2 outperforms other open-source models of similar size on public benchmarks.
    - Baichuan 2 excels in vertical domains like medicine and law.
    - The authors will release all pre-training model checkpoints to benefit the research community.

- **Significant Citations:**
    - **Claim:** "LLMs have demonstrated remarkable performance on a variety of natural language tasks based on just a few examples of natural language instructions, reducing the need for extensive feature engineering."
        - **Citation:** Peters, M. E., et al. "Deep contextualized word representations." *arXiv preprint arXiv:1802.05365* (2018).
        - **Relevance:** This citation highlights the advancement of LLMs in few-shot learning, setting the context for the paper's focus on open-source models.
    - **Claim:** "However, most powerful LLMs are closed-source or limited in their capability for languages other than English."
        - **Citation:** OpenAI. "ChatGPT." *Blog post openai.com/blog/chatgpt* (2022).
        - **Relevance:** This citation points out the limitations of existing powerful LLMs, motivating the development of Baichuan 2.
    - **Claim:** "In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens."
        - **Citation:** Touvron, H., et al. "LLaMA: Open and efficient foundation language models." *arXiv preprint arXiv:2302.13971* (2023a).
        - **Relevance:** This citation introduces the concept of open-source LLMs, which Baichuan 2 aims to contribute to.

**2.2 Pre-training:**

- **Key Points:**
    - Baichuan 2 models are trained on 2.6 trillion tokens, more than double the size of Baichuan 1.
    - Baichuan 2 achieves significant improvements over Baichuan 1 on various benchmarks.
    - Baichuan 2 is optimized for math and code problems.
    - Baichuan 2 demonstrates strong performance in medical and legal domains.
    - The authors release two chat models, Baichuan 2-7B-Chat and Baichuan 2-13B-Chat, optimized for dialogue and context understanding.
    - The authors will release checkpoints of Baichuan 2 at various stages of training to facilitate research.

- **Significant Citations:**
    - **Claim:** "However, most open-source large language models have focused primarily on English."
        - **Citation:** Touvron, H., et al. "LLaMA: Open and efficient foundation language models." *arXiv preprint arXiv:2302.13971* (2023a).
        - **Relevance:** This citation highlights the lack of multilingual support in existing open-source LLMs, justifying the development of Baichuan 2.
    - **Claim:** "Baichuan 2 achieves significant improvements over Baichuan 1 on general benchmarks like MMLU, CMMLU, and C-Eval."
        - **Citation:** Hendrycks, D., et al. "Measuring massive multitask language understanding." *arXiv preprint arXiv:2103.03874* (2021a).
        - **Relevance:** This citation introduces the MMLU benchmark, which is used to evaluate Baichuan 2's performance.
    - **Claim:** "Baichuan 2 is optimized to improve performance on math and code problems."
        - **Citation:** Cobbe, K., et al. "Training verifiers to solve math word problems." *arXiv preprint arXiv:2110.14168* (2021).
        - **Relevance:** This citation introduces the GSM8K benchmark, which is used to evaluate Baichuan 2's performance on math problems.

**2.3 Pre-training Data:**

- **Key Points:**
    - The authors aim for comprehensive data scalability and representativeness.
    - Data is sourced from diverse sources, including general internet webpages, books, research papers, and codebases.
    - The authors focus on data frequency and quality, using clustering and deduplication techniques.

- **Significant Citations:**
    - **Claim:** "The objective is to pursue comprehensive data scalability and representativeness."
        - **Citation:** Touvron, H., et al. "LLaMA: Open and efficient foundation language models." *arXiv preprint arXiv:2302.13971* (2023a).
        - **Relevance:** This citation highlights the importance of data quality and quantity in training LLMs, which Baichuan 2 emphasizes.

**2.4 Architecture:**

- **Key Points:**
    - Baichuan 2's architecture is based on the Transformer.
    - The authors made several modifications to the architecture, including changes to the tokenizer, positional embeddings, activations, and normalizations.

- **Significant Citations:**
    - **Claim:** "The model architecture of Baichuan 2 is based on the prevailing Transformer."
        - **Citation:** Vaswani, A., et al. "Attention is all you need." *Advances in Neural Information Processing Systems* (2017).
        - **Relevance:** This citation introduces the Transformer architecture, which is the foundation for Baichuan 2.

**2.5 Tokenizer:**

- **Key Points:**
    - The authors expanded the vocabulary size from 64,000 in Baichuan 1 to 125,696.
    - The tokenizer uses byte-pair encoding (BPE) from SentencePiece.

- **Significant Citations:**
    - **Claim:** "We use byte-pair encoding (BPE) from SentencePiece to tokenize the data."
        - **Citation:** Kudo, T., and Richardson, J. "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing." *arXiv preprint arXiv:1808.06226* (2018).
        - **Relevance:** This citation introduces the SentencePiece tokenizer, which is used in Baichuan 2.

**2.6 Positional Embeddings:**

- **Key Points:**
    - The authors adopt Rotary Positional Embedding (RoPE) for Baichuan 2-7B and ALiBi for Baichuan 2-13B.
    - The choice of positional embedding did not significantly impact model performance.

- **Significant Citations:**
    - **Claim:** "Building on Baichuan 1, we adopt Rotary Positional Embedding (RoPE) for Baichuan 2-7B and ALiBi for Baichuan 2-13B."
        - **Citation:** Su, J., et al. "Roformer: Enhanced transformer with rotary position embedding." *arXiv preprint arXiv:2104.09864* (2021).
        - **Relevance:** This citation introduces the RoPE positional embedding technique, which is used in Baichuan 2.
    - **Claim:** "ALiBi is a more recent positional encoding technique that has shown improved extrapolation performance."
        - **Citation:** Press, O., et al. "Train short, test long: Attention with linear biases enables input length extrapolation." *arXiv preprint arXiv:2108.12409* (2021).
        - **Relevance:** This citation introduces the ALiBi positional embedding technique, which is used in Baichuan 2.

**2.7 Activations and Normalizations:**

- **Key Points:**
    - The authors use SwiGLU activation function.
    - The authors adopt memory efficient attention implemented by xFormers.
    - The authors apply Layer Normalization to the input of the Transformer block.
    - The authors use RMSNorm implementation introduced by Zhang and Sennrich (2019).

- **Significant Citations:**
    - **Claim:** "We use SwiGLU (Shazeer, 2020) activation function, a switch-activated variant of GLU (Dauphin et al., 2017) which shows improved results."
        - **Citation:** Shazeer, N. "Glu variants improve transformer." *arXiv preprint arXiv:2002.05202* (2020).
        - **Relevance:** This citation introduces the SwiGLU activation function, which is used in Baichuan 2.
    - **Claim:** "We adopt the memory efficient attention (Rabe and Staats, 2021) implemented by xFormers."
        - **Citation:** Rabe, M. N., and Staats, C. "Self-attention does not need O(nÂ²) memory." *arXiv preprint arXiv:2112.05682* (2021).
        - **Relevance:** This citation introduces the memory efficient attention technique implemented by xFormers, which is used in Baichuan 2.
    - **Claim:** "We apply Layer Normalization (Ba et al., 2016) to the input of the Transformer block which is more robust to the warm-up schedule (Xiong et al., 2020)."
        - **Citation:** Ba, J. L., et al. "Layer normalization." *Advances in Neural Information Processing Systems* (2016).
        - **Relevance:** This citation introduces the Layer Normalization technique, which is used in Baichuan 2.

**2.8 Optimizations:**

- **Key Points:**
    - The authors use AdamW optimizer for training.
    - The authors use BFloat16 mixed precision for training.
    - The authors use NormHead to stabilize training and improve model performance.
    - The authors use max-z loss to normalize logits and make inference more robust to hyper-parameters.

- **Significant Citations:**
    - **Claim:** "We use AdamW (Loshchilov and Hutter, 2017) optimizer for training."
        - **Citation:** Loshchilov, I., and Hutter, F. "Decoupled weight decay regularization." *arXiv preprint arXiv:1711.05101* (2017).
        - **Relevance:** This citation introduces the AdamW optimizer, which is used in Baichuan 2.
    - **Claim:** "The whole models are trained using BFloat16 mixed precision."
        - **Citation:** Duderstadt, B., et al. "Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo." *GitHub* (2023).
        - **Relevance:** This citation mentions the use of BFloat16 mixed precision, which is also used in Baichuan 2.
    - **Claim:** "To stabilize training and improve the model performance, we normalize the output embeddings (which are also referred as 'head')."
        - **Citation:** Bai, Y., et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." *arXiv preprint arXiv:2204.05862* (2022a).
        - **Relevance:** This citation introduces the concept of NormHead, which is used in Baichuan 2.

**2.9 Scaling Laws:**

- **Key Points:**
    - The authors use scaling laws to predict the performance of larger models based on the performance of smaller models.
    - The authors trained models ranging from 10M to 3B parameters and fitted a scaling law to predict the performance of Baichuan 2-7B and Baichuan 2-13B.

- **Significant Citations:**
    - **Claim:** "Neural scaling laws, where the error decreases as a power function of training set size, model size, or both, have enabled an assuring performance when training became more and more expensive in deep learning and large language models."
        - **Citation:** Kaplan, J., et al. "Scaling laws for neural language models." *arXiv preprint arXiv:2001.08361* (2020).
        - **Relevance:** This citation introduces the concept of scaling laws, which are used in Baichuan 2.

**2.10 Infrastructure:**

- **Key Points:**
    - The authors developed a co-design approach for an elastic training framework and a smart cluster scheduling policy.
    - The framework integrates tensor parallelism and ZeRO-powered data parallelism.
    - The authors employ tensor-splitting technique to reduce peak memory consumption.
    - The authors implement mixed-precision training and optimize communication efficiency using topology-aware distributed training and hybrid and hierarchical partition for ZeRO.

- **Significant Citations:**
    - **Claim:** "To meet the requirement of the machine-level elasticity, our training framework integrates tensor parallelism (Narayanan et al., 2021) and ZeRO-powered data parallelism (Rajbhandari et al., 2020), where we set tensor parallelism inside each machine and employ ZeRO shared data parallelism for elastic scaling across machines."
        - **Citation:** Narayanan, D., et al. "Efficient large-scale language model training on gpu clusters using megatron-lm." *Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis* (2021).
        - **Relevance:** This citation introduces the concept of tensor parallelism, which is used in Baichuan 2.
    - **Claim:** "In addition, we employ a tensor-splitting technique (Nie et al., 2022) where we split certain calculations to reduce peak memory consumption."
        - **Citation:** Nie, X., et al. "Tsplit: Fine-grained gpu memory management for efficient dnn training via tensor splitting." *2022 IEEE 38th International Conference on Data Engineering (ICDE)* (2022).
        - **Relevance:** This citation introduces the tensor-splitting technique, which is used in Baichuan 2.

**3. Key Insights and Supporting Literature:**

- **Key Insight:** Baichuan 2 is a significant step towards open-source, large-scale multilingual LLMs, addressing the limitations of existing models primarily focused on English.
    - **Supporting Citations:**
        - Touvron, H., et al. "LLaMA: Open and efficient foundation language models." *arXiv preprint arXiv:2302.13971* (2023a).
        - Zhang, S., et al. "Opt: Open pre-trained transformer language models." *arXiv preprint arXiv:2205.01068* (2022).
        - Scao, T. L., et al. "Bloom: A 176b-parameter open-access multilingual language model." *arXiv preprint arXiv:2211.05100* (2022).
    - **Contribution:** This insight highlights the paper's contribution to the field of open-source LLMs, particularly in the context of multilingual capabilities.

- **Key Insight:** Baichuan 2 demonstrates strong performance on various benchmarks, including general benchmarks like MMLU, CMMLU, and C-Eval, as well as vertical domains like medicine and law.
    - **Supporting Citations:**
        - Hendrycks, D., et al. "Measuring massive multitask language understanding." *arXiv preprint arXiv:2103.03874* (2021a).
        - Li, H., et al. "Cmmlu: Measuring massive multitask language understanding in chinese." *arXiv preprint arXiv:2303.03057* (2023).
        - Huang, Y., et al. "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models." *arXiv preprint arXiv:2305.08322* (2023).
        - Jin, D., et al. "What disease does this patient have? a large-scale open domain question answering dataset from medical exams." *Applied Sciences* (2021).
        - Zhong, H., et al. "Jec-qa: A legal-domain question answering dataset." *Proceedings of AAAI* (2020).
    - **Contribution:** This insight showcases the effectiveness of Baichuan 2 in various tasks and domains, demonstrating its potential for real-world applications.

- **Key Insight:** The authors emphasize the importance of transparency and open-source research in the field of LLMs.
    - **Supporting Citations:**
        - Touvron, H., et al. "LLaMA: Open and efficient foundation language models." *arXiv preprint arXiv:2302.13971* (2023a).
        - Zhang, S., et al. "Opt: Open pre-trained transformer language models." *arXiv preprint arXiv:2205.01068* (2022).
        - Scao, T. L., et al. "Bloom: A 176b-parameter open-access multilingual language model." *arXiv preprint arXiv:2211.05100* (2022).
    - **Contribution:** This insight highlights the paper's commitment to open-source research, which is crucial for the advancement of the field.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:**
    - The authors trained Baichuan 2 models on 2.6 trillion tokens, using BFloat16 mixed precision and AdamW optimizer.
    - The authors implemented various techniques to improve training efficiency, including tensor parallelism, ZeRO-powered data parallelism, tensor-splitting, and mixed-precision training.
    - The authors evaluated Baichuan 2 models on various benchmarks, including MMLU, CMMLU, C-Eval, GSM8K, HumanEval, JEC-QA, MedQA, and MedMCQA.

- **Methodology Foundations:**
    - The authors used the Transformer architecture as the foundation for Baichuan 2.
    - The authors cited works on scaling laws, which guided their training process.
    - The authors cited works on various techniques used for training efficiency, including tensor parallelism, ZeRO, and tensor-splitting.

- **Novel Aspects:**
    - The authors introduced NormHead to stabilize training and improve model performance.
    - The authors used max-z loss to normalize logits and make inference more robust to hyper-parameters.
    - The authors developed a co-design approach for an elastic training framework and a smart cluster scheduling policy.
    - The authors proposed a hybrid and hierarchical partitioning scheme for ZeRO to address communication bottlenecks in large-scale clusters.

- **Citations for Novel Approaches:**
    - **NormHead:** Bai, Y., et al. "Training a helpful and harmless assistant with reinforcement learning from human feedback." *arXiv preprint arXiv:2204.05862* (2022a).
    - **Max-z Loss:** Chowdhery, A., et al. "PaLM: Scaling language modeling with pathways." *arXiv preprint arXiv:2204.02311* (2022).
    - **Elastic Training Framework:** Narayanan, D., et al. "Efficient large-scale language model training on gpu clusters using megatron-lm." *Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis* (2021).
    - **Hybrid and Hierarchical Partition for ZeRO:** Jiang, Y., et al. "Osdp: Optimal sharded data parallel for distributed deep learning." *arXiv preprint arXiv:2209.13258* (2023a).

**5. Results in Context:**

- **Main Results:**
    - Baichuan 2 outperforms other open-source models of similar size on various benchmarks, including MMLU, CMMLU, C-Eval, GSM8K, and HumanEval.
    - Baichuan 2 demonstrates strong performance in vertical domains like medicine and law.
    - Baichuan 2 exhibits significant improvements over Baichuan 1 on various benchmarks.
    - Baichuan 2's performance continues to improve even after training on 2.6 trillion tokens.

- **Comparison with Existing Literature:**
    - Baichuan 2 surpasses models like LLaMA 2-7B, MPT-7B, Falcon-7B, ChatGLM 2-6B, and Vicuna-13B on various benchmarks.
    - Baichuan 2-13B-Base outperforms models like XVERSE-13B and LLaMA 2-13B on various benchmarks.
    - Baichuan 2-7B-Base and Baichuan 2-13B-Base demonstrate significant improvements over Baichuan 1-7B and Baichuan 1-13B-Base on various benchmarks.

- **Confirmation, Contradiction, or Extension:**
    - Baichuan 2's results confirm the trend of scaling laws, where larger models trained on more data generally achieve better performance.
    - Baichuan 2's results extend the capabilities of open-source LLMs by demonstrating strong performance in multilingual tasks and vertical domains.

**6. Discussion and Related Work:**

- **Situating the Work:**
    - The authors situate their work within the context of the rapid development of large language models, highlighting the limitations of existing open-source models and the need for more transparency and open-source research.
    - The authors discuss the advancements in scaling laws, reinforcement learning from human feedback (RLHF), and other techniques for improving LLM performance.

- **Key Papers Cited:**
    - Touvron, H., et al. "LLaMA: Open and efficient foundation language models." *arXiv preprint arXiv:2302.13971* (2023a).
    - Zhang, S., et al. "Opt: Open pre-trained transformer language models." *arXiv preprint arXiv:2205.01068* (2022).
    - Scao, T. L., et al. "Bloom: A 176b-parameter open-access multilingual language model." *arXiv preprint arXiv:2211.05100* (2022).
    - Kaplan, J., et al. "Scaling laws for neural language models." *arXiv preprint arXiv:2001.08361* (2020).
    - Ouyang, L., et al. "Training language models to follow instructions with human feedback." *Advances in Neural Information Processing Systems* (2022).

- **Highlighting Novelty:**
    - The authors highlight the novelty of Baichuan 2 in its open-source nature, multilingual capabilities, and strong performance on various benchmarks.
    - The authors emphasize the importance of their contributions to the field of open-source LLMs, particularly in the context of multilingual support and vertical domain expertise.

**7. Future Work and Open Questions:**

- **Areas for Further Research:**
    - The authors suggest further research on improving the safety and alignment of Baichuan 2 models.
    - The authors encourage further exploration of the training dynamics of Baichuan 2, particularly in relation to scaling laws and the impact of different training stages on model performance.

- **Citations for Future Work:**
    - **Safety and Alignment:** Rafailov, R., et al. "Direct preference optimization: Your language model is secretly a reward model." *arXiv preprint arXiv:2305.18290* (2023).
    - **Training Dynamics:** Hoffmann, J., et al. "Training compute-optimal large language models." *arXiv preprint arXiv:2203.15556* (2022).

**8. Critical Analysis of Citation Usage:**

- **Effectiveness:** The authors effectively use citations to support their arguments and findings. They provide a comprehensive overview of the relevant literature, citing key works in the field of LLMs, scaling laws, and training techniques.

- **Areas for Improvement:**
    - The authors could have provided more citations for specific claims related to the performance of Baichuan 2 on various benchmarks.
    - The authors could have included more citations for works that address the ethical challenges of LLMs, particularly in the context of bias, toxicity, and misuse.

- **Potential Biases:**
    - The authors primarily cite works from major research institutions and companies like OpenAI, Google, Meta, and Anthropic.
    - The authors could have included more citations from academic researchers and non-profit organizations working on open-source LLMs.

**9. Final Summary:**

- **Contribution:** Baichuan 2 is a significant contribution to the field of open-source, large-scale multilingual LLMs. It addresses the limitations of existing models primarily focused on English and demonstrates strong performance on various benchmarks, including general benchmarks and vertical domains.

- **Influential Works:**
    - Touvron, H., et al. "LLaMA: Open and efficient foundation language models." *arXiv preprint arXiv:2302.13971* (2023a).
    - Kaplan, J., et al. "Scaling laws for neural language models." *arXiv preprint arXiv:2001.08361* (2020).
    - Vaswani, A., et al. "Attention is all you need." *Advances in Neural Information Processing Systems* (2017).

- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. The authors provide a comprehensive overview of the relevant research, citing key works in the field of LLMs, scaling laws, and training techniques. However, the authors could have included more citations for specific claims related to the performance of Baichuan 2 on various benchmarks and for works that address the ethical challenges of LLMs.

Overall, "Baichuan 2: Open Large-scale Language Models" is a valuable contribution to the field of open-source LLMs. The paper demonstrates the effectiveness of Baichuan 2 in various tasks and domains, highlighting its potential for real-world applications. The authors' commitment to transparency and open-source research is commendable and will likely contribute to the advancement of the field.
