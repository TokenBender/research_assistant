## CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving

**1. Introduction**

- **Title:** CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving
- **Authors:** Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Holtzman, Junchen Jiang
- **Publication Date:** August 4-8, 2024 (SIGCOMM '24)
- **Objective:** To address the network delay associated with fetching large KV caches for long contexts in LLM systems, the authors propose CacheGen, a context-loading module that compresses and streams KV caches to reduce the overall delay.
- **Number of References:** 157

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - LLMs are increasingly used in various applications, often requiring long contexts for improved response quality. [22, 38, 46, 128]
    - Processing long contexts can lead to significant delays, especially when fetching large KV caches from remote machines. [1, 2, 87]
    - Existing approaches focus on reducing GPU memory footprint of KV caches, but not the network delay. [62, 78, 97]
    - CacheGen aims to reduce the network delay by encoding KV caches into compact bitstreams and adapting the compression level to network bandwidth.
- **Significant Citations:**
    - **Claim:** "With impressive generative quality, large language models (LLMs) are ubiquitously used [22, 38, 46, 128] in personal assistance, AI healthcare, and marketing."
    - **Citation:** [22] Anastasiya Zharovskikh. 2023. Applications of Large Language Models - InData Labs. https://indatalabs.com/blog/large-language-model-apps. (June 2023). (Accessed on 09/21/2023).
    - **Explanation:** This citation supports the claim that LLMs are widely used in various applications, highlighting their growing popularity and impact.
    - **Claim:** "There have been a few recent efforts to reduce the run-time size of KV cache in GPU memory in order to fit the memory limit or LLM's input limit. Some drop unimportant tokens from KV cache or context text [71, 72, 95, 153], and others apply smart quantization on KV cache tensor [62, 78, 97]."
    - **Citation:** [71] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. (2023). arXiv:cs.CL/2310.05736
    - **Explanation:** This citation provides examples of existing techniques that aim to reduce the size of KV caches in GPU memory, highlighting the focus on reducing memory footprint rather than network delay.

**2.2 Background and Motivation**

- **Key Points:**
    - Transformers are the dominant architecture for LLMs. [37, 44, 131]
    - LLMs process input tokens in two phases: prefill and generation. [37, 44, 131]
    - The prefill phase involves computing KV caches, which are large and often stored in GPU memory. [82]
    - Reusing KV caches can reduce prefill delay, but fetching them from remote machines can introduce significant network delay. [23, 58, 82, 156]
    - The network delay for fetching KV caches has not received much attention in previous work. [58, 111, 157]
- **Significant Citations:**
    - **Claim:** "Transformers [37, 44, 131] are the de facto models for most large language model (LLM) services."
    - **Citation:** [37] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. (2020). arXiv:cs.CL/2005.14165
    - **Explanation:** This citation establishes the importance of transformers as the foundation for modern LLMs, providing context for the subsequent discussion of KV caches.
    - **Claim:** "The extra network delay for fetching the KV cache has not yet received much attention. Previous systems assume the KV cache of a context is always kept in the same GPU memory between different requests sharing the same context [58], or the KV cache is small enough to be sent quickly by a fast interconnection [111, 157]."
    - **Citation:** [58] In Gim, Guojun Chen, Seung seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin Zhong. 2023. Prompt Cache: Modular Attention Reuse for Low-Latency Inference. (2023). arXiv:cs.CL/2311.04934
    - **Explanation:** This citation highlights the common assumption in previous work that KV caches are always readily available in local GPU memory, setting the stage for the authors' argument that network delay is a significant issue.

**2.3 Context in LLM Input**

- **Key Points:**
    - LLMs often require additional context to generate high-quality responses, especially when dealing with domain-specific knowledge or conversational history. [53, 89]
    - Long contexts are increasingly common in practice, as they are more likely to contain the necessary information for accurate responses. [67, 35]
    - Long contexts are often reused across different inputs, making it beneficial to cache and reuse KV caches. [67, 35]
- **Significant Citations:**
    - **Claim:** "Thus, many LLM applications and users supplement the LLM input with additional texts, referred to as the context [53, 89]. The LLM can read the context first and use its in-context learning capability to generate high-quality responses."
    - **Citation:** [53] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented Generation for Large Language Models: A Survey. (2024). arXiv:cs.CL/2312.10997
    - **Explanation:** This citation emphasizes the importance of context in improving LLM performance, setting the stage for the discussion of long contexts and their reuse.
    - **Claim:** "Indeed, FiD [67] shows that the accuracy increases from 40% to 48% when the context increases from 1K tokens to 10K. Retro [35] similarly shows that the generation quality (perplexity) improves significantly when the context increases from 6K tokens to 24K."
    - **Citation:** [67] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. (2021). arXiv:cs.CL/2007.01282
    - **Explanation:** This citation provides empirical evidence supporting the claim that longer contexts can lead to improved response quality, further motivating the need for efficient context loading mechanisms.

**2.4 Context in LLM Input**

- **Key Points:**
    - Reusing KV caches can significantly reduce prefill delay, but only if the KV cache is readily available in local GPU memory. [23, 58, 82, 156]
    - In practice, KV caches are often stored in remote machines, leading to significant network delay when fetching them. [27, 31, 36, 84, 110]
    - The network delay for fetching KV caches has not been adequately addressed in previous work. [58, 111, 157]
- **Significant Citations:**
    - **Claim:** "Yet, the KV cache of a reused context may not always be in local GPU memory when the next input comes; instead, the KV cache may need to be retrieved from another machine(s) first, causing extra network delays (Figure 1a)."
    - **Citation:** [27] AutoGPT. 2023. Significant-Gravitas/Auto-GPT: An experimental open-source attempt to make GPT-4 fully autonomous. https://github.com/Significant-Gravitas/Auto-GPT. (September 2023). (Accessed on 09/21/2023).
    - **Explanation:** This citation highlights the practical challenge of KV cache availability, emphasizing that the KV cache may not always be readily available in local GPU memory, leading to network delay.
    - **Claim:** "The extra network delay for fetching the KV cache has not yet received much attention. Previous systems assume the KV cache of a context is always kept in the same GPU memory between different requests sharing the same context [58], or the KV cache is small enough to be sent quickly by a fast interconnection [111, 157]."
    - **Citation:** [58] In Gim, Guojun Chen, Seung seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin Zhong. 2023. Prompt Cache: Modular Attention Reuse for Low-Latency Inference. (2023). arXiv:cs.CL/2311.04934
    - **Explanation:** This citation highlights the common assumption in previous work that KV caches are always readily available in local GPU memory, setting the stage for the authors' argument that network delay is a significant issue.

**3. Key Insights and Supporting Literature**

- **Insight 1:** KV cache values exhibit token-wise locality, meaning values for nearby tokens are more similar than those for distant tokens. [5.1.1]
    - **Supporting Citations:**
        - [49] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
        - **Explanation:** This citation provides background on the self-attention mechanism in transformers, which underlies the observed token-wise locality in KV caches.
- **Insight 2:** Different layers in KV caches have varying sensitivity to data loss, with shallower layers being more sensitive. [5.1.2]
    - **Supporting Citations:**
        - [119] Hang Shao, Bei Liu, and Yanmin Qian. 2024. One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (2024). arXiv:cs.CL/2310.09499
        - **Explanation:** This citation provides evidence that different layers in transformers extract different levels of information, explaining the observed sensitivity to data loss in different layers.
- **Insight 3:** Grouping KV cache values by channel and layer provides higher information gain than grouping by token position. [5.1.3]
    - **Supporting Citations:**
        - [49] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
        - **Explanation:** This citation provides background on the self-attention mechanism in transformers, which underlies the observed token-wise locality in KV caches.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The authors evaluate CacheGen on three LLMs (Mistral-7B, Llama-34B, and Llama-70B) fine-tuned for long contexts.
    - They use four datasets: LongChat, TriviaQA, NarrativeQA, and WikiText.
    - They compare CacheGen with three baselines: default quantization, text context, and context compression (H2O and LLMlingua).
    - They measure performance using TTFT, KV cache size, accuracy, F1 score, and perplexity.
- **Methodology Foundations:**
    - The authors leverage the insights from their empirical observations on KV cache characteristics to design CacheGen's KV cache encoder. [5.1]
    - They adapt the compression level of KV caches dynamically based on network bandwidth to meet service-level objectives (SLOs). [5.3]
    - They implement CacheGen in Python and CUDA, integrating it with HuggingFace Transformers and LangChain. [6]
- **Novel Aspects of Methodology:**
    - The authors propose a novel KV cache encoder that leverages the distributional properties of KV caches, including token-wise locality, layer-wise sensitivity to loss, and distribution along layers, channels, and tokens. [5.2]
    - They introduce a KV cache streaming adaptation logic that dynamically adjusts the compression level based on network bandwidth to meet SLOs. [5.3]
    - They conduct a user study to evaluate the quality of experience (QoE) of CacheGen compared to baselines. [7.5]

**5. Results in Context**

- **Main Results:**
    - CacheGen reduces TTFT by 3.1-4.7x compared to text context and 3.2-3.7x compared to default quantization. [7.2]
    - CacheGen reduces KV cache size by 3.5-4.3x compared to default quantization. [7.2]
    - CacheGen further reduces KV cache size by 3.3-4.2x when combined with context compression baselines (H2O and LLMlingua). [7.2]
    - CacheGen's performance is consistent across various workloads, including different context lengths, network bandwidths, and numbers of concurrent requests. [7.3]
    - CacheGen's decoding overhead is minimal compared to LLM inference. [7.5]
- **Comparison with Existing Literature:**
    - The authors compare CacheGen with existing context compression techniques, including H2O and LLMlingua, demonstrating that CacheGen can further reduce KV cache size even after applying these techniques. [7.2]
    - They also compare CacheGen with other approaches for speeding up LLM inference, such as using smaller models, token dropping, and gisting, highlighting the advantages of CacheGen's non-intrusive approach. [7.3]
- **Confirmation, Contradiction, or Extension:**
    - CacheGen's results confirm the benefits of KV cache reuse for reducing prefill delay, but also highlight the importance of addressing network delay in fetching KV caches. [23, 58, 82, 156]
    - CacheGen's results extend existing work on context compression by demonstrating that further compression is possible even after applying techniques like H2O and LLMlingua. [72, 153]

**6. Discussion and Related Work**

- **Situating the Work:**
    - The authors discuss the limitations of existing approaches for reducing KV cache size and highlight the need for a solution that addresses network delay. [62, 78, 97]
    - They compare CacheGen with other techniques for speeding up LLM inference, including model parallelism, context shortening, and retrieval-augmented generation (RAG). [86, 95, 103, 111, 112, 115, 120, 137, 152, 157, 35, 67, 68, 88, 113, 117, 134]
    - They emphasize the novelty of CacheGen's approach, which leverages the distributional properties of KV caches to achieve efficient compression and streaming. [5.2, 5.3]
- **Key Papers Cited:**
    - [62] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. 2024. KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization. arXiv preprint arXiv:2401.18079 (2024).
    - **Explanation:** This citation highlights the limitations of existing quantization techniques for KV caches, motivating the need for a more efficient approach like CacheGen.
    - [35] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. (2022). arXiv:cs.CL/2112.04426
    - **Explanation:** This citation provides context for the discussion of retrieval-augmented generation (RAG), highlighting the importance of context in improving LLM performance.

**7. Future Work and Open Questions**

- **Future Work:**
    - The authors suggest extending CacheGen to stream KV caches incrementally, similar to Scalable Video Coding (SVC). [61]
    - They propose investigating the use of CacheGen in real-world LLM applications where context reuse is common. [2.2]
    - They plan to evaluate CacheGen on higher-end GPUs and extra-large LLMs. [9]
    - They aim to combine CacheGen with other techniques for managing KV caches, such as caching policies and location optimization. [52, 74, 147]
- **Open Questions:**
    - How can CacheGen be adapted to handle volatile contexts, such as those used in search-based applications? [9]
    - What are the trade-offs between different KV cache compression techniques in terms of compression ratio, decoding overhead, and impact on LLM performance? [7.5]

**8. Critical Analysis of Citation Usage**

- **Effectiveness:**
    - The authors effectively use citations to support their arguments and findings, providing a strong foundation for their claims.
    - They cite a wide range of relevant works, demonstrating a thorough understanding of the existing literature.
- **Areas for Improvement:**
    - While the authors cite a broad range of works, they could have included more citations from the field of tensor compression, as CacheGen's KV cache encoding is essentially a form of tensor compression. [109, 154]
- **Potential Biases:**
    - The authors primarily cite works from the fields of deep learning, natural language processing, and computer systems. They could have included more citations from related fields, such as information retrieval and database management, to provide a more comprehensive overview of the research landscape.

**9. Final Summary**

- **Contribution:** CacheGen makes a significant contribution to the field of LLM serving by addressing the network delay associated with fetching large KV caches for long contexts.
- **Influential Works:**
    - [37] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. (2020). arXiv:cs.CL/2005.14165
    - **Explanation:** This work is foundational to the field of LLMs, providing context for the authors' research on improving LLM serving performance.
    - [58] In Gim, Guojun Chen, Seung seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin Zhong. 2023. Prompt Cache: Modular Attention Reuse for Low-Latency Inference. (2023). arXiv:cs.CL/2311.04934
    - **Explanation:** This work highlights the importance of addressing network delay in LLM serving, motivating the authors' research on CacheGen.
- **Integration of Existing Literature:**
    - The authors effectively integrate existing literature to support their claims and findings, providing a clear understanding of the research context and the novelty of their work.
    - They demonstrate a strong understanding of the challenges and opportunities in LLM serving, and their work builds upon a solid foundation of previous research.

**Overall Assessment:**

CacheGen is a well-written and well-researched paper that makes a significant contribution to the field of LLM serving. The authors effectively address the network delay associated with fetching large KV caches for long contexts, proposing a novel solution that leverages the distributional properties of KV caches to achieve efficient compression and streaming. Their work is well-supported by a comprehensive review of the existing literature, and their experimental results demonstrate the effectiveness of CacheGen in reducing TTFT and KV cache size while maintaining high performance. The authors also acknowledge the limitations of their work and suggest promising directions for future research.
