Okay, here's the markdown-formatted analysis of the provided academic paper, "SALMONN: Towards Generic Hearing Abilities for Large Language Models", following the guidelines you provided:


# SALMONN: Towards Generic Hearing Abilities for Large Language Models - Citation Analysis

## 1. Introduction

- **Title:** SALMONN: Towards Generic Hearing Abilities for Large Language Models
- **Authors:** Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang
- **Publication Date:** Published as a conference paper at ICLR 2024 (likely early 2024 based on arXiv submission)
- **Main Objective:** The research aims to develop SALMONN, a multimodal large language model (LLM) that can perceive and understand general audio inputs, including speech, audio events, and music, achieving generic hearing abilities for LLMs.
- **Total Number of References:** 77


## 2. Section-by-Section Analysis with Citation Extraction

### 2.1 Introduction

**Summary:** This section introduces the concept of generic hearing abilities for AI agents and highlights the growing interest in multimodal LLMs. It introduces SALMONN as a novel multimodal LLM that integrates speech and audio encoders with a pre-trained text-based LLM. The authors emphasize the model's ability to handle diverse audio tasks, including both trained and emergent abilities.

**Significant Citations:**

1. **Claim:** "Text-based large language models (LLMs) (Brown et al., 2020; Touvron et al., 2023; Chiang et al., 2023; Anil et al., 2023; Du et al., 2022) have demonstrated remarkable and even human-level performance in many natural language processing (NLP) tasks (OpenAI, 2023)."
   - **Citation:** Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems*.
   - **Touvron, H., Lachaux, M. A., L'Huillier, N., Olive, F., Lacroix, T., & Grave, E. (2023). LLaMA: Open and efficient foundation language models*. *arXiv preprint arXiv:2302.13971*.
   - **Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., & Xing, E. P. (2023). Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality*. *arXiv preprint arXiv:2303.08774*.
   - **Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, P., Bailey, Z., Chen, Z., et al. (2023). PaLM 2 technical report*. *arXiv preprint arXiv:2305.10403*.
   - **Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., & Tang, J. (2022). GLM: General language model pretraining with autoregressive blank infilling*. *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics*.
   - **OpenAI. (2023). GPT-4 technical report*. *arXiv preprint arXiv:2303.08774*.
   - **Relevance:** This citation establishes the context of LLMs' recent success in NLP tasks, setting the stage for the paper's focus on extending these capabilities to audio.


2. **Claim:** "Meanwhile, instruction tuning (Wei et al., 2022a; Chung et al., 2022; Ouyang et al., 2022; Peng et al., 2023), where data is organized as pairs of user instruction (or prompt) and reference response, has emerged as an LLM training paradigm that allows LLMs to follow open-ended user instructions."
   - **Citation:** Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., & Le, Q. V. (2022). Finetuned language models are zero-shot learners. *Proceedings of the 36th International Conference on Machine Learning*.
   - **Chung, H. W., Lee, K.-H., Wang, S.-H., Hsiao, C.-Y., Kuan, C.-Y., Wu, H., Arora, S., Chang, K.-W., Shi, J., Peng, Y., et al. (2022). Dynamic-SUPERB: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech*. *arXiv preprint arXiv:2309.09510*.
   - **Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback*. *Advances in Neural Information Processing Systems*.
   - **Peng, B., Li, C., He, P., Galley, M., & Gao, J. (2023). Instruction tuning with GPT-4*. *arXiv preprint arXiv:2304.03277*.
   - **Relevance:** This citation introduces instruction tuning, a key training paradigm for LLMs, which is relevant to the paper's methodology and the training of SALMONN.


3. **Claim:** "There is a burgeoning research interest in empowering LLMs with multimodal perception abilities. Recent studies focus on connecting LLMs with either the encoder of one additional type of input, such as image (Li et al., 2023a; Alayrac et al., 2022; Dai et al., 2023), silent video (Maaz et al., 2023; Chen et al., 2023b; Zhao et al., 2022), audio events (Gong et al., 2023b; Lyu et al., 2023) or speech (Chen et al., 2023a), or the encoders of multiple input types together (Su et al., 2023; Zhang et al., 2023b)."
   - **Citation:** Li, J., Savarese, S., & Hoi, S. (2023). BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. *Proceedings of the 40th International Conference on Machine Learning*.
   - **Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., et al. (2022). Flamingo: a visual language model for few-shot learning*. *Advances in Neural Information Processing Systems*.
   - **Dai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., et al. (2023). InstructBLIP: Towards general-purpose vision-language models with instruction tuning*. *arXiv preprint arXiv:2305.06500*.
   - **Maaz, M., Rasheed, H., Khan, S., & Khan, F. S. (2023). Video-ChatGPT: Towards detailed video understanding via large vision and language models*. *arXiv preprint arXiv:2306.05424*.
   - **Chen, G., Zheng, Y.-D., Wang, J., Xu, J., Huang, Y., Pan, J., Wang, Y., Wang, Y., Qiao, Y., Lu, T., et al. (2023). VideoLLM: Modeling video sequence with large language models*. *arXiv preprint arXiv:2305.13292*.
   - **Zhao, Y., Misra, I., Krähenbühl, P., & Girdhar, R. (2022). Learning video representations from large language models*. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
   - **Gong, Y., Luo, H., Liu, A. H., Karlinsky, L., & Glass, J. (2023). Listen, think, and understand*. *arXiv preprint arXiv:2305.10790*.
   - **Lyu, C., Wu, M., Wang, L., Huang, X., Liu, B., et al. (2023). Macaw-LLM: Multi-modal language modeling with image, audio, video, and text integration*. *arXiv preprint arXiv:2306.09093*.
   - **Chen, F., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. (2023). BEATS: Audio pre-training with acoustic tokenizers*. *Proceedings of the 37th International Conference on Machine Learning*.
   - **Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., & Cai, D. (2023). PandaGPT: One model to instruction-follow them all*. *arXiv preprint arXiv:2305.16355*.
   - **Zhang, H., Li, X., & Bing, L. (2023). Video-LLaMA: An instruction-tuned audio-visual language model for video understanding*. *arXiv preprint arXiv:2306.02858*.
   - **Relevance:** This citation highlights the growing trend of multimodal LLMs, emphasizing the paper's contribution to this area by focusing on audio integration.


### 2.2 Related Work

**Summary:** This section reviews existing work on integrating speech and audio into LLMs. It discusses various approaches for handling variable-length audio inputs, including frame-rate reduction techniques and the use of connection modules. The authors also discuss the challenges of jointly modeling speech and audio events, as well as the integration of music into LLMs.

**Significant Citations:**

1. **Claim:** "LLMs, as text-based dialogue models, have a fundamental connection with speech, and several studies have attempted to extend LLMs to support direct speech inputs with a connection module (Chen et al., 2023a; Wu et al., 2023; Fathullah et al., 2023; Yu et al., 2023; Huang et al., 2023a)."
   - **Citation:** Chen, F., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. (2023). BEATS: Audio pre-training with acoustic tokenizers. *Proceedings of the 37th International Conference on Machine Learning*.
   - **Wu, J., Gaur, Y., Chen, Z., Zhou, L., Zhu, Y., Wang, T., Li, J., Liu, S., Ren, B., Liu, L., et al. (2023). On decoder-only architecture for speech-to-text and large language model integration*. *arXiv preprint arXiv:2307.03917*.
   - **Fathullah, Y., Wu, C., Lakomkin, E., Shangguan, Y., Li, K., Guo, J., Xiong, W., Mahadeokar, J., Kalinli, O., et al. (2023). Prompting large language models with speech recognition abilities*. *arXiv preprint arXiv:2307.11795*.
   - **Yu, W., Tang, C., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., & Zhang, C. (2023). Connecting speech encoder and large language model for asr*. *arXiv preprint arXiv:2309.13963*.
   - **Huang, R., Li, M., Yang, D., Shi, J., Chang, X., Ye, Z., Wu, Y., Hong, Z., Huang, J., Liu, J., et al. (2023). AudioGPT: Understanding and generating speech, music, sound, and talking head*. *arXiv preprint arXiv:2304.12995*.
   - **Relevance:** This citation establishes the connection between LLMs and speech, highlighting the research direction of integrating speech into LLMs, which is the focus of the paper.


2. **Claim:** "To avoid the LLMs having overly long input speech token sequences caused by long-form speech inputs, different frame-rate reduction approaches have been developed, including stacking-based fixed-rate reduction approach (Fathullah et al., 2023; Yu et al., 2023), speech-recognition-based variable frame-rate reduction approach (Wu et al., 2023; Chen et al., 2023a), and Q-Former-based approach with a fixed number of output frames (Yu et al., 2023) etc."
   - **Citation:** Fathullah, Y., Wu, C., Lakomkin, E., Shangguan, Y., Li, K., Guo, J., Xiong, W., Mahadeokar, J., Kalinli, O., et al. (2023). Prompting large language models with speech recognition abilities*. *arXiv preprint arXiv:2307.11795*.
   - **Yu, W., Tang, C., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., & Zhang, C. (2023). Connecting speech encoder and large language model for asr*. *arXiv preprint arXiv:2309.13963*.
   - **Wu, J., Gaur, Y., Chen, Z., Zhou, L., Zhu, Y., Wang, T., Li, J., Liu, S., Ren, B., Liu, L., et al. (2023). On decoder-only architecture for speech-to-text and large language model integration*. *arXiv preprint arXiv:2307.03917*.
   - **Chen, F., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. (2023). BEATS: Audio pre-training with acoustic tokenizers*. *Proceedings of the 37th International Conference on Machine Learning*.
   - **Yu, W., Tang, C., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., & Zhang, C. (2023). Connecting speech encoder and large language model for asr*. *arXiv preprint arXiv:2309.13963*.
   - **Relevance:** This citation discusses the challenges of handling long audio sequences and the different approaches used to address them, which is relevant to the paper's methodology.


3. **Claim:** "Unlike speech, audio event inputs are often treated as fixed-sized spectrogram images that can be processed using visual-language LLM methods without explicitly modelling temporal correlations (Gong et al., 2023a;b; Zhang et al., 2023b). These methods are therefore unable to handle speech."
   - **Citation:** Gong, Y., Khurana, S., Karlinsky, L., & Glass, J. (2023). Whisper-AT: Noise-robust automatic speech recognizers are also strong general audio event taggers. *Proceedings of Interspeech*.
   - **Gong, Y., Luo, H., Liu, A. H., Karlinsky, L., & Glass, J. (2023). Listen, think, and understand*. *arXiv preprint arXiv:2305.10790*.
   - **Zhang, H., Li, X., & Bing, L. (2023). Video-LLaMA: An instruction-tuned audio-visual language model for video understanding*. *arXiv preprint arXiv:2306.02858*.
   - **Relevance:** This citation highlights the difference in how speech and audio events are typically handled in LLMs, emphasizing the challenge of jointly modeling them, which is addressed by SALMONN.


4. **Claim:** "Although Lyu et al. (2023) uses the speech encoder from the Whisper model, only audio event inputs are supported, which indicates the difficulty of the joint modelling of speech and audio events."
   - **Citation:** Lyu, C., Wu, M., Wang, L., Huang, X., Liu, B., et al. (2023). Macaw-LLM: Multi-modal language modeling with image, audio, video, and text integration*. *arXiv preprint arXiv:2306.09093*.
   - **Relevance:** This citation highlights the difficulty of jointly modeling speech and audio events, which is a challenge that SALMONN addresses.


5. **Claim:** "Without using LLMs, Narisetty et al. (2022) studies achieving speech recognition and audio captioning separately using the same model."
   - **Citation:** Narisetty, C., Tsunoo, E., Chang, X., Kashiwagi, Y., Hentschel, M., & Watanabe, S. (2022). Joint speech recognition and audio captioning. *Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.
   - **Relevance:** This citation highlights a prior approach to speech and audio processing without LLMs, providing a contrast to the LLM-based approach of SALMONN.


6. **Claim:** "Regarding music inputs, Liu et al. (2023) integrates the MERT music encoder (Li et al., 2023b) with an LLM for music understanding tasks."
   - **Citation:** Liu, S., Hussain, A. S., Sun, C., & Shan, Y. (2023). Music understanding LLaMA: Advancing text-to-music generation with question answering and captioning. *arXiv preprint arXiv:2308.11276*.
   - **Li, Y., Yuan, R., Zhang, G., Ma, Y., Chen, X., Yin, H., Lin, C., Ragni, E., Benetos, E., Gyenge, N., et al. (2023). MERT: Acoustic music understanding model with large-scale self-supervised training*. *arXiv preprint arXiv:2306.00107*.
   - **Relevance:** This citation shows how music has been integrated with LLMs in prior work, providing context for SALMONN's broader goal of handling diverse audio types.


7. **Claim:** "AudioGPT allows a text-based LLM to process speech, audio events, and music by interacting with other models in a pipeline based on a set of pre-defined tasks (Huang et al., 2023b)."
   - **Citation:** Huang, R., Li, M., Yang, D., Shi, J., Chang, X., Ye, Z., Wu, Y., Hong, Z., Huang, J., Liu, J., et al. (2023). AudioGPT: Understanding and generating speech, music, sound, and talking head*. *arXiv preprint arXiv:2304.12995*.
   - **Relevance:** This citation introduces AudioGPT, a prior approach to multimodal audio processing with LLMs, providing a comparison point for SALMONN's end-to-end approach.


### 2.3 Methodology

**Summary:** This section details the architecture and training process of SALMONN. It describes the dual auditory encoders (Whisper and BEATs), the window-level Q-Former connection module, the Vicuna LLM, and the LoRA adaptation technique. The authors also introduce a three-stage training process: pre-training, instruction tuning, and activation tuning, with the latter addressing the issue of task overfitting.

**Significant Citations:**

1. **Claim:** "Dual Auditory Encoders: A speech encoder from OpenAI's Whisper model (Radford et al., 2023) and a non-speech BEATs audio encoder (Chen et al., 2023c) are used."
   - **Citation:** Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023). Robust speech recognition via large-scale weak supervision. *Proceedings of the 40th International Conference on Machine Learning*.
   - **Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. (2023). BEATS: Audio pre-training with acoustic tokenizers*. *Proceedings of the 37th International Conference on Machine Learning*.
   - **Relevance:** This citation introduces the core components of the auditory encoders used in SALMONN, highlighting the choice of Whisper for speech and BEATs for non-speech audio.


2. **Claim:** "Window-level Q-Former: The Q-Former structure is commonly used to convert the output of an image encoder into a fixed number of textual input tokens of an LLM (Li et al., 2023a), which requires modification when applied to handle audio inputs of variable lengths."
   - **Citation:** Li, J., Savarese, S., & Hoi, S. (2023). BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. *Proceedings of the 40th International Conference on Machine Learning*.
   - **Relevance:** This citation explains the origin and purpose of the Q-Former module, which is adapted for audio processing in SALMONN.


3. **Claim:** "LLM and LoRA: A pre-trained Vicuna LLM is used in this work (Chiang et al., 2023) which is a LLAMA LLM (Touvron et al., 2023) fine-tuned to follow instructions. LoRA (Hu et al., 2022) is a widely used parameter-efficient fine-tuning method for LLM adaptation, which is used in SALMONN to adapt the query and value weight matrices in the self-attention layers of Vicuna."
   - **Citation:** Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., & Xing, E. P. (2023). Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality*. *arXiv preprint arXiv:2303.08774*.
   - **Touvron, H., Lachaux, M. A., L'Huillier, N., Olive, F., Lacroix, T., & Grave, E. (2023). LLaMA: Open and efficient foundation language models*. *arXiv preprint arXiv:2302.13971*.
   - **Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. (2022). LoRA: Low-Rank Adaptation of large language models*. *Proceedings of the 36th International Conference on Machine Learning*.
   - **Relevance:** This citation introduces the core LLM (Vicuna) and the LoRA technique used for adapting it to the multimodal setting of SALMONN.


4. **Claim:** "We attribute task over-fitting to two reasons. First, compared to the text-only data used in LLM training, only simpler instruction prompts are used in our cross-modal instruction tuning (Wei et al., 2022a) and the resulting responses are not as complex and diverse."
   - **Citation:** Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., & Le, Q. V. (2022). Finetuned language models are zero-shot learners. *Proceedings of the 36th International Conference on Machine Learning*.
   - **Relevance:** This citation connects the task overfitting issue to the nature of instruction prompts used in cross-modal training, providing a theoretical basis for the activation tuning stage.


### 2.4 Experimental Setup

**Summary:** This section details the specific model configurations, including the versions of Whisper, BEATs, and Vicuna used. It also describes the data used for each training stage and the task specifications for the evaluation.

**Significant Citations:**

1. **Claim:** "SALMONN uses the encoder part of Whisper-Large-v2 (Radford et al., 2023) model as the speech encoder, the fine-tuned BEATS (Chen et al., 2023c) encoder as the audio encoder, and a Vicuna LLM with 13 billion parameters (Chiang et al., 2023) as the backbone LLM."
   - **Citation:** Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023). Robust speech recognition via large-scale weak supervision. *Proceedings of the 40th International Conference on Machine Learning*.
   - **Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. (2023). BEATS: Audio pre-training with acoustic tokenizers*. *Proceedings of the 37th International Conference on Machine Learning*.
   - **Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., & Xing, E. P. (2023). Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality*. *arXiv preprint arXiv:2303.08774*.
   - **Relevance:** This citation specifies the exact models used in SALMONN, providing crucial information for reproducibility.


2. **Claim:** "The three-stage training proposed in Section 3.2 is used. The data used for the first pre-training stage consists of both 960-hour LibriSpeech training set (Panayotov et al., 2015) and 1000-hour GigaSpeech M-set (Chen et al., 2021) for speech recognition, as well as 2800-hour WavCaps (Mei et al., 2023) (with audio clips longer than 180 seconds removed), AudioCaps (Kim et al., 2019) and Clotho (Drossos et al., 2020) datasets for audio captioning."
   - **Citation:** Panayotov, V., Chen, G., Povey, D., & Khudanpur, S. (2015). Librispeech: An ASR corpus based on public domain audio books. *2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.
   - **Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., et al. (2021). GigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio*. *Proceedings of Interspeech 2021*.
   - **Mei, X., Meng, C., Liu, H., Kong, Q., Ko, T., Zhao, C., Plumbley, M. D., Zou, Y., & Wang, W. (2023). WavCaps: A ChatGPT-assisted weakly-labelled audio captioning dataset for audio-language multimodal research*. *arXiv preprint arXiv:2303.17395*.
   - **Kim, C. D., Kim, B., Lee, H., & Kim, G. (2019). AudioCaps: Generating captions for audios in the wild*. *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*.
   - **Drossos, K., Lipping, S., & Virtanen, T. (2020). Clotho: An audio captioning dataset*. *2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.
   - **Relevance:** This citation provides the details of the datasets used for training, which is crucial for understanding the scope and nature of the training data.


### 2.5 Results

**Summary:** This section presents the results of SALMONN on 15 tasks, including both trained and untrained tasks. The authors demonstrate that SALMONN achieves competitive performance on trained tasks and exhibits emergent abilities on untrained tasks, particularly with activation tuning. They also analyze the impact of the LoRA scaling factor and the effectiveness of activation tuning.

**Significant Citations:**

1. **Claim:** "SALMONN, without or with activation tuning, can produce competitive results on all level 1 tasks."
   - **Relevance:** This claim is supported by the results presented in Table 3, which shows the performance of SALMONN on various tasks.


2. **Claim:** "However, the model without activation tuning suffers severely from task over-fitting and can barely perform level 2 and level 3 tasks."
   - **Relevance:** This claim is also supported by the results in Table 3, which shows that SALMONN without activation tuning struggles with more complex tasks.


3. **Claim:** "The FRs of performing SQQA, SF, Story and SAC tasks improve considerably with activation tuning."
   - **Relevance:** This claim is supported by the results in Figure 2, which shows the improvement in FR for these tasks with activation tuning.


4. **Claim:** "The underlying reason for using the cascaded Whisper + Vicuna system for reference values of the level 2 tasks lies in the fact that all level 2 tasks are zero-shot and there is no other audio-grounding system apart from SALMONN that can perform such tasks as zero-shot."
   - **Citation:** Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023). Robust speech recognition via large-scale weak supervision. *Proceedings of the 40th International Conference on Machine Learning*.
   - **Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., & Xing, E. P. (2023). Vicuna: An open-source chatbot impressing GPT-4 with 90% ChatGPT quality*. *arXiv preprint arXiv:2303.08774*.
   - **Relevance:** This citation explains the rationale for using the cascaded Whisper + Vicuna system as a baseline for comparison, highlighting the novelty of SALMONN's zero-shot capabilities.


### 2.6 Discussion and Related Work

**Summary:** This section discusses the limitations of SALMONN and compares its performance to existing methods. The authors highlight the novelty of SALMONN as the first multimodal LLM capable of understanding general audio inputs and emphasize its ability to handle a wide range of tasks, including those not seen during training.

**Significant Citations:**

1. **Claim:** "Despite such advantages, SALMONN has performance limitations on some tasks."
   - **Relevance:** This statement sets the stage for a discussion of the model's limitations, which is important for a balanced assessment of its contribution.


2. **Claim:** "First, PR is achieved by extending the LLM to consider phonemes as a new writing system. Since recognising phonemes requires finer-grained modelling of pronunciation information than recognising the word pieces used by the original Whisper ASR, it is not easy for the SALMONN model built upon an existing Whisper speech encoder to perform as well as a specialised model on the PR task."
   - **Citation:** Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2023). Robust speech recognition via large-scale weak supervision. *Proceedings of the 40th International Conference on Machine Learning*.
   - **Relevance:** This citation connects the limitations of SALMONN's phoneme recognition to the limitations of the Whisper model it builds upon, providing context for the observed performance.


3. **Claim:** "The success of SQQA mainly relies on the understanding of the spoken questions (e.g., "What is the highest mountain in the world") and answering the questions based on the commonsense knowledge stored in the text-based LLM. The drop in SQQA performance indicates that the use of LoRA cross-modal adaptation may cause the LLM to "forget" some text-based commonsense knowledge."
   - **Relevance:** This statement highlights the reliance of SQQA on the LLM's knowledge base and suggests a potential trade-off between cross-modal adaptation and knowledge retention.


### 2.7 Future Work and Open Questions

**Summary:** This section suggests potential future directions for research, including extending SALMONN to speech and audio generation and exploring the impact of the Lombard reflex.

**Significant Citations:**

1. **Claim:** "Although SALMONN is designed to focus on enabling LLMs with hearing abilities, it is possible to extend SALMONN to speech generation. The human speech production mechanism is related to auditory perception. A well-known phenomenon attributed to the speech chain is the “Lombard reflex" which describes the effect where individuals raise their voice level to be heard more clearly while speaking in noisy environments (Lane & Tranel, 1971)."
   - **Citation:** Lane, H., & Tranel, B. (1971). The Lombard sign and the role of hearing in speech. *Journal of Speech and Hearing Research, 14*(4), 677-709.
   - **Relevance:** This citation introduces the Lombard reflex, a phenomenon related to speech production and auditory perception, suggesting a potential avenue for future research in extending SALMONN's capabilities.


2. **Claim:** "This also matches the recent development in TTS that the text and audio contexts from the surrounding utterances are useful to achieve more natural prosody modelling and enable the use of more natural and casual speech data (Xu et al., 2021; Guo et al., 2021; Oplustil-Gallegos et al., 2021; Zhang et al., 2023c)."
   - **Citation:** Xu, G., Song, W., Zhang, Z., Zhang, C., He, X., & Zhou, B. (2021). Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. *2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.
   - **Guo, H., Zhang, S., Soong, F. K., He, L., & Xie, L. (2021). Conversational end-to-end TTS for voice agents*. *2021 IEEE Spoken Language Technology Workshop (SLT)*.
   - **Oplustil-Gallegos, P., O'Mahony, J., & King, S. (2021). Comparing acoustic and textual representations of previous linguistic context for improving text-to-speech*. *Proceedings of the 2021 International Conference on Speech Prosody*.
   - **Zhang, Y.-J., Zhang, C., Song, W., Zhang, Z., Wu, Y., & He, X. (2023). Prosody modelling with pre-trained cross-utterance representations for improved speech synthesis*. *IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31*(12), 2812-2823*.
   - **Relevance:** This citation connects