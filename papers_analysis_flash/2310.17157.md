## Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time

**1. Introduction**

- **Title:** Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time
- **Authors:** Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré, Beidi Chen
- **Publication Date:** 26 October 2023 (arXiv preprint)
- **Objective:** The paper aims to address the computational cost of large language models (LLMs) at inference time by proposing a novel approach called "contextual sparsity" to efficiently prune attention heads and MLP parameters without compromising model quality or in-context learning ability.
- **Number of References:** 82

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - LLMs with billions of parameters are computationally expensive at inference time.
    - Existing sparsity methods either require retraining, hinder in-context learning, or fail to achieve wall-clock time speedup.
    - The authors propose "contextual sparsity" as a solution, where input-dependent sets of attention heads and MLP parameters are pruned to achieve similar output as the dense model.
    - They introduce DEJAVU, a system that predicts contextual sparsity on-the-fly and implements it asynchronously for hardware-aware efficiency.
    - DEJAVU reduces inference latency of OPT-175B by over 2x compared to FasterTransformer and 6x compared to Hugging Face implementation.

- **Significant Citations:**
    - **Claim:** "Large language models (LLMs), such as GPT-3, PaLM, and OPT have demonstrated that an immense number of parameters unleashes impressive performance and emergent in-context-learning abilities—they can perform a task by conditioning on input-output examples, without updating their parameters"
    - **Citation:** Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.
    - **Explanation:** This citation supports the claim by highlighting the impressive capabilities of LLMs, setting the stage for the paper's focus on addressing their computational cost.
    - **Claim:** "However, they are very expensive at inference time, especially for latency-sensitive applications"
    - **Citation:** Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.
    - **Explanation:** This citation emphasizes the need for efficient inference methods, particularly for latency-sensitive applications, motivating the paper's research on contextual sparsity.

**2.2 Related Work and Problem Formulation**

- **Key Points:**
    - The paper discusses existing research on efficient inference techniques, including quantization, pruning, and distillation.
    - It highlights the challenges of applying these techniques to LLMs, particularly the difficulty of retraining large models and preserving in-context learning ability.
    - The paper analyzes the latency breakdown of LLM inference, identifying attention and MLP blocks as major bottlenecks.
    - It formally defines the problem of sparsifying attention and MLP blocks using contextual sparsity.

- **Significant Citations:**
    - **Claim:** "Various relaxations have been studied for decades for model inference in machine learning. There are three main techniques: quantization, pruning or sparsity, and distillation."
    - **Citation:** Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
    - **Explanation:** This citation provides a broad overview of existing techniques for efficient inference, setting the context for the paper's discussion of their limitations in the context of LLMs.
    - **Claim:** "The generative procedure of LLMs consists of two phases: (i) the prompt phase takes an input sequence to generate the keys and values (KV cache) for each transformer block of LLMS, which is similar to the forwarding pass of LLMs training; and (ii) the token generation phase utilizes and updates the KV cache to generate tokens step by step, where the current token generation depends on previously generated tokens."
    - **Citation:** Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102, 2022.
    - **Explanation:** This citation provides a detailed explanation of the LLM inference process, highlighting the key components that contribute to latency, and setting the stage for the paper's focus on optimizing these components.

**2.3 Pre-trained LLMs are Contextually Sparse**

- **Key Points:**
    - The authors empirically demonstrate the existence of contextual sparsity in pre-trained LLMs.
    - They show that up to 85% of attention heads and MLP parameters can be pruned without significant accuracy loss.
    - They analyze the reasons behind contextual sparsity, attributing it to the clustering behavior of attention heads and the slowly changing nature of embeddings across layers.

- **Significant Citations:**
    - **Claim:** "Inspired by prior pruning literature (Molchanov et al., 2016), we find a surprisingly simple method is sufficient to study and verify our hypothesis."
    - **Citation:** Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
    - **Explanation:** This citation acknowledges the influence of prior work on pruning, highlighting the authors' approach to building upon existing research.
    - **Claim:** "It is intuitive that we can find contextual sparsity in MLP blocks at inference time because of their activation functions, e.g., ReLU or GeLU (Kurtz et al., 2020)."
    - **Citation:** Kurtz, M., Kopinsky, J., Gelashvili, R., Matveev, A., Carr, J., Goin, M., Leiserson, W., Moore, S., Shavit, N., and Alistarh, D. Inducing and exploiting activation sparsity for fast inference on deep neural networks. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5533–5543. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/kurtz20a.html.
    - **Explanation:** This citation provides a theoretical basis for the existence of contextual sparsity in MLP blocks, linking it to the activation functions used in these blocks.

**2.4 DEJAVU**

- **Key Points:**
    - The paper presents DEJAVU, a system that exploits contextual sparsity for efficient LLM inference.
    - DEJAVU uses a low-cost learning-based algorithm to predict contextual sparsity on-the-fly.
    - It employs an asynchronous predictor to avoid sequential overhead and achieve hardware-aware efficiency.
    - The paper provides theoretical guarantees for the effectiveness of the asynchronous predictor.

- **Significant Citations:**
    - **Claim:** "Because at inference time, model parameters are static, inspired by the classical nearest neighbor search (NNS) literature and its applications in efficient deep learning, it is possible to formulate the above similarity-based prediction as an NNS problem"
    - **Citation:** Indyk, P. and Motwani, R. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing (STOC), pp. 604–613, 1998b.
    - **Explanation:** This citation highlights the authors' use of nearest neighbor search techniques, drawing inspiration from existing literature on efficient deep learning.
    - **Claim:** "Luckily, we exploit a phenomenon of LLM where token embeddings change slowly across layers due to residual connections (well-known in computer vision (He et al., 2016))."
    - **Citation:** He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
    - **Explanation:** This citation explains the key observation that enables the asynchronous predictor in DEJAVU, linking it to the well-established concept of residual connections in computer vision.

**2.5 Empirical Evaluation**

- **Key Points:**
    - The paper presents end-to-end results showing that DEJAVU achieves over 2x latency reduction compared to FasterTransformer and 6x compared to Hugging Face implementation without accuracy loss.
    - It conducts ablation studies to evaluate the effectiveness of contextual sparsity on MLP and attention blocks independently.
    - The paper explores the potential of sparsifying LLMs by skipping layers, demonstrating its feasibility.

- **Significant Citations:**
    - **Claim:** "DEJAVU achieves over 2× reduction in token generation latency compared to the state-of-the-art FasterTransformer and over 6× compared to Hugging Face with no accuracy loss."
    - **Citation:** NVIDIA. Fastertransformer. https://github.com/NVIDIA/FasterTransformer.
    - **Explanation:** This citation provides a benchmark for comparing DEJAVU's performance with existing state-of-the-art libraries, highlighting the significant speedup achieved by the proposed method.
    - **Claim:** "We present several key observations and theoretical understandings of sparsity in LLMs, upon which the DEJAVU design is based."
    - **Citation:** Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
    - **Explanation:** This citation emphasizes the authors' approach to building upon existing research on pruning, highlighting the theoretical foundation for their work on contextual sparsity.

**3. Key Insights and Supporting Literature**

- **Key Insight 1:** Pre-trained LLMs exhibit contextual sparsity, meaning that a significant portion of attention heads and MLP parameters can be pruned without affecting model accuracy for a given input.
    - **Supporting Citations:**
        - Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
        - Kurtz, M., Kopinsky, J., Gelashvili, R., Matveev, A., Carr, J., Goin, M., Leiserson, W., Moore, S., Shavit, N., and Alistarh, D. Inducing and exploiting activation sparsity for fast inference on deep neural networks. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 5533–5543. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/kurtz20a.html.
    - **Explanation:** These citations provide evidence for the existence of contextual sparsity, highlighting the authors' empirical observations and theoretical understanding of this phenomenon.

- **Key Insight 2:** Contextual sparsity can be accurately predicted using a low-cost learning-based algorithm.
    - **Supporting Citations:**
        - Indyk, P. and Motwani, R. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing (STOC), pp. 604–613, 1998b.
        - He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
    - **Explanation:** These citations provide the theoretical foundation for the authors' approach to predicting contextual sparsity, drawing upon existing literature on nearest neighbor search and residual connections.

- **Key Insight 3:** Asynchronous prediction of contextual sparsity can significantly reduce inference latency, particularly on modern hardware.
    - **Supporting Citations:**
        - NVIDIA. Fastertransformer. https://github.com/NVIDIA/FasterTransformer.
        - NVIDIA. Gpu performance background user's guide, 2022. URL https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html.
    - **Explanation:** These citations highlight the hardware-aware aspects of DEJAVU, emphasizing the importance of asynchronous prediction and memory coalescing for achieving significant speedup on modern GPUs.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The authors evaluate DEJAVU on OPT-175B, OPT-66B, and OPT-30B models.
    - They use various downstream tasks, including language modeling, question answering, and commonsense reasoning.
    - Experiments are conducted on NVIDIA A100 80GB GPU servers.

- **Methodology Foundations:**
    - The authors draw upon existing research on pruning and nearest neighbor search techniques.
    - They leverage the concept of residual connections to justify the use of asynchronous prediction.

- **Novel Aspects:**
    - The authors introduce the concept of "contextual sparsity" as a novel approach to sparsifying LLMs.
    - They propose a low-cost learning-based algorithm for predicting contextual sparsity on-the-fly.
    - They implement an asynchronous predictor to achieve hardware-aware efficiency.

**5. Results in Context**

- **Main Results:**
    - DEJAVU achieves over 2x latency reduction compared to FasterTransformer and 6x compared to Hugging Face implementation without accuracy loss.
    - Contextual sparsity is effective for both MLP and attention blocks, achieving significant speedup without compromising model quality.
    - Sparsifying LLMs by skipping layers is feasible and can further improve inference efficiency.

- **Comparison with Existing Literature:**
    - The authors compare DEJAVU's performance with existing state-of-the-art libraries, including FasterTransformer and Hugging Face implementation.
    - They demonstrate that DEJAVU outperforms these libraries in terms of latency reduction.

- **Confirmation, Contradiction, or Extension:**
    - DEJAVU's results confirm the effectiveness of pruning techniques for LLMs, but extend them by introducing the novel concept of contextual sparsity.
    - The authors' findings contradict the limitations of existing sparsity methods, demonstrating that contextual sparsity can achieve significant speedup without compromising model quality or in-context learning ability.

**6. Discussion and Related Work**

- **Situating Work within Literature:**
    - The authors discuss the limitations of existing sparsity methods, highlighting the need for a new approach that addresses the challenges of retraining large models and preserving in-context learning ability.
    - They emphasize the importance of hardware-aware efficiency for achieving significant speedup on modern GPUs.

- **Key Papers Cited:**
    - Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
    - Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
    - Indyk, P. and Motwani, R. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing (STOC), pp. 604–613, 1998b.
    - He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.
    - NVIDIA. Fastertransformer. https://github.com/NVIDIA/FasterTransformer.

- **Novelty and Importance:**
    - The authors highlight the novelty of contextual sparsity as a new approach to sparsifying LLMs.
    - They emphasize the importance of DEJAVU's hardware-aware implementation for achieving significant speedup on modern GPUs.

**7. Future Work and Open Questions**

- **Areas for Further Research:**
    - Exploring the potential of sparsifying LLMs by skipping layers.
    - Investigating the use of contextual sparsity for high-throughput settings.
    - Exploring the combination of contextual sparsity with other efficiency techniques, such as quantization.

- **Citations:**
    - The authors do not explicitly cite any works to support these suggestions for future work.

**8. Critical Analysis of Citation Usage**

- **Effectiveness:**
    - The authors effectively use citations to support their arguments and findings.
    - They provide a comprehensive overview of existing research on efficient inference techniques, highlighting the limitations of existing methods and motivating the need for their novel approach.

- **Areas for Improvement:**
    - The authors could have provided more citations to support their suggestions for future work.
    - They could have included a more detailed discussion of the limitations of existing sparsity methods, particularly in the context of LLMs.

- **Potential Biases:**
    - The authors primarily cite works from the field of deep learning and computer vision, potentially overlooking relevant research from other areas, such as natural language processing.

**9. Final Summary**

- **Contribution:** The paper makes a significant contribution to the field of efficient LLM inference by introducing the novel concept of "contextual sparsity" and demonstrating its effectiveness in reducing inference latency without compromising model quality or in-context learning ability.

- **Influential Works:**
    - Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J. Pruning convolutional neural networks for resource efficient inference. arXiv preprint arXiv:1611.06440, 2016.
    - Indyk, P. and Motwani, R. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing (STOC), pp. 604–613, 1998b.
    - He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.

- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. It provides a comprehensive overview of existing research on efficient inference techniques, highlighting the limitations of existing methods and motivating the need for their novel approach. The authors also draw upon existing research on nearest neighbor search and residual connections to justify their methodology.

Overall, the paper presents a compelling case for the use of contextual sparsity as a promising approach to addressing the computational cost of LLMs at inference time. The authors' empirical results and theoretical analysis provide strong evidence for the effectiveness of their proposed method. The paper's contribution to the field is significant, and it opens up new avenues for research on efficient LLM inference.
