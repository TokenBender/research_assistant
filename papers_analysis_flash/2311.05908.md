## FLASHFFTCONV: Efficient Convolutions for Long Sequences with Tensor Cores

**1. Introduction**

- **Title:** FLASHFFTCONV: Efficient Convolutions for Long Sequences with Tensor Cores
- **Authors:** Daniel Y. Fu, Hermann Kumbong, Eric Nguyen, Christopher Ré
- **Publication Date:** November 13, 2023
- **Objective:** To optimize the Fast Fourier Transform (FFT) convolution for long sequences, addressing the bottleneck of poor hardware utilization and expensive I/O between memory layers.
- **Total References:** 116

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - Convolution models with long filters excel in long-sequence tasks but lag behind Transformers in wall-clock time.
    - The FFT convolution, while asymptotically efficient, suffers from poor hardware utilization and I/O bottlenecks.
- **Citations:**
    - **Claim:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time.
        - **Citation:** [42, 76, 94, 110], [36, 46, 103, 115], [74, 81, 109], [82], [27, 55, 61, 71, 77, 80]
        - **Explanation:** This citation provides examples of successful applications of long-filter convolutions in various domains, highlighting their potential while acknowledging their performance limitations compared to Transformers.
    - **Claim:** A major reason is poor hardware support. Unlike classical convolutions used in vision applications, which often have short filters (e.g., 3 × 3 or 7 × 7 [53, 63]), convolutions for sequence modeling often use filters as long as the input sequence [71, 97].
        - **Citation:** [53, 63], [71, 97]
        - **Explanation:** This citation contrasts the typical short filters used in computer vision with the long filters required for sequence modeling, emphasizing the need for specialized algorithms like FFT convolution.
    - **Claim:** Such long filters necessitate the use of the FFT convolution algorithm, which computes the convolution between an input u and convolution kernel k via a conversion to frequency space.
        - **Citation:** [71, 97]
        - **Explanation:** This citation reinforces the necessity of FFT convolution for long-filter sequence modeling, setting the stage for the paper's focus on optimizing this algorithm.

**2.2 Background**

- **Key Points:**
    - The paper provides background on the FFT convolution and the Monarch FFT decomposition, highlighting their strengths and limitations.
    - It also discusses the performance characteristics of GPUs, emphasizing the importance of memory hierarchy and specialized compute units.
- **Citations:**
    - **Claim:** Recall the definition of a convolution operation: (u * k)[i] = ∑ujki-j. Computing this formula directly incurs O(NNk) FLOPs in sequence length N and kernel length Nk.
        - **Citation:** None
        - **Explanation:** This is a standard definition of convolution, not requiring a specific citation.
    - **Claim:** For long convolutions, where Nk = N, a popular strategy is to use the Fourier transform to convert the signal u and kernel k to the frequency domain, and compute the convolution using pointwise multiplication in frequency domain, using Equation 1.
        - **Citation:** None
        - **Explanation:** This is a standard explanation of FFT convolution, not requiring a specific citation.
    - **Claim:** Critically, a Fourier transform Fy over an input of length N can be computed in O(N log N) time using the FFT-bringing the overall cost of the long convolution from O(N2) to O(N log N).
        - **Citation:** None
        - **Explanation:** This is a standard explanation of the computational complexity of FFT, not requiring a specific citation.
    - **Claim:** We note that the FFT convolution technically computes a circular convolution ∑j ujki-j, where i − j < 0 loops back to the end of k.
        - **Citation:** None
        - **Explanation:** This is a standard explanation of the difference between circular and causal convolution, not requiring a specific citation.
    - **Claim:** For this reason, u and k are often padded with zeros to compute a causal convolution.
        - **Citation:** None
        - **Explanation:** This is a standard practice in causal convolution, not requiring a specific citation.
    - **Claim:** Figure 2 shows a demonstration of the order-2 Monarch FFT decomposition.
        - **Citation:** None
        - **Explanation:** This refers to a figure within the paper, not requiring a specific citation.
    - **Claim:** For N = N1 N2, an order-2 Monarch FFT decomposition rewrites Fn = P(IN2⊗FN1)DP-1(IN₁®FN2)P, where denotes the Kronecker product, FN is the NX N discrete Fourier matrix, P is a permutation matrix that reshapes the input to N1 × N2, transposes it to N2 × N1, and then reshapes it back to N, and D∈ CN×N is a diagonal matrix containing correctional values called Twiddle factors [6].
        - **Citation:** [6]
        - **Explanation:** This citation provides the source for the definition of Twiddle factors, a key component of the Monarch FFT decomposition.
    - **Claim:** Higher-order Monarch decompositions recursively apply the order-2 decomposition to FN₁ or FN2, which reduces FLOP costs but increases the number of permutation operations, increasing I/O cost.
        - **Citation:** None
        - **Explanation:** This is a general observation about the trade-offs of higher-order Monarch decompositions, not requiring a specific citation.
    - **Claim:** We provide some background on the GPU memory hierarchy and available compute units, as well as compute-bound vs. memory-bound operations.
        - **Citation:** None
        - **Explanation:** This is a general statement about the paper's scope, not requiring a specific citation.
    - **Claim:** We focus on GPU programming in this paper, but the general principles extend to most modern hardware accelerators [35, 57, 68, 114].
        - **Citation:** [35, 57, 68, 114]
        - **Explanation:** This citation acknowledges the broader applicability of the paper's concepts beyond GPUs, highlighting its relevance to a wider range of hardware accelerators.

**2.3 GPU Performance Characteristics**

- **Key Points:**
    - The paper describes the GPU memory hierarchy and its impact on performance.
    - It highlights the importance of specialized compute units like tensor cores for matrix multiplication.
    - It distinguishes between memory-bound and compute-bound operations, explaining their respective bottlenecks.
    - It introduces kernel fusion as a technique for reducing I/O costs.
- **Citations:**
    - **Claim:** GPUs have a memory hierarchy consisting of global memory (HBM), shared memory (SRAM), and registers, as shown in Figure 1 Left.
        - **Citation:** None
        - **Explanation:** This is a general description of GPU memory hierarchy, not requiring a specific citation.
    - **Claim:** Lower/larger levels of the memory hierarchy have more space but are much slower, whereas higher/smaller levels of the memory hierarchy have less space but are much faster [83-85].
        - **Citation:** [83-85]
        - **Explanation:** This citation provides the source for the general relationship between memory level and speed in GPU memory hierarchy.
    - **Claim:** The memory hierarchy is closely tied to the GPU compute model. A GPU is composed of many independent streaming multiprocessors (SMs), each of which is composed of independent threads.
        - **Citation:** None
        - **Explanation:** This is a general description of GPU architecture, not requiring a specific citation.
    - **Claim:** HBM is shared among all SMs, but each SM has an independent SRAM. The SRAM is shared among all the threads in the SM. Each thread has access to its own registers, but cannot access the registers of other threads.
        - **Citation:** None
        - **Explanation:** This is a general description of memory access patterns in GPU architecture, not requiring a specific citation.
    - **Claim:** Thus, performing global operations between SMs requires moving data to and from HBM, whereas independent work in each SM can remain local to SRAM.
        - **Citation:** None
        - **Explanation:** This is a general observation about the impact of memory hierarchy on GPU operations, not requiring a specific citation.
    - **Claim:** Modern GPUs (since the V100 [83]) have specialized matrix multiply units called tensor cores, which can compute matrix-matrix multiply operations with much higher TFLOPs than the general-purpose compute units.
        - **Citation:** [83]
        - **Explanation:** This citation provides the source for the introduction of tensor cores in GPUs, highlighting their significance for matrix multiplication.
    - **Claim:** For example, the H100 tensor core can compute matrix multiplication between 16 × 16 matrices at 1.0 PFLOPs, whereas the general-purpose compute units can only compute at 67 TFLOPs [85].
        - **Citation:** [85]
        - **Explanation:** This citation provides specific performance figures for tensor cores and general-purpose compute units on the H100 GPU, illustrating the significant performance advantage of tensor cores.
    - **Claim:** GPU operations can be memory-bound or compute-bound.
        - **Citation:** None
        - **Explanation:** This is a general classification of GPU operations, not requiring a specific citation.
    - **Claim:** Memory-bound operations are bottlenecked by the amount of I/O between HBM and registers they need to perform, and are limited by the bandwidth of the memory hierarchy.
        - **Citation:** None
        - **Explanation:** This is a general explanation of memory-bound operations, not requiring a specific citation.
    - **Claim:** Examples include simple pointwise operations such as addition or multiplication, as well as most traditional FFT implementations.
        - **Citation:** None
        - **Explanation:** This provides examples of memory-bound operations, not requiring a specific citation.
    - **Claim:** Compute-bound operations are bottlenecked by the amount of FLOPs they need to execute, and are limited by the speed of the compute units.
        - **Citation:** None
        - **Explanation:** This is a general explanation of compute-bound operations, not requiring a specific citation.
    - **Claim:** Examples include large matrix multiply operations.
        - **Citation:** None
        - **Explanation:** This provides an example of a compute-bound operation, not requiring a specific citation.
    - **Claim:** A popular method for reducing I/O costs is kernel fusion-loading data for multiple operations into SRAM, computing them independently in each SM, and then writing the final results back to HBM.
        - **Citation:** None
        - **Explanation:** This is a general description of kernel fusion, not requiring a specific citation.
    - **Claim:** Kernel fusion is common (and can be automated) for pointwise operations [93], but is more challenging for complex operations that require referencing multiple pieces of data.
        - **Citation:** [93]
        - **Explanation:** This citation provides the source for the statement that kernel fusion is common for pointwise operations, highlighting the challenges associated with complex operations.
    - **Claim:** For example, fusing the operations in attention was not common until the development of FlashAttention [24].
        - **Citation:** [24]
        - **Explanation:** This citation highlights the significance of FlashAttention in enabling kernel fusion for complex operations like attention, demonstrating the progress in this area.

**2.4 FlashFFTConv**

- **Key Points:**
    - The paper introduces FLASHFFTCONV, a new system that optimizes FFT convolution for long sequences.
    - It adapts the Monarch FFT decomposition to convolutions, enabling kernel fusion and reducing I/O costs.
    - It presents a cost model for different order-p Monarch decompositions, guiding the selection of the optimal decomposition based on sequence length.
    - It proposes two architectural extensions: partial convolutions and frequency-sparse convolutions, offering further opportunities for optimization.
- **Citations:**
    - **Claim:** Section 3.1 provides a broad overview of FLASHFFTCONV and shows how to adapt the Monarch FFT decomposition to convolutions, which involves broadcasting the matrix multiply in parallel across the input sequence.
        - **Citation:** None
        - **Explanation:** This is a general statement about the section's content, not requiring a specific citation.
    - **Claim:** We also describe our kernel fusion strategy and how we exploit domain-specific properties of the convolution in ML for further optimization.
        - **Citation:** None
        - **Explanation:** This is a general statement about the section's content, not requiring a specific citation.
    - **Claim:** Section 3.2 presents a cost model characterizing the relative cost of different order-p decompositions of the FFT as sequence length changes, along with a simple heuristic for selecting p given hardware characteristics.
        - **Citation:** None
        - **Explanation:** This is a general statement about the section's content, not requiring a specific citation.
    - **Claim:** Finally, Section 3.3 discusses architectural extensions by presenting analogues to sparsity in convolutional kernels.
        - **Citation:** None
        - **Explanation:** This is a general statement about the section's content, not requiring a specific citation.

**2.5 FlashFFTConv Algorithm**

- **Key Points:**
    - The paper describes the core FLASHFFTCONV algorithm, outlining its steps and key optimizations.
    - It explains how the Monarch decomposition enables kernel fusion for long sequences.
    - It highlights the benefits of recomputation for reducing memory footprint and I/O costs.
    - It discusses domain-specific optimizations tailored to sequence learning workloads.
- **Citations:**
    - **Claim:** Algorithm 1 FLASHFFTCONV core algorithm, with order-2 Monarch decomposition. We assume N = N₁? for simplicity here.
        - **Citation:** None
        - **Explanation:** This refers to an algorithm within the paper, not requiring a specific citation.
    - **Claim:** Input: Input u ∈ RB×H×N, convolution kernel kf ∈ CH×N, FFT matrices F ∈ CN1×N1, F−1 ∈ CN1×N1, Twiddle factors t ∈ CN, tinv ∈ CN, B tile size Btile, H tile size Htile.
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's inputs, not requiring a specific citation.
    - **Claim:** Output: Output y ∈ RB×H×N͵
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's output, not requiring a specific citation.
    - **Claim:** for SMs in parallel across B/Btile × H/Htile do
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's parallel execution, not requiring a specific citation.
    - **Claim:** Load F, F-1, t, tinv from HBM.
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's data loading, not requiring a specific citation.
    - **Claim:** for h1 to Htile do
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's loop structure, not requiring a specific citation.
    - **Claim:** Load Kfkf[h] from HBM, reshaped to N₁ × N1.
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's data loading, not requiring a specific citation.
    - **Claim:** for b← 1 to Btile do
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's loop structure, not requiring a specific citation.
    - **Claim:** Load X ← u[b, h] from HBM, reshaped to N₁ × N1.
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's data loading, not requiring a specific citation.
    - **Claim:** X ← ((FX) *t)F
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's FFT computation, not requiring a specific citation.
    - **Claim:** X + X * Kf
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's elementwise multiplication, not requiring a specific citation.
    - **Claim:** Y ← ((XF−1)T * tinv)F-1
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's iFFT computation, not requiring a specific citation.
    - **Claim:** Write YT to HBM.
        - **Citation:** None
        - **Explanation:** This is a description of the algorithm's data writing, not requiring a specific citation.
    - **Claim:** Adapting Monarch for Fusion The Monarch FFT decomposition, as well as classical algorithms such as Bailey's FFT algorithm [6], traditionally broadcasts the matrix operation against the batch dimension and the hidden dimension, as shown in Figure 3 top left.
        - **Citation:** [6]
        - **Explanation:** This citation provides the source for the traditional approach to broadcasting matrix operations, setting the stage for the paper's novel approach.
    - **Claim:** This allows each FN₁ operation in the IN2 FN1 matrix to run independently.
        - **Citation:** None
        - **Explanation:** This is a general observation about the traditional approach, not requiring a specific citation.
    - **Claim:** However, it also makes kernel fusion difficult; fusing across the matrix multiply and permutation operations requires loading at least 16 sequences at once into SRAM to fill out the matrix multiply unit-limiting sequence length to around 2K on A100 and H100.
        - **Citation:** None
        - **Explanation:** This highlights the limitations of the traditional approach in terms of kernel fusion and sequence length.
    - **Claim:** Instead, we broadcast the matrix operation across the entire sequence, as shown in Figure 3 top right, and run the algorithm in parallel across the batch and hidden dimensions.
        - **Citation:** None
        - **Explanation:** This introduces the paper's novel approach to broadcasting matrix operations, setting the stage for the subsequent discussion of its benefits.
    - **Claim:** This reduces the SRAM requirements for kernel fusion, since we only need to load a single sequence into SRAM at a time allowing us to fuse the entire kernel for sequences up to 32K on A100 and H100.
        - **Citation:** None
        - **Explanation:** This highlights the key benefit of the novel approach in terms of reduced SRAM requirements and increased sequence length.
    - **Claim:** Broadcasting along the sequence has an added benefit: the permutations simply become matrix transposes (Figure 3 bottom), which can be done quickly using well-established routines on-chip [84].
        - **Citation:** [84]
        - **Explanation:** This citation provides the source for the statement that matrix transposes can be efficiently performed on-chip, supporting the claim that the novel approach simplifies permutation operations.
    - **Claim:** Finally, we also tile the computation across the B and H dimensions to reduce the cost of loading kf, F, and the twiddle factors from HBM.
        - **Citation:** None
        - **Explanation:** This describes an additional optimization for reducing I/O costs, not requiring a specific citation.
    - **Claim:** The core algorithm is shown in Algorithm 1 for a two-way decomposition. Higher-order decompositions and more details are given in Appendix A.
        - **Citation:** None
        - **Explanation:** This refers to an algorithm and appendix within the paper, not requiring a specific citation.
    - **Claim:** Kernel Fusion and Recomputation The Monarch decomposition allows kernel fusion for long sequences.
        - **Citation:** None
        - **Explanation:** This introduces the concept of kernel fusion, not requiring a specific citation.
    - **Claim:** Inner layers of the decomposition do not require the entire sequence, which reduces the SRAM requirements for fusion.
        - **Citation:** None
        - **Explanation:** This explains the key benefit of Monarch decomposition for kernel fusion, not requiring a specific citation.
    - **Claim:** Thus, for long sequences, we can fuse the innermost matrix operations and elementwise multiplications, and take an I/O each for the outermost matrix operations.
        - **Citation:** None
        - **Explanation:** This describes the specific implementation of kernel fusion, not requiring a specific citation.
    - **Claim:** We use also use recomputation in the backward pass to reduce the memory footprint and I/O cost.
        - **Citation:** None
        - **Explanation:** This introduces the concept of recomputation, not requiring a specific citation.
    - **Claim:** Instead of storing intermediate results on HBM for the backward pass (e.g., the intermediate result of Fnu), we simply recompute them in the backward pass.
        - **Citation:** None
        - **Explanation:** This explains the specific implementation of recomputation, not requiring a specific citation.
    - **Claim:** Domain-Specific Optimizations Finally, we use a few domain-specific optimizations to adapt the convolution specifically for the sequence learning workload.
        - **Citation:** None
        - **Explanation:** This introduces the concept of domain-specific optimizations, not requiring a specific citation.
    - **Claim:** First, since the convolutions used in sequence learning are real-to-real convolutions (with real kernel weights), we can use a classic algorithm called one-stage decimation in time to compute the FFT of a sequence of length N using a complex FFT of length N/2 (see Appendix A)-cutting the FFT cost in half.
        - **Citation:** [102]
        - **Explanation:** This citation provides the source for the one-stage decimation in time algorithm, a key optimization for real-to-real convolutions.
    - **Claim:** Second, inputs and outputs are often padded with zeros in the convolution to compute a causal convolution [42, 46, 94].
        - **Citation:** [42, 46, 94]
        - **Explanation:** This citation provides the source for the common practice of zero-padding in causal convolutions, setting the stage for the paper's optimization.
    - **Claim:** We special-case this padding, and use it to eliminate half of the outermost matrix multiply operations in the FFT and iFFT.
        - **Citation:** None
        - **Explanation:** This describes the specific optimization for zero-padding, not requiring a specific citation.
    - **Claim:** We also fuse in additional operations around the convolution, such as elementwise-gating, to further reduce I/O.
        - **Citation:** None
        - **Explanation:** This describes an additional optimization for reducing I/O costs, not requiring a specific citation.

**2.6 Cost Model of order-p Monarch Decomposition**

- **Key Points:**
    - The paper presents a formal cost model for order-p Monarch decomposition, considering both compute and I/O costs.
    - It analyzes the trade-offs between different order-p decompositions based on sequence length and hardware characteristics.
- **Citations:**
    - **Claim:** We present a formal cost model for an order-p Monarch decomposition of the convolution based on sequence length.
        - **Citation:** None
        - **Explanation:** This is a general statement about the section's content, not requiring a specific citation.
    - **Claim:** The cost model accounts for both the cost of compute and I/O, similar to a roofline analysis [54].
        - **Citation:** [54]
        - **Explanation:** This citation provides the source for the concept of roofline analysis, which is used as a framework for the cost model.
    - **Claim:** Let B and H be the batch size and model hidden dimension, respectively, and assume that we compute the convolution in half precision.
        - **Citation:** None
        - **Explanation:** This is a description of the model's assumptions, not requiring a specific citation.
    - **Claim:** Let N be the sequence length, and let N = I1Ni be the product of p factors.
        - **Citation:** None
        - **Explanation:** This is a description of the model's assumptions, not requiring a specific citation.
    - **Claim:** For simplicity, we will assume that N is a power of 2. Let u be the size of the matrix-matrix multiply unit on the GPU (e.g., 16 for A100 [84] and H100 [85]).
        - **Citation:** [84, 85]
        - **Explanation:** This citation provides the source for the size of matrix-matrix multiply units on A100 and H100 GPUs, which is a key parameter in the cost model.
    - **Claim:** Let TG and T™ be the empirically-achievable FLOPs on the GPU for general-purpose arithmetic, and matrix-matrix multiply arithmetic, respectively.
        - **Citation:** None
        - **Explanation:** This is a description of the model's assumptions, not requiring a specific citation.
    - **Claim:** For convenience, define (Ni) as a helper function that returns τα if N₁ < μ, and t™ if Ni ≥ μ.
        - **Citation:** None
        - **Explanation:** This is a description of the model's helper function, not requiring a specific citation.
    - **Claim:** Finally, let ση and os be empirically-achievable bandwidth for HBM and SRAM, respectively.
        - **Citation:** None
        - **Explanation:** This is a description of the model's assumptions, not requiring a specific citation.
    - **Claim:** Sample values for these constants are given in Appendix C.
        - **Citation:** None
        - **Explanation:** This refers to an appendix within the paper, not requiring a specific citation.
    - **Claim:** Now, we can present the cost of an FFT convolution with an order-p Monarch decomposition.
        - **Citation:** None
        - **Explanation:** This is a general statement about the section's content, not requiring a specific citation.
    - **Claim:** Let w(i) be a helper function that returns the bandwidth of the memory where the intermediate results of decomposition step i is stored.
        - **Citation:** None
        - **Explanation:** This is a description of the model's helper function, not requiring a specific citation.
    - **Claim:** The overall cost of the convolution using an order-p Monarch decomposition is given by the following:
        - **Citation:** None
        - **Explanation:** This introduces the cost model equation, not requiring a specific citation.
    - **Claim:** Figure 4 graphs Equation 2 for different order-p decompositions on different sequence lengths for A100, for p∈ {2,3,4}.
        - **Citation:** None
        - **Explanation:** This refers to a figure within the paper, not requiring a specific citation.
    - **Claim:** For cases where N₁ = = Np, the total FLOP cost of an order-p decomposition grows with O(N(p+1)/p).
        - **Citation:** None
        - **Explanation:** This is a general observation about the computational complexity of order-p decompositions, not requiring a specific citation.
    - **Claim:** However, for shorter sequences, higher-order decompositions are actually more expensive, since they decompose to matrices that are smaller than the matrix-matrix multiply unit (corresponding to the early bumps).
        - **Citation:** None
        - **Explanation:** This explains the trade-off between different order-p decompositions for short sequences, not requiring a specific citation.
    - **Claim:** Note also the bump in cost for p = 3 between 32K and 64K, which is a result of running out of SRAM but which is mediated by an extra decomposition for p = 4.
        - **Citation:** None
        - **Explanation:** This highlights a specific observation from the cost model graph, not requiring a specific citation.

**2.7 Architectural Extensions: Sparsity in Convolutions**

- **Key Points:**
    - The paper proposes two architectural extensions to FLASHFFTCONV: partial convolutions and frequency-sparse convolutions.
    - It explains the benefits of these extensions in terms of memory footprint, sequence length, and computational efficiency.
- **Citations:**
    - **Claim:** We present 2 architectural extensions to FLASHFFTCONV: partial convolutions and frequency-sparse convolutions.
        - **Citation:** None
        - **Explanation:** This is a general statement about the section's content, not requiring a specific citation.
    - **Claim:** These can be thought of as convolutional analogues to sparse attention and present opportunities for further optimization.
        - **Citation:** None
        - **Explanation:** This draws a connection between the proposed extensions and sparse attention, highlighting their potential for optimization.

**2.8 Partial Convolutions**

- **Key Points:**
    - Partial convolutions involve zeroing out later portions of the convolution kernel, analogous to local attention.
    - This reduces memory footprint and allows for natural extensions of pretrained models to longer sequences.
- **Citations:**
    - **Claim:** In partial convolutions, we zero out later portions of the convolution kernel, analogous to local attention.
        - **Citation:** None
        - **Explanation:** This is a description of partial convolutions, not requiring a specific citation.
    - **Claim:** This has two benefits. First, it reduces the memory footprint, since it requires fewer elements to be held in GPU memory at once.
        - **Citation:** None
        - **Explanation:** This explains the first benefit of partial convolutions, not requiring a specific citation.
    - **Claim:** Second, it allows for natural extensions of a pretrained convolutional model to longer sequences (i.e., via a sliding window approach).
        - **Citation:** None
        - **Explanation:** This explains the second benefit of partial convolutions, not requiring a specific citation.

**2.9 Frequency-Sparse Convolutions**

- **Key Points:**
    - Frequency-sparse convolutions involve zeroing out portions of the convolution kernel in frequency space.
    - This can lead to computational benefits by skipping portions of matrix multiplications.
- **Citations:**
    - **Claim:** In frequency-sparse convolutions, we zero out portions of the convolution kernel in frequency space, i.e. zeroing out portions of kf.
        - **Citation:** None
        - **Explanation:** This is a description of frequency-sparse convolutions, not requiring a specific citation.
    - **Claim:** This can be thought of as a variant of partial convolutions in frequency space.
        - **Citation:** None
        - **Explanation:** This draws a connection between frequency-sparse convolutions and partial convolutions, highlighting their relationship.
    - **Claim:** Here, the specific sparsity pattern can yield computational benefits. Zeroing out the right portions of the kernel can obviate the need to compute portions of the matrix-matrix multiplies in the Monarch decomposition.
        - **Citation:** None
        - **Explanation:** This explains the computational benefits of frequency-sparse convolutions, not requiring a specific citation.
    - **Claim:** We present examples of such sparsity patterns in Appendix A.
        - **Citation:** None
        - **Explanation:** This refers to an appendix within the paper, not requiring a specific citation.

**3. Experiments**

- **Key Points:**
    - The paper evaluates FLASHFFTCONV in terms of quality and efficiency.
    - It demonstrates that FLASHFFTCONV enables models to achieve better quality for the same compute budget, higher resolution in image classification, and longer sequence lengths.
    - It provides detailed benchmarks comparing FLASHFFTCONV to PyTorch and FlashAttention-v2.
- **Citations:**
    - **Claim:** In this section, we evaluate FLASHFFTCONV in terms of quality and efficiency.
        - **Citation:** None
        - **Explanation:** This is a general statement about the section's content, not requiring a specific citation.
    - **Claim:** First (Section 4.1), we show that FLASHFFTCONV allows models to achieve better quality for the same compute budget in language modeling-matching the performance of models with twice the parameters for free.
        - **Citation:** None
        - **Explanation:** This refers to a specific section within the paper, not requiring a specific citation.
    - **Claim:** FLASHFFTCONV also enables higher quality via higher resolution in image classification—solving the challenging Path-512 task for the first time simply via increased sequence length.
        - **Citation:** None
        - **Explanation:** This highlights a key finding of the experiments, not requiring a specific citation.
    - **Claim:** Next (Section 4.2), we demonstrate FLASHFFTCONV'S speedup over other implementations of convolutions, evaluate its efficiency gains when used in convolutional models, and compare a convolutional model using FLASHFFTCONV to Transformers using FlashAttention-v2.
        - **Citation:** [22]
        - **Explanation:** This refers to a specific section within the paper and cites the source for FlashAttention-v2, a key benchmark for comparison.
    - **Claim:** Finally (Section 4.3), we evaluate partial and frequency-sparse convolutions.
        - **Citation:** None
        - **Explanation:** This refers to a specific section within the paper, not requiring a specific citation.
    - **Claim:** Partial convolutions yield the first DNA model that can embed the longest genes at single-nucleotide resolution (2.3M base pairs), and frequency-sparse convolutions yield speedup while maintaining or improving quality.
        - **Citation:** None
        - **Explanation:** This highlights key findings of the experiments, not requiring a specific citation.

**3.1 Impact of Efficiency on Quality**

- **Key Points:**
    - The paper investigates the impact of FLASHFFTCONV's efficiency on model quality.
    - It shows that FLASHFFTCONV achieves higher quality for the same compute budget and enables higher quality via longer sequence lengths.
- **Citations:**
    - **Claim:** We study how FLASHFFTCONV impacts downstream quality.
        - **Citation:** None
        - **Explanation:** This is a general statement about the section's content, not requiring a specific citation.
    - **Claim:** First, given two implementations with the same compute budget, FLASHFFTCONV achieves higher quality due to higher training throughput.
        - **Citation:** None
        - **Explanation:** This highlights the first finding of the experiments, not requiring a specific citation.
    - **Claim:** Second, we show that improved efficiency can lead to higher quality via longer sequence length.
        - **Citation:** None
        - **Explanation:** This highlights the second finding of the experiments, not requiring a specific citation.
    - **Claim:** Improvement in Quality with Fixed Compute Budget To evaluate the impacts of efficiency on downstream quality, we train two popular convolutional language models, M2-BERT-base [42] and Hyena-s [94], from scratch.
        - **Citation:** [42, 94]
        - **Explanation:** This citation provides the source for the two convolutional language models used in the experiment, setting the stage for the subsequent discussion of their performance.
    - **Claim:** These models are trained BERT-style (masked language modeling) and GPT-style (next token prediction), respectively.
        - **Citation:** None
        - **Explanation:** This describes the training methods used for the models, not requiring a specific citation.
    - **Claim:** We compare the quality of models trained with the same compute budget but different implementations of the convolution either FLASHFFTCONV or a PyTorch implementation of the FFT convolution.
        - **Citation:** None
        - **Explanation:** This describes the experimental setup, not requiring a specific citation.
    - **Claim:** FLASHFFTCONV achieves higher pretraining throughput, which allows the models to see more data during pretraining.
        - **Citation:** None
        - **Explanation:** This highlights the key benefit of FLASHFFTCONV in terms of training throughput, not requiring a specific citation.
    - **Claim:** These efficiency gains improve average GLUE score by up to 3.4 points for M2-BERT-base and perplexity by 2.3 points for Hyena-s.
        - **Citation:** None
        - **Explanation:** This presents the key results of the experiment, not requiring a specific citation.
    - **Claim:** For context, these improvements in quality are similar in magnitude to the effect of doubling the number of parameters in the model (see Appendix B for reference results).
        - **Citation:** None
        - **Explanation:** This provides context for the significance of the observed quality improvements, referring to an appendix within the paper.
    - **Claim:** Longer Sequence Models Next, we show how increased efficiency can lead to higher quality via longer sequence lengths.
        - **Citation:** None
        - **Explanation:** This introduces the concept of