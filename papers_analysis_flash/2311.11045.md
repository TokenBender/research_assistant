## Analysis of "Orca 2: Teaching Small Language Models How to Reason"

**1. Introduction:**

- **Title:** Orca 2: Teaching Small Language Models How to Reason
- **Authors:** Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, Hamid Palangi, Guoqing Zheng, Corby Rosset, Hamed Khanpour, Ahmed Awadallah
- **Publication Date:** 21 November 2023 (v2)
- **Objective:** The paper aims to improve the reasoning abilities of smaller language models (LLMs) by teaching them to employ different solution strategies for different tasks and to select the most effective strategy for each task.
- **Number of References:** 69

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:**
    - LLMs are enabling more natural and sophisticated interactions between humans and machines, enhancing user experience in existing applications.
    - Scaling LLMs to ever more parameters led to emergent abilities unseen in smaller models, most notably the remarkable ability to reason zero-shot.
    - Imitation learning has emerged as the go-to approach to improve small language models, where the goal is to replicate the outputs of larger, more capable teacher models.
    - Imitation learning may limit the potential of smaller models, restricting them from utilizing the best solution strategies given the problem and the capacity of the model.
    - The paper aims to teach smaller models how to use a suite of reasoning techniques and to help them decide when to use the most effective reasoning strategy for the task at hand.
- **Significant Citations:**
    - **[3]:** "coding [3]" - This citation supports the claim that LLMs are enhancing user experience in existing applications like coding.
    - **[36]:** "web search [36]" - This citation supports the claim that LLMs are enhancing user experience in existing applications like web search.
    - **[45, 56]:** "chatbots [45, 56]" - This citation supports the claim that LLMs are enhancing user experience in existing applications like chatbots.
    - **[44]:** "GPT-4 [44]" - This citation refers to a large language model that exhibits emergent abilities.
    - **[1]:** "PaLM-2 [1]" - This citation refers to another large language model that exhibits emergent abilities.
    - **[63]:** "emergent abilities [63]" - This citation supports the claim that scaling LLMs to ever more parameters led to emergent abilities unseen in smaller models.
    - **[23]:** "reason zero-shot [23]" - This citation supports the claim that scaling LLMs to ever more parameters led to emergent abilities unseen in smaller models, most notably the remarkable ability to reason zero-shot.
    - **[51]:** "US Medical Licensing exam, on which LLMs now achieve a passing score [51]" - This citation provides an example of a complex task that LLMs can now solve.
    - **[6, 64, 56]:** "Imitation learning has emerged as the go-to approach to improve small language models [6, 64, 56]" - This citation supports the claim that imitation learning is a common approach to improve smaller language models.
    - **[13]:** "imitation learning may limit the potential of smaller models, restricting them from utilizing the best solution strategies given the problem and the capacity of the model [13]" - This citation supports the claim that imitation learning may limit the potential of smaller models.

**2.2 Preliminaries:**

**2.2.1 Instruction Tuning:**

- **Key Points:**
    - Instruction tuning involves learning from input-output pairs where the input is natural language task description, and the output is a demonstration of the desired behavior.
    - Instruction tuning has been shown to improve the model's ability to follow instructions on both seen and unseen tasks, improve the overall quality of the generations, and give models enhanced zero-shot and reasoning abilities.
    - Several studies have adopted instruction tuning to train smaller "student" language models using outputs generated by larger foundational models.
    - Instruction tuning, while very beneficial for teaching the model how to solve a task, does not necessarily teach the model new knowledge.
- **Significant Citations:**
    - **[46, 38, 62, 61]:** "Instruction tuning [46, 38, 62, 61] has emerged as a crucial step in training language models." - This citation introduces the concept of instruction tuning.
    - **[47]:** "Instruction tuning has been shown to improve the model's ability to follow instructions on both seen and unseen tasks [47]" - This citation supports the claim that instruction tuning improves the model's ability to follow instructions.
    - **[7]:** "Instruction tuning has been shown to improve the overall quality of the generations [7]" - This citation supports the claim that instruction tuning improves the overall quality of the generations.
    - **[62]:** "Instruction tuning has been shown to improve the overall quality of the generations [7] and give models enhanced zero-shot and reasoning abilities [62]." - This citation supports the claim that instruction tuning gives models enhanced zero-shot and reasoning abilities.
    - **[55]:** "Alpaca [55]" - This citation refers to a study that used instruction tuning to train a smaller language model.
    - **[6]:** "Vicuna [6]" - This citation refers to a study that used instruction tuning to train a smaller language model.
    - **[64]:** "WizardLM [64]" - This citation refers to a study that used instruction tuning to train a smaller language model.
    - **[65]:** "Baize [65]" - This citation refers to a study that used instruction tuning to train a smaller language model.
    - **[12]:** "Koala [12]" - This citation refers to a study that used instruction tuning to train a smaller language model.
    - **[42, 5]:** "However, as shown in [42, 5], it may not result in proportional improvement to small model performance when thoroughly evaluated on knowledge-intensive or reasoning-intensive tasks where correctness is not just judged by style." - This citation highlights the limitations of instruction tuning for knowledge-intensive or reasoning-intensive tasks.

**2.2.2 Explanation Tuning:**

- **Key Points:**
    - One of the known weaknesses of instruction tuning is that a resulting student model could learn to generate stylistically correct, but ultimately wrong, outputs.
    - Explanation Tuning was introduced to address this drawback by training student models on richer and more expressive reasoning signals.
    - Explanation Tuning begins with a compilation of hand-crafted, general purpose system instructions designed to elicit more careful reasoning.
    - The student model is trained to predict the LLM answer from the system instruction and user prompt.
    - Explanation Tuning has demonstrated substantial improvements over traditional instruction-tuned models, especially in complex zero-shot reasoning tasks.
- **Significant Citations:**
    - **[13]:** "One of the known weaknesses of instruction tuning is that a resulting student model could learn to generate stylistically correct, but ultimately wrong, outputs [13]." - This citation highlights the limitations of instruction tuning.
    - **[42]:** "In Orca 1, we introduced Explanation Tuning [42] to address this drawback by training student models on richer and more expressive reasoning signals." - This citation introduces the concept of Explanation Tuning.
    - **[22]:** "The primary objective of these system instructions is to extract rich demonstrations of "Slow Thinking" [22] from capable LLMs like GPT-4." - This citation explains the goal of Explanation Tuning.
    - **[35]:** "Numerous models such as Orca 1 [42], StableBeluga [35] and Dolphin have capitalized on Explanation Tuning to demonstrate substantial improvements over traditional instruction-tuned models, especially in complex zero-shot reasoning tasks." - This citation provides examples of models that have used Explanation Tuning.

**2.3 Teaching Orca 2 to be a Cautious Reasoner:**

- **Key Points:**
    - The key to Explanation Tuning is the extraction of answers with detailed explanations from LLMs based on system instructions.
    - Not every combination of system instruction cross tasks is appropriate, and in fact, the response quality can vary significantly based on the strategy described in the system instruction.
    - The authors argue that smaller models should be taught to select the most effective solution strategy based on the problem at hand.
    - The authors introduce the term "Cautious Reasoning" to refer to the act of deciding which solution strategy to choose for a given task.
    - The authors describe a four-step process for training a Cautious Reasoning LLM.
- **Significant Citations:**
    - **[22]:** "We use the term Cautious Reasoning to refer to the act of deciding which solution strategy to choose for a given task - among direct answer generation, or one of many "Slow Thinking" [22] strategies (step-by-step, guess and check or explain-then-answer, etc.)." - This citation introduces the term "Cautious Reasoning".

**2.4 Technical Details:**

**2.4.1 Dataset Construction:**

- **Key Points:**
    - The Orca 2 dataset was created from four main sources: FLAN, Few-Shot Data, Math, and Fully Synthetic Data.
    - The FLAN dataset was used to generate synthetic data for training.
    - The Few-Shot dataset was constructed by re-purposing the zero-shot data from Orca 1 dataset.
    - The Math dataset was collected from Deepmind Math dataset and other existing datasets.
    - The Fully Synthetic Data was created by instructing GPT-4 to create a summary of a Doctor-Patient Conversation.
- **Significant Citations:**
    - **[33]:** "FLAN: Our main source of prompts for synthetic data generation is the FLAN-v2 Collection [33], which consists of five sub-collections, namely, CoT, NiV2, TO, Flan 2021 and Dialogue." - This citation introduces the FLAN dataset.
    - **[50]:** "Math: We collected data for ~160K math problems from the Deepmind Math dataset [50]" - This citation introduces the Deepmind Math dataset.

**2.4.2 Training:**

- **Key Points:**
    - Orca 2 was trained with progressive learning, starting with a LLaMA-2 checkpoint and fine-tuning it on subsets of data from FLAN, Orca 1, and Orca 2 datasets.
    - The LLaMA Byte Pair Encoding (BPE) tokenizer was used for processing the input examples.
    - The packing technique was employed to optimize the training process and utilize computational resources efficiently.
    - The loss was computed only on the tokens generated by the teacher model.
- **Significant Citations:**
    - **[25]:** "Packing: To optimize the training process and utilize computational resources efficiently, we employ the packing technique [25]." - This citation introduces the packing technique.

**3. Key Insights and Supporting Literature:**

- **Key Insight 1:** Orca 2 significantly surpasses models of a similar size, even matching or exceeding those 5 to 10 times larger, especially on tasks that require reasoning.
    - **Supporting Citations:**
        - **[13]:** "While these models can produce content that matches the style of their teachers, they often fall short of their reasoning and comprehension skills [13]." - This citation highlights the limitations of imitation learning for reasoning tasks.
        - **[42]:** "This Prompt Erasure technique makes Orca 2 a Cautious Reasoner because it learns not only how to execute specific reasoning steps, but to strategize at a higher level how to approach a particular task." - This citation explains the novel approach used in Orca 2 to improve reasoning abilities.
- **Key Insight 2:** The authors demonstrate that training smaller models on tailored synthetic data can improve their reasoning abilities and achieve performance levels comparable to, and often exceeding, models that are much larger.
    - **Supporting Citations:**
        - **[21]:** "Our study has demonstrated that improving the reasoning capabilities of smaller language models is not only possible, but also attainable through training on tailored synthetic data." - This citation supports the claim that training smaller models on tailored synthetic data can improve their reasoning abilities.
        - **[42]:** "This Prompt Erasure technique makes Orca 2 a Cautious Reasoner because it learns not only how to execute specific reasoning steps, but to strategize at a higher level how to approach a particular task." - This citation explains the novel approach used in Orca 2 to improve reasoning abilities.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:**
    - The authors evaluated Orca 2 on a comprehensive set of 15 diverse benchmarks, covering approximately 100 tasks and over 36K unique prompts.
    - The benchmarks cover a variety of aspects including language understanding, common sense reasoning, multi-step reasoning, math problem solving, reading comprehension, summarization, groundedness, truthfulness, and toxic content generation and identification.
    - The authors compared Orca 2 to several other models, including LLaMA-2, WizardLM, ChatGPT, and GPT-4.
    - All models were evaluated in zero-shot settings, without any exemplars or chain-of-thought prompting.
- **Foundations:**
    - The authors used existing benchmarks and evaluation methodologies as a basis for their experimental setup.
    - **Significant Citations:**
        - **[69]:** "AGIEval [69] is a collection of diverse sets of standardized tests including general college admission tests like the GRE, GMAT, and SAT; law-focused examinations such as the LSAT and lawyer qualification assessments; math competitions; and national civil service examinations [69]." - This citation introduces the AGIEval benchmark.
        - **[52]:** "Big-Bench Hard (BBH): BBH [54] is a subset of the 23 hardest tasks of BIG-Bench [52] with a focus on challenging tasks such as those requiring multi-step reasoning." - This citation introduces the Big-Bench Hard benchmark.
        - **[10]:** "Discrete Reasoning Over Paragraphs: DROP [10] is an adversarialy-created reading comprehension benchmark, which requires models to navigate through references and execute discrete operations like addition or sorting and was adopted as part of InstructEval [5] and the OpenLLM Leaderboard." - This citation introduces the DROP benchmark.
        - **[27]:** "RACE: The RACE dataset [27] is a collection of reading comprehension questions derived from English examinations given to Chinese students aged between 12 to 18 years." - This citation introduces the RACE benchmark.
        - **[9]:** "GSM8K: This is a collection of word problems that test the ability to perform multi-step mathematical reasoning [9]." - This citation introduces the GSM8K benchmark.
        - **[17]:** "Massive Multitask Language Understanding benchmark: MMLU [17] is designed to measure the language understanding, knowledge and reasoning abilities of models and consists of 57 tasks." - This citation introduces the MMLU benchmark.
        - **[8]:** "ARC: The AI2 Reasoning Challenge [8] is a benchmark that tests the ability of text models to answer multiple-choice questions from science exams spanning Grade 3 to Grade 9 with two subsets: Easy and Challenge." - This citation introduces the ARC benchmark.
        - **[66]:** "HellaSwag: A dataset [66] for evaluating commonsense natural language inference. It tests the ability of natural language models to complete text with what might happen next in the scene about physical situations." - This citation introduces the HellaSwag benchmark.
        - **[48]:** "LAMBADA: This dataset [48] is a collection of 10,022 passages from 2,663 novels that tests the ability of natural language models to perform long-range contextual understanding." - This citation introduces the LAMBADA benchmark.
        - **[67]:** "MT-bench: is a benchmark tailored for evaluating the proficiency of chat assistants in multi-turn conversations [67] using GPT-4 as the judge." - This citation introduces the MT-bench benchmark.
        - **[59]:** "ACI-BENCH: It contains full doctor-patient conversations and associated clinical notes from various medical domains. The task is to generate a clinical note from the dialogue [59]." - This citation introduces the ACI-BENCH benchmark.
        - **[2]:** "MS-MARCO: This dataset [2] is a large-scale collection of natural language questions and answers derived from real web queries and documents." - This citation introduces the MS-MARCO benchmark.
        - **[68]:** "QMSum: A benchmark [68] for query-based multi-domain meeting summarization, where models have to select and summarize relevant spans of meetings in response to a query." - This citation introduces the QMSum benchmark.
        - **[16]:** "ToxiGen: This is a large-scale, machine-generated dataset [16] of 274,186 toxic and benign statements about 13 minority groups with a focus on implicit hate speech that does not contain slurs or profanity." - This citation introduces the ToxiGen benchmark.
        - **[53]:** "HHH: This dataset [53] is benchmark for evaluating the alignment of language models with respect to helpfulness, honesty and harmlessness, where a language model is asked to choose the best response among two options." - This citation introduces the HHH benchmark.
        - **[30]:** "TruthfulQA: A benchmark [30] for evaluating the truthfulness of LLMs in generating answers to questions constructed in a way that humans tend to answer the curated questions falsely due to false believes, biases and misconceptions." - This citation introduces the TruthfulQA benchmark.
        - **[34]:** "Automated RAI Measurement Framework: We also use a recently proposed framework [34] for evaluating the safety of a given chat-optimized model in conversational setting. Particularly, one LLM poses as a user and engages in a conversation with the LLM under test to evaluate potential harmful content, IP leakage and jailbreaks." - This citation introduces the Automated RAI Measurement Framework.
        - **[5]:** "In choosing the benchmarks, we follow the suggestions and choices made by the OpenLLM Leaderboard 10 and InstructEval [5]." - This citation explains the rationale for selecting the benchmarks.

**5. Results in Context:**

- **Reasoning Capabilities:**
    - Orca 2 significantly outperforms models of the same size on zero-shot reasoning tasks.
    - Orca 2-13B provides a relative improvement of 47.54% over LLaMA-2-Chat-13B and 28.15% over WizardLM-13B.
    - Orca 2-13B exceeds the performance of LLaMA-2-Chat-70B and performs comparably to WizardLM-70B and ChatGPT.
    - Orca 2-7B is better or comparable to LLaMA-2-Chat-70B on all reasoning tasks.
    - Using the cautious system message with both the 7B and 13B models provides small gains over the empty system message.
- **Knowledge and Language Understanding:**
    - Orca 2-13B surpasses LLaMA-2-Chat-13B and WizardLM-13B in performance on each individual benchmark.
    - Orca 2-13B achieves a relative improvement of 25.38% over LLaMA-2-Chat-13B and 44.22% over WizardLM-13B.
    - Orca 2-13B achieves a score similar to LLaMA-2-Chat-70B and WizardLM-70B in the MMLU benchmark.
    - Orca 2-7B surpasses both 70B baselines on the ARC test set.
- **Text Completion:**
    - Orca 2-7B and Orca 2-13B exhibit strong performance on HellaSwag, outperforming the 13B and 70B baselines.
    - Orca 2-13B achieves a relative improvement of 33.13% over LLaMA-2-Chat-13B and 61.94% over WizardLM-13B.
- **Multi-Turn Open Ended Conversations:**
    - Orca 2-13B performs comparably with other 13B models on MT-Bench.
    - Orca 2 is still capable of engaging in conversations, and this ability can be enhanced by packing multiple zero-shot examples into the same input sequence.
- **Grounding:**
    - Orca 2-13B exhibits the lowest rate of hallucination among all Orca 2 variants and other 13B and 70B LLMs.
    - Orca 2-13B demonstrates a relative reduction of 76.92% and 61.71% in hallucination rate compared to LLaMA-2-13B and WizardLM-13B.
- **Safety:**
    - Orca 2 models perform better at classifying toxic statements than classifying neutral statements on ToxiGen.
    - Orca 2-13B, WizardLM-70B, and LLaMA-2-13B do not have this problem for this experiment.
    - Orca 2-13B outperforms models of similar sizes on the HHH task.
    - Orca 2 models (w/ cautious systems message) tend to produce open-ended responses to some questions.
    - Orca 2-13B demonstrates a lower defect rate for Harmful Content and IP compared to LLaMA-2-Chat-13B in the Automated RAI Measurement Framework.
    - Orca 2-13B demonstrates a lower defect rate for Jailbreak compared to LLaMA-2-Chat-13B in the Automated RAI Measurement Framework.
    - Orca 2-13B performs better in answering the questions compared to other models of similar size and comparable to models with much larger size on TruthfulQA.
- **Effect of Task-Specific Data with Story Reordering:**
    - Orca 2 models trained on task-specific data show improved performance on the story reordering task.
    - Orca 2-13B outperforms other models, including GPT-4, on the story reordering task.

**6. Discussion and Related Work:**

- **Key Papers Cited in Discussion:**
    - **[13]:** "While these models can produce content that matches the style of their teachers, they often fall short of their reasoning and comprehension skills [13]." - This citation highlights the limitations of imitation learning for reasoning tasks.
    - **[42]:** "This Prompt Erasure technique makes Orca 2 a Cautious Reasoner because it learns not only how to execute specific reasoning steps, but to strategize at a higher level how to approach a particular task." - This citation explains the novel approach used in Orca 2 to improve reasoning abilities.
    - **[21]:** "Our study has demonstrated that improving the reasoning capabilities of smaller language models is not only possible, but also attainable through training on tailored synthetic data." - This citation supports the claim that training smaller models on tailored synthetic data can improve their reasoning abilities.
    - **[67]:** "We note that using any model as a proxy for evaluation (including GPT-4) has limitations depending on the model, for example, if the model has tendency to favour samples with specific characteristics like its own generations, long text or specific order of samples [67, 60, 37]." - This citation highlights the limitations of using LLMs as a proxy for evaluation.
    - **[34]:** "Automated RAI Measurement Framework: We employ a recently proposed framework for automated measurement of Responsible AI metrics for LLMs [34]." - This citation introduces the Automated RAI Measurement Framework.
    - **[57]:** "For generative style evaluation we have used the framework proposed in [34] and ToxiGen. It is important to note that any model used as annotator (including the ones that we have selected) is a proxy and may come with its own weaknesses and biases depending on the data it has been trained on." - This citation highlights the limitations of using LLMs as a proxy for evaluation.
- **Novelty and Importance:**
    - The authors highlight the novelty of their approach in teaching smaller models to employ different solution strategies for different tasks and to select the most effective strategy for each task.
    - They argue that this approach has the potential to improve the reasoning abilities of smaller models and to make them more competitive with larger models.

**7. Future Work and Open Questions:**

- **Future Work:**
    - The authors suggest that future research should focus on improving the safety and alignment of smaller models.
    - They also suggest that further research is needed to evaluate the few-shot capabilities of Orca 2.
    - The authors propose that future work should investigate the potential of using tailored and high-quality synthetic data for post-training to improve the overall safety of the models.
- **Open Questions:**
    - The authors acknowledge that there are still limitations to the current approach, such as the potential for bias and hallucination in smaller models.
    - They also note that further research is needed to understand the impact of data distribution on the performance of smaller models.

**8. Critical Analysis of Citation Usage:**

- **Effectiveness:**
    - The authors effectively use citations to support their arguments and findings.
    - They cite a wide range of relevant works, including both foundational papers and recent studies.
- **Areas for Improvement:**
    - The authors could have provided more citations to support their claims about the limitations of imitation learning.
    - They could have also provided more citations to support their claims about the potential for bias and hallucination in smaller models.
- **Potential Biases:**
    - The authors primarily cite works from Microsoft Research and other major research institutions.
    - They could have included more citations to works from independent researchers and smaller research groups.

**9. Final Summary:**

- **Contribution:** The paper makes a significant contribution to the field of deep learning by demonstrating that it is possible to improve the reasoning abilities of smaller language models by teaching them to employ different solution strategies for different tasks and to select the most effective strategy for each task.
- **Influential Works:**
    - **[13]:** This paper highlights the limitations of imitation learning for reasoning tasks.
    - **[42]:** This paper introduces the concept of Explanation Tuning.
    - **[21]:** This paper supports the claim that training smaller models on tailored synthetic data can improve their reasoning abilities.
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. The authors cite a wide range of relevant works, including both foundational papers and recent studies. They also acknowledge the limitations of their approach and suggest areas for future research.

**Overall Assessment:** This paper is a valuable contribution to the field of deep learning. The authors present a novel approach to improving the reasoning abilities of smaller language models, and they provide strong evidence to support their claims. The paper is well-written and well-organized, and it is clear and concise in its presentation of the research. The authors also acknowledge the limitations of their approach and suggest areas for future research. This paper is a must-read for anyone interested in the field of deep learning and large language models.
