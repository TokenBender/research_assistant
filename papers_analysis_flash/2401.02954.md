## DeepSeek LLM: Scaling Open-Source Language Models with Longtermism

**1. Introduction**

- **Title:** DeepSeek LLM: Scaling Open-Source Language Models with Longtermism
- **Authors:** Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang, Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou
- **Publication Date:** January 5, 2024
- **Objective:** The paper aims to investigate scaling laws for large language models (LLMs) and use these findings to develop DeepSeek LLM, a project dedicated to advancing open-source LLMs with a long-term perspective.
- **Total References:** 67

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:** The paper highlights the rapid development of open-source LLMs, particularly the LLaMA series models, and emphasizes the importance of scaling laws research for advancing LLMs towards Artificial General Intelligence (AGI). The authors note that previous research on scaling laws has yielded varying conclusions and lacked a complete description of hyperparameter settings.
- **Citations:**
    - **Claim:** "Over the past few years, Large Language Models (LLMs) based on decoder-only Transformers (Vaswani et al., 2017) have increasingly become the cornerstone and pathway to achieving Artificial General Intelligence (AGI)."
    - **Citation:** Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).
    - **Relevance:** This citation establishes the foundation of LLMs based on the Transformer architecture, which is a key component of the DeepSeek LLM.
    - **Claim:** "Subsequent developments like supervised fine-tuning and reward modeling have enabled Large Language Models (LLMs) to better follow user intentions and instructions."
    - **Citation:** Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, C., ... & Agarwal, S. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35, 27730-27744.
    - **Relevance:** This citation highlights the importance of fine-tuning and reward modeling for improving the alignment of LLMs with user intentions, which is a crucial aspect of DeepSeek LLM's development.
    - **Claim:** "This wave is sparked with closed products, such as ChatGPT (OpenAI, 2022), Claude (Anthropic, 2023), and Bard (Google, 2023), which are developed with extensive computational resources and substantial annotation costs."
    - **Citation:** OpenAI. (2022). Introducing ChatGPT. URL https://openai.com/blog/chatgpt.
    - **Relevance:** This citation introduces the closed-source LLMs that have driven the development of open-source LLMs, including DeepSeek LLM.
    - **Claim:** "Among these, the LLaMA series models (Touvron et al., 2023a,b) stand out."
    - **Citation:** Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., ... & Goyal, N. (2023a). LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.
    - **Relevance:** This citation introduces the LLaMA series models, which serve as a benchmark for DeepSeek LLM's development.
    - **Claim:** "Early works (Hoffmann et al., 2022; Kaplan et al., 2020) reached varying conclusions on the scaling of model and data with increased compute budgets and inadequately addressed hyperparameter discussions."
    - **Citation:** Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, D., ... & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.
    - **Citation:** Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.
    - **Relevance:** These citations highlight the limitations of previous research on scaling laws, which DeepSeek LLM aims to address.

**2.2 Pre-Training**

- **Key Points:** This section details the pre-training process for DeepSeek LLM, covering data collection, model architecture, hyperparameter selection, and infrastructure.
- **Citations:**
    - **Claim:** "We have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al., 2020; Penedo et al., 2023; Touvron et al., 2023a)."
    - **Citation:** Computer. (2023). Redpajama: an open dataset for training large language models. URL https://github.com/togethercomputer/RedPajama.
    - **Citation:** Gao, L., Biderman, S., Black, L., Golding, T., Hoppe, C., Foster, J., ... & Thite, A. (2020). The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.
    - **Citation:** Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, A., Cappelli, A., Alobeidli, H., ... & Launay, J. (2023). The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116.
    - **Citation:** Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., ... & Goyal, N. (2023a). LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.
    - **Relevance:** These citations highlight the sources of data used for pre-training DeepSeek LLM, demonstrating the authors' reliance on existing datasets and their efforts to improve data quality and diversity.
    - **Claim:** "We adopted an aggressive deduplication strategy, expanding the deduplication scope."
    - **Citation:** None.
    - **Relevance:** This claim highlights a novel aspect of the DeepSeek LLM's data pre-processing, which is not explicitly justified by any cited works.
    - **Claim:** "For our tokenizer, we implemented the Byte-level Byte-Pair Encoding (BBPE) algorithm based on the tokenizers library (Huggingface Team, 2019)."
    - **Citation:** Huggingface Team. (2019). Tokenizers: Fast state-of-the-art tokenizers optimized for research and production. URL https://github.com/huggingface/tokenizers.
    - **Relevance:** This citation provides the foundation for the tokenizer used in DeepSeek LLM, demonstrating the authors' reliance on existing tools and libraries.
    - **Claim:** "The micro design of DeepSeek LLM largely follows the design of LLaMA (Touvron et al., 2023a,b), adopting a Pre-Norm structure with RMSNorm (Zhang and Sennrich, 2019) function and using SwiGLU (Shazeer, 2020) as the activation function for the Feed-Forward Network (FFN)."
    - **Citation:** Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., ... & Goyal, N. (2023a). LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.
    - **Citation:** Zhang, B., & Sennrich, R. (2019). Root mean square layer normalization. Advances in Neural Information Processing Systems, 32.
    - **Citation:** Shazeer, N. (2020). Glu variants improve transformer. arXiv preprint arXiv:2002.05202.
    - **Relevance:** These citations demonstrate the authors' reliance on existing model architectures and techniques, particularly those used in the LLaMA model.
    - **Claim:** "We use an efficient and light-weight training framework named HAI-LLM (High-flyer, 2023) to train and evaluate large language models."
    - **Citation:** High-flyer. (2023). Hai-llm: 高效且轻量的大模型训练工具. URL https://www.high-flyer.cn/en/blog/hai-llm.
    - **Relevance:** This citation introduces the training framework used for DeepSeek LLM, highlighting the authors' reliance on existing tools and libraries.

**2.3 Scaling Laws**

- **Key Points:** This section delves into the scaling laws of LLMs, focusing on the scaling behavior of hyperparameters, model size, and data size. The authors highlight the importance of using a more precise representation of model scale (non-embedding FLOPs/token) and demonstrate that data quality significantly influences the optimal model/data scaling-up allocation strategy.
- **Citations:**
    - **Claim:** "Research on scaling laws (Hestness et al., 2017) predates the emergence of large language models."
    - **Citation:** Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, M. M. A., ... & Zhou, Y. (2017). Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409.
    - **Relevance:** This citation establishes the historical context of scaling laws research, which is crucial for understanding the authors' contributions to the field.
    - **Claim:** "Scaling laws (Henighan et al., 2020; Hoffmann et al., 2022; Kaplan et al., 2020) suggest that model performance can be predictably improved with increases in compute budget C, model scale N, and data scale D."
    - **Citation:** Henighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, H., ... & Petrov, S. (2020). Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701.
    - **Citation:** Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, D., ... & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.
    - **Citation:** Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.
    - **Relevance:** These citations introduce the key concepts of scaling laws, which are central to the paper's arguments and findings.
    - **Claim:** "The development of LLMs (Dai et al., 2019; Radford et al., 2019), with larger models achieving unexpected and significant performance improvements, has brought scaling laws research to a new peak."
    - **Citation:** Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.
    - **Citation:** Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., ... & Narasimhan, K. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.
    - **Relevance:** These citations highlight the recent advancements in LLM development, which have fueled the renewed interest in scaling laws research.
    - **Claim:** "However, as shown in Table 4, early works (Hoffmann et al., 2022; Kaplan et al., 2020) on the optimal model/data scaling-up allocation strategy have shown varying conclusions, raising doubts about the general applicability of scaling laws."
    - **Citation:** Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, D., ... & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.
    - **Citation:** Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.
    - **Relevance:** These citations highlight the inconsistencies and limitations of previous research on scaling laws, which DeepSeek LLM aims to address.
    - **Claim:** "To ensure that models under different compute budgets can achieve optimal performance, we first studied the scaling laws of hyperparameters."
    - **Citation:** None.
    - **Relevance:** This claim introduces a novel aspect of the DeepSeek LLM's scaling laws analysis, which is not explicitly justified by any cited works.
    - **Claim:** "Early works (Goyal et al., 2017; McCandlish et al., 2018; Shallue et al., 2019; Smith et al., 2017; Zhang et al., 2019) provided some empirical observations for setting batch size and learning rate, but we found these observations have limited applicability in our preliminary experiments."
    - **Citation:** Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... & He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677.
    - **Citation:** McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). An empirical model of large-batch training. arXiv preprint arXiv:1812.06162.
    - **Citation:** Shallue, C. J., Lee, J., Antognini, J., Sohl-Dickstein, R., Frostig, R., & Dahl, G. E. (2019). Measuring the effects of data parallelism on neural network training. Journal of Machine Learning Research, 20(112), 1-49.
    - **Citation:** Smith, S. L., Kindermans, P.-J., Ying, C., & Le, Q. V. (2017). Don't decay the learning rate, increase the batch size. arXiv preprint arXiv:1711.00489.
    - **Citation:** Zhang, G., Li, L., Nado, Z., Martens, J., Sachdeva, G., Dahl, C., ... & Grosse, R. B. (2019). Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model. Advances in neural information processing systems, 32.
    - **Relevance:** These citations highlight the limitations of previous research on hyperparameter scaling, which DeepSeek LLM aims to address.
    - **Claim:** "We then study the scaling laws of the model and data scales. To reduce experimental costs and fitting difficulties, we adopted the IsoFLOP profile approach from Chinchilla (Hoffmann et al., 2022) to fit the scaling curve."
    - **Citation:** Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, D., ... & Sifre, L. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.
    - **Relevance:** This citation introduces the IsoFLOP profile approach, which is a key methodology used in DeepSeek LLM's scaling laws analysis.
    - **Claim:** "Additionally, in the process of exploring scaling laws, the data we used underwent multiple iterations, continually improving in quality."
    - **Citation:** None.
    - **Relevance:** This claim highlights a novel aspect of the DeepSeek LLM's data pre-processing, which is not explicitly justified by any cited works.
    - **Claim:** "We initially conducted a grid search for batch size and learning rate on small-scale experiments with a compute budget of 1e17, and the results of a specific model size (177M FLOPs/token) are illustrated in Figure 2(a)."
    - **Citation:** None.
    - **Relevance:** This claim introduces the experimental setup used for analyzing hyperparameter scaling, which is not explicitly justified by any cited works.
    - **Claim:** "Then, we utilized the aforementioned multi-step learning rate scheduler to effectively train multiple models with different batch sizes, learning rates, and compute budgets ranging from 1e17 to 2e19 by reusing the first stage."
    - **Citation:** None.
    - **Relevance:** This claim highlights a novel aspect of the DeepSeek LLM's training methodology, which is not explicitly justified by any cited works.
    - **Claim:** "We validated our formulae on a series of models with a 1e20 compute budget, and the results of a specific model size (2.94B FLOPs per token) are shown in Figure 2(b)."
    - **Citation:** None.
    - **Relevance:** This claim introduces the experimental setup used for validating the scaling laws, which is not explicitly justified by any cited works.
    - **Claim:** "However, it's important to note that we have not yet considered the impact of factors beyond the compute budget C on the optimal hyperparameters."
    - **Citation:** None.
    - **Relevance:** This claim highlights a limitation of the DeepSeek LLM's scaling laws analysis, which is not explicitly justified by any cited works.
    - **Claim:** "Early works (Kaplan et al., 2020; McCandlish et al., 2018) which suggested that the optimal batch size can be modeled as being solely related to the generalization error L."
    - **Citation:** Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.
    - **Citation:** McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). An empirical model of large-batch training. arXiv preprint arXiv:1812.06162.
    - **Relevance:** These citations highlight the limitations of previous research on hyperparameter scaling, which DeepSeek LLM aims to address.
    - **Claim:** "We established the scaling laws for hyperparameters, providing an empirical framework for determining the optimal hyperparameters."
    - **Citation:** None.
    - **Relevance:** This claim highlights a key contribution of the DeepSeek LLM's scaling laws analysis, which is not explicitly justified by any cited works.
    - **Claim:** "Instead of model parameters N, we adopt non-embedding FLOPs/token M to represent the model scale, leading to a more accurate optimal model/data scaling-up allocation strategy and a better prediction of generalization loss for large-scale models."
    - **Citation:** None.
    - **Relevance:** This claim highlights a novel aspect of the DeepSeek LLM's scaling laws analysis, which is not explicitly justified by any cited works.
    - **Claim:** "The quality of pre-training data impacts the optimal model/data scaling-up allocation strategy. The higher the data quality, the more the increased compute budget should be allocated to model scaling."
    - **Citation:** None.
    - **Relevance:** This claim highlights a key finding of the DeepSeek LLM's scaling laws analysis, which is not explicitly justified by any cited works.

**2.4 Alignment**

- **Key Points:** This section describes the alignment process for DeepSeek LLM, focusing on supervised fine-tuning and direct preference optimization (DPO).
- **Citations:**
    - **Claim:** "We observed that GSM8K (Cobbe et al., 2021) and HumanEval (Chen et al., 2021) are improved consistently for the 7B model, while the 67B model hits the upper bound soon."
    - **Citation:** Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., ... & Polosukhin, I. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.
    - **Citation:** Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., ... & Sutskever, I. (2021). Evaluating large language models trained on code. CoRR, abs/2107.03374.
    - **Relevance:** These citations highlight the benchmarks used for evaluating the alignment process, demonstrating the authors' reliance on existing evaluation methods.
    - **Claim:** "To further enhance the model's ability, we used the direct preference optimization algorithm (Rafailov et al., 2023), which is proven to be a simple but effective method for LLM alignment."
    - **Citation:** Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct preference optimization: Your language model is secretly a reward model.
    - **Relevance:** This citation introduces the DPO algorithm, which is a key component of the DeepSeek LLM's alignment process.

**2.5 Evaluation**

- **Key Points:** This section presents the evaluation results for DeepSeek LLM, covering both base and chat models across various benchmarks, including multi-subject multiple-choice, language understanding and reasoning, closed-book question answering, reading comprehension, reference disambiguation, language modeling, Chinese understanding and culture, math, code, and standardized exams. The authors also discuss the performance of DeepSeek LLM on open-ended evaluation tasks in both Chinese and English.
- **Citations:**
    - **Claim:** "We evaluate our models on a series of public benchmarks both in English and Chinese, based on the internal evaluation framework."
    - **Citation:** None.
    - **Relevance:** This claim introduces the evaluation framework used for DeepSeek LLM, which is not explicitly justified by any cited works.
    - **Claim:** "Multi-subject multiple-choice datasets including MMLU (Hendrycks et al., 2020), C-Eval (Huang et al., 2023) and CMMLU (Li et al., 2023)."
    - **Citation:** Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, D., Song, D., & Steinhardt, J. (2020). Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.
    - **Citation:** Huang, Y., Bai, Y., Zhu, Z., Zhang, J., Zhang, J., Su, T., ... & Lei, J. (2023). C-Eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322.
    - **Citation:** Li, H., Zhang, Y., Koto, F., Yang, Y., Zhao, H., Gong, Y., ... & Baldwin, T. (2023). CMMLU: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212.
    - **Relevance:** These citations introduce the benchmarks used for evaluating DeepSeek LLM, demonstrating the authors' reliance on existing evaluation methods.
    - **Claim:** "Language understanding and reasoning datasets including HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018) and BigBench Hard (BBH) (Suzgun et al., 2022)."
    - **Citation:** Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., & Choi, Y. (2019). HellaSwag: Can a machine really finish your sentence?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4791-4800. Association for Computational Linguistics.
    - **Citation:** Bisk, Y., Zellers, R., Bras, R. L., Gao, J., & Choi, Y. (2020). PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020.
    - **Citation:** Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., ... & Tafjord, O. (2018). Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457.
    - **Citation:** Mihaylov, T., Clark, P., Khot, T., & Sabharwal, A. (2018). Can a suit of armor conduct electricity? a new dataset for open book question answering.
    - **Citation:** Suzgun, M., Freitag, M., Srivats, S., Vosoughi, H. W., Chung, Y., Tay, S., ... & Ruder, S. (2022). BigBench Hard (BBH): A challenging benchmark for evaluating the reasoning abilities of large language models. arXiv preprint arXiv:2205.08322.
    - **Relevance:** These citations introduce the benchmarks used for evaluating DeepSeek LLM, demonstrating the authors' reliance on existing evaluation methods.
    - **Claim:** "Closed-book question answering datasets including TriviaQA (Joshi et al., 2017) and NaturalQuestions (Kwiatkowski et al., 2019)."
    - **Citation:** Joshi, M., Choi, E., Weld, D., & Zettlemoyer, L. (2017). TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.
    - **Citation:** Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A. P., Alberti, D., ... & Petrov, S. (2019). Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7, 452-466.
    - **Relevance:** These citations introduce the benchmarks used for evaluating DeepSeek LLM, demonstrating the authors' reliance on existing evaluation methods.
    - **Claim:** "Reading comprehension datasets including RACE Lai et al. (2017) and DROP (Dua et al., 2019), C3 (Sun et al., 2019)."
    - **Citation:** Lai, G., Xie, Q., Liu, H., Yang, Y., & Hovy, E. H. (2017). RACE: large-scale reading comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 785–794. Association for Computational Linguistics.
    - **Citation:** Dua, D., Wang, Y., Dasigi, P., Stanovsky, S., Singh, S., & Gardner, M. (2019). DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368–2378. Association for Computational Linguistics.
    - **Citation:** Sun, C., Li, L., Pan, S., Bo, W., & Liu, Y. (2019). C3: A Chinese reading comprehension dataset for evaluating the ability of language understanding and reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational Linguistics.
    - **Relevance:** These citations introduce the benchmarks used for evaluating DeepSeek LLM, demonstrating the authors' reliance on existing evaluation methods.
    - **Claim:** "Reference disambiguation datasets including WinoGrande Sakaguchi et al. (2019) and CLUEWSC (Xu et al., 2020)."
    - **Citation:** Sakaguchi, K., Bras, R. L., Antognini, J., Sohl-Dickstein, R., Frostig, R., & Dahl, G. E. (2019). Winogrande: An adversarial winograd schema challenge at scale.
    - **Citation:** Xu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., ... & Lan, Z. (2020). CLUE: A chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4762–4772. International Committee on Computational Linguistics.
    - **Relevance:** These citations introduce the benchmarks used for evaluating DeepSeek LLM, demonstrating the authors' reliance on existing evaluation methods.
    - **Claim:** "Language modeling datasets including Pile (Gao et al., 2020)."
    - **Citation:** Gao, L., Biderman, S., Black, L., Golding, T., Hoppe, C., Foster, J., ... & Thite, A. (2020). The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027.
    - **Relevance:** This citation introduces the benchmark used for evaluating DeepSeek LLM, demonstrating the authors' reliance on existing evaluation methods.
    - **Claim:** "Chinese understanding and culture datasets including CHID (Zheng et al., 2019) and CCPM (Li et al., 2021)."
    - **Citation:** Zheng, C., Huang, M., & Sun, A. (2019). Chid: A large-scale chinese idiom dataset for cloze test. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 778–787. Association for Computational Linguistics.
    - **Citation:** Li, W., Qi, F., Sun, M., Yi, X., & Zhang, J. (2021). Ccpm: A chinese classical poetry matching dataset.
    - **Relevance:** These citations introduce the benchmarks used for evaluating DeepSeek LLM, demonstrating the authors' reliance on existing evaluation methods.
    - **Claim:** "Math datasets including GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021) and CMath (Wei et al., 2023)."
    - **Citation:** Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., ... & Polosukhin, I. (2021). Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.
    - **Citation:** Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, D., Song, D., & Steinhardt, J. (2020). Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.
    - **Citation:** Wei, T., Luan, J., Liu, W., Dong, S., & Wang, B. (2023). Cmath: Can your language model pass chinese elementary school math test?.
    - **Relevance:** These citations introduce the benchmarks used for evaluating DeepSeek LLM, demonstrating the authors' reliance on existing evaluation methods.
    - **Claim:** "Code datasets including HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)."
    - **Citation:** Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., ... & Sutskever, I. (2021). Evaluating large language models trained on code. CoRR, abs/2107.03374.
    - **Citation:** Austin, J., Odena, A., Nye, M., Bosma, H., Michalewski, D., Dohan, D., ... & Terry, M. (2021). Program synthesis