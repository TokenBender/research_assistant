## Analysis of "A Minimaximalist Approach to Reinforcement Learning from Human Feedback"

**1. Introduction:**

- **Title:** A Minimaximalist Approach to Reinforcement Learning from Human Feedback
- **Authors:** Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, Alekh Agarwal
- **Publication Date:** 2024 (Proceedings of the 41st International Conference on Machine Learning)
- **Objective:** The paper introduces Self-Play Preference Optimization (SPO), a novel algorithm for reinforcement learning from human feedback that avoids training a reward model and adversarial training, while being robust to non-Markovian, intransitive, and stochastic preferences.
- **Number of References:** 78

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1. Introduction:**

- **Key Points:** The paper introduces the problem of reinforcement learning from human feedback (RLHF) and highlights the limitations of existing reward-model based approaches, such as sensitivity to intransitive preferences and compounding errors. The authors propose a new approach, SPO, that directly optimizes based on preference feedback, avoiding the need for a reward model and adversarial training.
- **Significant Citations:**
    - **Claim:** RLHF, also known as preference-based reinforcement learning (PbRL), is a technique for policy optimization based on relative, rather than absolute, feedback.
    - **Citation:** Christiano et al. (2017), Akrour et al. (2012), Wirth et al. (2017), Sadigh et al. (2017), Ibarz et al. (2018), Lee et al. (2021b;a), Sikchi et al. (2022)
    - **Relevance:** This citation establishes the context of RLHF within the broader field of reinforcement learning and provides a list of key works that have contributed to the development of the field.
    - **Claim:** The predominantly studied approach to RLHF is via Reward-based RLHF, a two-stage procedure.
    - **Citation:**  Christiano et al. (2017), Akrour et al. (2012), Wirth et al. (2017), Sadigh et al. (2017), Ibarz et al. (2018), Lee et al. (2021b;a), Sikchi et al. (2022)
    - **Relevance:** This citation highlights the common approach to RLHF, which involves training a reward model and then optimizing it via reinforcement learning. This sets the stage for the authors to introduce their novel approach, SPO, which avoids this two-stage process.
    - **Claim:** RLHF has been successfully applied across fields from robotics to recommendation and, as of late, has attracted renewed interest as a leading technique for fine-tuning large language models (LLMs).
    - **Citation:** Zucker et al. (2011), Cakmak et al. (2011), Tucker et al. (2020), Swamy et al. (2020), Bıyık et al. (2020), De Gemmis et al. (2009), Ailon & Mohri (2010), Viappiani & Boutilier (2010), Afsar et al. (2022), Yue & Joachims (2009), Ziegler et al. (2020), Stiennon et al. (2020), Bai et al. (2022a), Ouyang et al. (2022)
    - **Relevance:** This citation demonstrates the wide applicability of RLHF across various domains and highlights its recent prominence in the field of natural language processing.

**2.2. Related Work:**

- **Key Points:** The authors discuss related work on dueling bandits, dueling RL, and reward-model-free approaches to RLHF. They highlight the limitations of existing approaches, such as the need for strong linearity assumptions or the reliance on adversarial training.
- **Significant Citations:**
    - **Claim:** Beginning with the seminal work of Yue et al. (2012), various authors have viewed preference-based optimization of a multi-armed or contextual bandit as a two-player zero-sum game.
    - **Citation:** Yue et al. (2012), Dudík et al. (2015), Saha et al. (2021), Saha & Krishnamurthy (2022), Bengs et al. (2021)
    - **Relevance:** This citation provides a historical overview of the development of dueling bandits and highlights the key works that have contributed to the field.
    - **Claim:** Recent work by Chen et al. (2024) formulates inverse RL for LLM fine-tuning as a kind of self-play – we focus on optimizing from preferences rather than from demonstrations.
    - **Citation:** Chen et al. (2024)
    - **Relevance:** This citation highlights a recent work that is related to the authors' approach, but focuses on a different aspect of the problem.
    - **Claim:** Several authors have proposed eliminating reward models from RLHF by leveraging the well-known bijection between the optimal policies of minimum-relative-entropy RL problems and their advantage functions.
    - **Citation:** Ziebart (2010), Zhao et al. (2023), Rafailov et al. (2023), Hejna et al. (2023), Azar et al. (2023)
    - **Relevance:** This citation highlights a recent trend in RLHF research that aims to eliminate the need for reward models. The authors discuss the limitations of these approaches and contrast them with their own approach, SPO.

**2.3. Preliminaries:**

- **Key Points:** This section introduces the mathematical framework for the paper, defining key concepts such as Markov Decision Processes (MDPs), preference oracles, and Minimax Winners.
- **Significant Citations:**
    - **Claim:** Consider a finite-horizon reward-free Markov Decision Process (MDP) parameterized by (S, A,T, H) where S, A are the state and action spaces, T:S×A → A(S) is the transition operator, and H is the horizon.
    - **Citation:** Puterman (2014)
    - **Relevance:** This citation introduces the standard mathematical framework for MDPs, which is used throughout the paper.
    - **Claim:** In the preference-based RL setup, we are given query access to a preference function P: Ξ× Ξ→ [-1,1] which, given two trajectories §1, §2 ΕΞ × Ξ, outputs a scalar that indicates the preferred trajectory.
    - **Citation:** Bai et al. (2022b), Munos et al. (2023), Zhao et al. (2023), Tucker et al. (2020)
    - **Relevance:** This citation defines the preference function, which is the core input to the SPO algorithm.

**2.4. A Brief Introduction to Social Choice Theory:**

- **Key Points:** This section introduces the concept of Minimax Winners from social choice theory, highlighting their advantages over Copeland Winners in terms of uniqueness and robustness to intransitive preferences.
- **Significant Citations:**
    - **Claim:** Social choice theory (Sen, 1986) studies the question of how best to select options that satisfy the diversity of preferences inherent in the said population.
    - **Citation:** Sen (1986)
    - **Relevance:** This citation introduces the field of social choice theory, which provides the theoretical foundation for the concept of Minimax Winners.
    - **Claim:** Observe that our above matrix has an intransitivity: a > c, c > d,d > a. This means that no reward function can explain the above preferences as it would need to satisfy r(a) > r(c), r(c) > r(d) and r(d) > r(a) simultaneously, an impossibility.
    - **Citation:** Tversky (1969), Gardner (1970)
    - **Relevance:** This citation highlights the issue of intransitive preferences, which is a common problem in real-world settings where preferences are aggregated from multiple individuals.
    - **Claim:** One potential solution to the issues with the Copeland Winner is to randomize.
    - **Citation:** Arrow (1950), Satterthwaite (1975)
    - **Relevance:** This citation introduces the concept of randomization as a way to address the issue of non-uniqueness in Copeland Winners.
    - **Claim:** For example, we see empirical evidence of this point in the high rates of inter-annotator disagreement.
    - **Citation:** Taori et al. (2023), Touvron et al. (2023)
    - **Relevance:** This citation provides empirical evidence for the issue of intransitive preferences in real-world settings, specifically in the context of large language models.
    - **Claim:** Via Sion's minimax theorem (Sion, 1958), we can guarantee that the above solution concept always exists, unlike a unique Copeland Winner.
    - **Citation:** Sion (1958)
    - **Relevance:** This citation introduces the minimax theorem, which provides a theoretical guarantee for the existence of Minimax Winners.

**2.5. One Player is All You Need for RLHF:**

- **Key Points:** The authors introduce their main contribution, SPO, which is a single-player algorithm for computing Minimax Winners. They prove that SPO avoids the need for reward modeling, compounding errors, and adversarial training, while maintaining strong convergence guarantees.
- **Significant Citations:**
    - **Claim:** Efficient algorithms for computing Nash equilibria of 2p0s games are a central focus in computational game theory.
    - **Citation:** Freund & Schapire (1997), Zinkevich (2003), Goodfellow et al. (2014), Yue et al. (2012)
    - **Relevance:** This citation provides context for the discussion of algorithms for computing Nash equilibria, which is relevant to the SPO algorithm.
    - **Claim:** By building upon the concept of a Minimax Winner from social choice theory, we are able to frame RLHF as a two-player zero-sum game.
    - **Citation:** Dudík et al. (2015), Kreweras (1965), Simpson (1969), Kramer (1973), Fishburn (1984)
    - **Relevance:** This citation highlights the key insight that allows the authors to frame RLHF as a two-player zero-sum game, which is crucial for their derivation of the SPO algorithm.
    - **Claim:** We prove rigorously that we only need a single player due to the anti-symmetry of preference functions.
    - **Citation:**  Fey (2012), Nash (1951)
    - **Relevance:** This citation highlights the key theoretical result that allows the authors to derive a single-player algorithm for computing Minimax Winners.

**2.6. SPO: Self-Play Preference Optimization:**

- **Key Points:** This section provides a detailed description of the SPO algorithm, including its theoretical foundation, convergence properties, and practical implementation.
- **Significant Citations:**
    - **Claim:** We assume access to the following optimization oracle.
    - **Citation:** Zinkevich (2003), Hazan et al. (2016)
    - **Relevance:** This citation introduces the concept of a no-regret online linear optimization algorithm, which is used as a building block for the SPO algorithm.
    - **Claim:** By the results of Freund & Schapire (1997), we know that updating Pt+1 = O(l1:t) and qt+1 = O(l1:t) implies that average strategies p = ¦ ΣPi, q = Σqi converge to a Nash equilibrium (Minimax Winner) at the rate of the underlying no-regret algorithm.
    - **Citation:** Freund & Schapire (1997)
    - **Relevance:** This citation provides a theoretical guarantee for the convergence of the SPO algorithm to a Minimax Winner.
    - **Claim:** We prove that this is not the case in general by analyzing multiple algorithms which assume that there exists a reward function that explains the observed preferences, even if it is not maintained explicitly.
    - **Citation:** Rafailov et al. (2023)
    - **Relevance:** This citation highlights the limitations of reward-model based approaches to RLHF, which motivates the need for a reward-model-free approach like SPO.
    - **Claim:** For last iterate (rather than average iterate) convergence, one can simply set the no-regret algorithm to be Optimistic Mirror Descent and apply the results of Daskalakis et al. (2017).
    - **Citation:** Daskalakis et al. (2017), Leslie & Collins (2006)
    - **Relevance:** This citation provides a theoretical guarantee for the convergence rate of the SPO algorithm, which is based on the underlying no-regret algorithm.

**2.7. Experimental Evaluation:**

- **Key Points:** This section presents the experimental results of the SPO algorithm, comparing its performance to reward-model based approaches across a variety of preference structures, including intransitive preferences, stochastic preferences, and non-Markovian preferences.
- **Significant Citations:**
    - **Claim:** We consider aggregating three populations in different proportions, each of which has transitive preferences internally.
    - **Citation:** May (1954)
    - **Relevance:** This citation provides a theoretical justification for the use of intransitive preferences in the experiments.
    - **Claim:** We measure how far off SPO is from the exact MW. We also present qualitative results on a continuous control task from Mujoco, (Brockman et al., 2016) where computing the MW for comparison is infeasible.
    - **Citation:** Brockman et al. (2016)
    - **Relevance:** This citation introduces the MuJoCo Gym environment, which is used for the continuous control experiments.
    - **Claim:** We study the robustness of RM and SPO to corruptions of various probabilities (i.e. Bernoulli noise) in preference labels.
    - **Citation:**  Agranov & Ortoleva (2017)
    - **Relevance:** This citation provides a theoretical justification for the use of stochastic preferences in the experiments.
    - **Claim:** We consider a challenging situation where we want to elicit qualitatively non-Markovian behavior (e.g. constraints on just a part of a trajectory) from a Markovian policy purely on the basis of trajectory-level relative feedback.
    - **Citation:**  Ziebart (2010)
    - **Relevance:** This citation introduces the concept of inverse reinforcement learning (IRL), which is relevant to the task of eliciting non-Markovian behavior.
    - **Claim:** We use Soft Actor Critic (SAC, Haarnoja et al. (2018)) for continuous control and Proximal Policy Optimization (PPO, Schulman et al. (2017)) for discrete action tasks, both as implemented in the ACME framework (Hoffman et al., 2020).
    - **Citation:** Haarnoja et al. (2018), Schulman et al. (2017), Hoffman et al. (2020)
    - **Relevance:** This citation introduces the specific reinforcement learning algorithms used in the experiments.

**3. Key Insights and Supporting Literature:**

- **Key Insight:** SPO is a novel algorithm for RLHF that avoids training a reward model and adversarial training, while being robust to non-Markovian, intransitive, and stochastic preferences.
    - **Supporting Citations:**  Christiano et al. (2017), Akrour et al. (2012), Wirth et al. (2017), Sadigh et al. (2017), Ibarz et al. (2018), Lee et al. (2021b;a), Sikchi et al. (2022), Dudík et al. (2015), Kreweras (1965), Simpson (1969), Kramer (1973), Fishburn (1984), Fey (2012), Nash (1951), Freund & Schapire (1997), Zinkevich (2003), Goodfellow et al. (2014), Yue et al. (2012),  Agranov & Ortoleva (2017), Ziebart (2010),  Haarnoja et al. (2018), Schulman et al. (2017), Hoffman et al. (2020), May (1954), Brockman et al. (2016)
    - **Contribution:** This insight highlights the key contribution of the paper, which is the development of a new algorithm for RLHF that addresses the limitations of existing approaches.
- **Key Insight:** SPO is more sample-efficient than reward-model based approaches, especially in settings with intransitive preferences or non-Markovian reward functions.
    - **Supporting Citations:**  Christiano et al. (2017), Akrour et al. (2012), Wirth et al. (2017), Sadigh et al. (2017), Ibarz et al. (2018), Lee et al. (2021b;a), Sikchi et al. (2022), Rafailov et al. (2023),  Munos et al. (2023),  Chen et al. (2024),  Zhao et al. (2023),  Bai et al. (2022a),  Ouyang et al. (2022),  Ziebart (2010),  Haarnoja et al. (2018), Schulman et al. (2017), Hoffman et al. (2020),  May (1954),  Brockman et al. (2016),  Agranov & Ortoleva (2017)
    - **Contribution:** This insight highlights the practical advantages of SPO over reward-model based approaches, particularly in settings where the assumptions of reward-model based approaches are violated.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:** The authors evaluate SPO on a variety of tasks, including discrete bandit problems, continuous control tasks, and contextual bandit problems. They compare SPO to reward-model based approaches across different preference structures, including intransitive preferences, stochastic preferences, and non-Markovian preferences.
- **Cited Works for Methodology:**
    - **Claim:** We use the PPO implementation in Hoffman et al. (2020) with learning rate 10-4.
    - **Citation:** Hoffman et al. (2020)
    - **Relevance:** This citation provides the basis for the implementation of the PPO algorithm used in the experiments.
    - **Claim:** We use the MuJoCo Gym (Brockman et al., 2016) Ant-v3 environment as the base environment.
    - **Citation:** Brockman et al. (2016)
    - **Relevance:** This citation introduces the MuJoCo Gym environment, which is used for the continuous control experiments.
    - **Claim:** We use Soft Actor Critic (SAC, Haarnoja et al. (2018)) for continuous control and Proximal Policy Optimization (PPO, Schulman et al. (2017)) for discrete action tasks, both as implemented in the ACME framework (Hoffman et al., 2020).
    - **Citation:** Haarnoja et al. (2018), Schulman et al. (2017), Hoffman et al. (2020)
    - **Relevance:** This citation introduces the specific reinforcement learning algorithms used in the experiments.
- **Novel Aspects of Methodology:**
    - **Claim:** SPO is a single-player algorithm for computing Minimax Winners.
    - **Citation:**  Fey (2012), Nash (1951)
    - **Relevance:** This novel aspect of the methodology is justified by the theoretical results presented in the paper.

**5. Results in Context:**

- **Main Results:**
    - SPO consistently computes Minimax Winners across a variety of intransitive preference structures, while reward-model based approaches fail to do so.
    - SPO learns comparable to reward-model based approaches with stochastic preferences, without the burden of an extra model.
    - SPO handles complex non-Markovian preferences, while reward-model based approaches fail to do so.
    - SPO is more sample-efficient than reward-model based approaches across a variety of preference structures, especially in settings with intransitive preferences or non-Markovian reward functions.
- **Comparison with Existing Literature:**
    - **Claim:** SPO is more sample-efficient than reward-model based approaches, especially in settings with intransitive preferences or non-Markovian reward functions.
    - **Citation:**  Christiano et al. (2017), Akrour et al. (2012), Wirth et al. (2017), Sadigh et al. (2017), Ibarz et al. (2018), Lee et al. (2021b;a), Sikchi et al. (2022), Rafailov et al. (2023),  Munos et al. (2023),  Chen et al. (2024),  Zhao et al. (2023),  Bai et al. (2022a),  Ouyang et al. (2022),  Ziebart (2010),  Haarnoja et al. (2018), Schulman et al. (2017), Hoffman et al. (2020),  May (1954),  Brockman et al. (2016),  Agranov & Ortoleva (2017)
    - **Confirmation/Contradiction/Extension:** The authors' results confirm the limitations of reward-model based approaches in settings with intransitive preferences or non-Markovian reward functions, while demonstrating the superior performance of SPO in these settings.

**6. Discussion and Related Work:**

- **Situating Work within Literature:** The authors discuss how their work relates to existing research on dueling bandits, dueling RL, and reward-model-free approaches to RLHF. They highlight the limitations of existing approaches and emphasize the novelty of their approach, SPO, which avoids the need for reward modeling and adversarial training, while being robust to non-Markovian, intransitive, and stochastic preferences.
- **Key Papers Cited:**
    - **Claim:** Beginning with the seminal work of Yue et al. (2012), various authors have viewed preference-based optimization of a multi-armed or contextual bandit as a two-player zero-sum game.
    - **Citation:** Yue et al. (2012), Dudík et al. (2015), Saha et al. (2021), Saha & Krishnamurthy (2022), Bengs et al. (2021)
    - **Relevance:** This citation highlights the key works that have contributed to the development of dueling bandits and dueling RL, which are relevant to the authors' work.
    - **Claim:** Recent work by Chen et al. (2024) formulates inverse RL for LLM fine-tuning as a kind of self-play – we focus on optimizing from preferences rather than from demonstrations.
    - **Citation:** Chen et al. (2024)
    - **Relevance:** This citation highlights a recent work that is related to the authors' approach, but focuses on a different aspect of the problem.
    - **Claim:** Several authors have proposed eliminating reward models from RLHF by leveraging the well-known bijection between the optimal policies of minimum-relative-entropy RL problems and their advantage functions.
    - **Citation:** Ziebart (2010), Zhao et al. (2023), Rafailov et al. (2023), Hejna et al. (2023), Azar et al. (2023)
    - **Relevance:** This citation highlights a recent trend in RLHF research that aims to eliminate the need for reward models. The authors discuss the limitations of these approaches and contrast them with their own approach, SPO.

**7. Future Work and Open Questions:**

- **Areas for Further Research:**
    - The authors suggest exploring the application of SPO to fine-tuning generative models with AI feedback from large models, or using preference models learned from human annotations.
    - They also suggest investigating the computational limitations of preference model-based methods compared to reward model-based methods in the contextual setting.
- **Citations for Future Work:**
    - **Claim:** The authors suggest exploring the application of SPO to fine-tuning generative models with AI feedback from large models, or using preference models learned from human annotations.
    - **Citation:**  Bai et al. (2022b),  Zhu et al. (2023),  Calandriello et al. (2024),  Rosset et al. (2024),  Gao et al. (2024)
    - **Relevance:** These citations highlight recent works that are exploring the use of preference-based methods for fine-tuning large language models, which provides a context for the authors' suggestions for future work.

**8. Critical Analysis of Citation Usage:**

- **Effectiveness of Citation Usage:** The authors effectively use citations to support their arguments and findings. They provide a comprehensive overview of the relevant literature, highlighting key works and discussing their limitations.
- **Areas for Additional Citations:**
    - The authors could have provided more citations for the discussion of compounding errors in RLHF, particularly in the context of language modeling.
- **Potential Biases:**
    - The authors primarily cite works from the field of reinforcement learning, with a limited number of citations from other fields, such as social choice theory or game theory.

**9. Final Summary:**

- **Contribution to the Field:** The paper makes a significant contribution to the field of RLHF by introducing SPO, a novel algorithm that avoids training a reward model and adversarial training, while being robust to non-Markovian, intransitive, and stochastic preferences. SPO is more sample-efficient than reward-model based approaches, especially in settings with intransitive preferences or non-Markovian reward functions.
- **Influential or Frequently Cited Works:**
    - Christiano et al. (2017)
    - Dudík et al. (2015)
    - Ziebart (2010)
    -  Haarnoja et al. (2018)
    - Schulman et al. (2017)
    - Hoffman et al. (2020)
- **Assessment of Literature Integration:** The paper effectively integrates existing literature to support its claims and findings. The authors provide a comprehensive overview of the relevant literature, highlighting key works and discussing their limitations. They also clearly articulate the novelty of their approach, SPO, and its advantages over existing methods.

Overall, this paper presents a valuable contribution to the field of RLHF by introducing a novel algorithm that addresses the limitations of existing approaches. The authors provide a strong theoretical foundation for their approach and demonstrate its effectiveness through a comprehensive set of experiments. The paper is well-written and clearly structured, making it accessible to a broad audience of researchers in the field of reinforcement learning.
