## AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers

**1. Introduction**

- **Title:** AttnLRP: Attention-Aware Layer-Wise Relevance Propagation for Transformers
- **Authors:** Reduan Achtibat, Sayed Mohammad Vakilzadeh Hatefi, Aakriti Jain, Thomas Wiegand, Sebastian Lapuschkin, Maximilian Dreyer, Wojciech Samek
- **Publication Date:** June 10, 2024 (v2)
- **Objective:** To extend the Layer-wise Relevance Propagation (LRP) attribution method to handle attention layers in transformer models, aiming for faithful and computationally efficient explanations of both input and latent representations.
- **Number of References:** 60

**2. Section-by-Section Analysis with Citation Extraction**

**2.1. Introduction**

- **Key Points:**
    - Large Language Models (LLMs) are prone to biased predictions and hallucinations, highlighting the need for understanding their internal reasoning process.
    - Existing attribution methods struggle to achieve faithful attributions for the entire transformer model while maintaining computational efficiency.
    - The paper proposes AttnLRP, a novel method that extends LRP to handle attention layers, addressing these challenges.
- **Significant Citations:**
    - **Claim:** LLMs are prone to biased predictions and hallucinations.
        - **Citation:** Huang et al., 2023. "Large Language Models are Prone to Hallucination." *arXiv preprint arXiv:2307.09288*.
        - **Relevance:** This citation highlights the problem that AttnLRP aims to address, emphasizing the need for understanding LLM reasoning.
    - **Claim:** Existing attribution methods struggle to achieve faithful attributions for the entire transformer model while maintaining computational efficiency.
        - **Citation:** Miglani et al., 2023. "Using Captum to Explain Generative Language Models." *Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023)*, pages 165-173.
        - **Relevance:** This citation provides context for the challenges faced by existing methods, setting the stage for the introduction of AttnLRP.

**2.2. Related Work**

- **Key Points:**
    - The paper reviews existing model-agnostic and transformer-specialized attribution methods, including perturbation, local surrogate, attention-based, and backpropagation-based approaches.
    - It highlights the limitations of each approach, such as high computational cost, limited resolution, and lack of faithfulness.
- **Significant Citations:**
    - **Claim:** Attention maps contain rich information about the data distribution, but lack class specificity and do not provide a meaningful interpretation of the final prediction.
        - **Citation:** Wiegreffe and Pinter, 2019. "Attention is not not explanation." *Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 276–286.
        - **Relevance:** This citation highlights the limitations of using attention maps alone for understanding model behavior, motivating the need for a more comprehensive approach.
    - **Claim:** Input × Gradient (I×G) is highly efficient but suffers from noisy gradients.
        - **Citation:** Simonyan et al., 2014. "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps." *Proceedings of the International Conference on Learning Representations (ICLR)*. ICLR.
        - **Relevance:** This citation introduces a prominent gradient-based method and its limitations, providing a baseline for comparison with AttnLRP.
    - **Claim:** Previous attempts to apply LRP to transformers reused standard LRP rules, leading to numerical instabilities or low faithfulness.
        - **Citation:** Ding et al., 2017. "Visualizing and Understanding Neural Machine Translation." *Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 1150-1159.
        - **Relevance:** This citation highlights the limitations of existing LRP approaches for transformers, setting the stage for the introduction of AttnLRP's novel rules.

**2.3. Attention-Aware LRP for Transformers**

- **Key Points:**
    - The paper motivates LRP within the framework of additive explanatory models.
    - It generalizes the design of new rules for non-linear operations, specifically focusing on attention and normalization layers.
    - The paper presents a detailed derivation of AttnLRP rules for each operation, emphasizing efficiency and faithfulness.
- **Significant Citations:**
    - **Claim:** LRP belongs to the family of additive explanatory models, which includes Shapley, Gradient × Input, and DeepLIFT.
        - **Citation:** Bach et al., 2015. "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation." *PLoS ONE*, 10(7):e0130140.
        - **Relevance:** This citation provides a theoretical foundation for LRP and its relationship to other attribution methods, contextualizing AttnLRP's approach.
    - **Claim:** The Deep Taylor Decomposition framework (Montavon et al., 2017) is used to locally linearize and decompose neural network operations.
        - **Citation:** Montavon et al., 2017. "Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition." *Pattern Recognition*, 65:211-222.
        - **Relevance:** This citation introduces the theoretical framework upon which AttnLRP is built, providing a basis for understanding its derivation.

**2.4. Layer-wise Relevance Propagation**

- **Key Points:**
    - The paper explains the concept of LRP and its conservation property.
    - It presents the Deep Taylor Decomposition framework and its application to LRP.
    - It derives a novel rule for handling the bias term in LRP.
- **Significant Citations:**
    - **Claim:** LRP decomposes a function into individual contributions of single input variables.
        - **Citation:** Bach et al., 2015. "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation." *PLoS ONE*, 10(7):e0130140.
        - **Relevance:** This citation introduces the core concept of LRP and its decomposition property, providing a foundation for the subsequent derivation of specific rules.
    - **Claim:** The Deep Taylor Decomposition framework (Montavon et al., 2017) is used to locally linearize and decompose neural network operations.
        - **Citation:** Montavon et al., 2017. "Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition." *Pattern Recognition*, 65:211-222.
        - **Relevance:** This citation introduces the theoretical framework upon which AttnLRP is built, providing a basis for understanding its derivation.

**2.5. Attributing the Multilayer Perceptron**

- **Key Points:**
    - The paper discusses the application of LRP to multilayer perceptrons (MLPs).
    - It introduces the ɛ-LRP and γ-LRP rules for handling linear and non-linear operations in MLPs.
- **Significant Citations:**
    - **Claim:** The ɛ-LRP rule is used for linear layers in MLPs.
        - **Citation:** Bach et al., 2015. "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation." *PLoS ONE*, 10(7):e0130140.
        - **Relevance:** This citation introduces the ɛ-LRP rule, providing a foundation for its application to MLPs.
    - **Claim:** The γ-LRP rule is used to improve the signal-to-noise ratio in MLPs.
        - **Citation:** Montavon et al., 2019. "Layer-wise Relevance Propagation: An Overview." *Explainable AI: interpreting, explaining and visualizing deep learning*, pages 193-209.
        - **Relevance:** This citation introduces the γ-LRP rule, providing a solution for addressing the noise problem in deep models.

**2.6. Attributing Non-linear Attention**

- **Key Points:**
    - The paper focuses on deriving novel LRP rules for handling the non-linear attention mechanism in transformers.
    - It presents specific rules for the softmax and matrix multiplication operations within the attention layer.
    - It addresses the challenges of bias term handling and conservation property violation in these operations.
- **Significant Citations:**
    - **Claim:** The softmax function is highly non-linear.
        - **Citation:** Vaswani et al., 2017. "Attention is all you need." *Advances in Neural Information Processing Systems*, 30.
        - **Relevance:** This citation highlights the complexity of the softmax function, motivating the need for specific LRP rules.
    - **Claim:** Previous attempts to apply LRP to the softmax function in transformers led to numerical instabilities.
        - **Citation:** Ali et al., 2022. "XAI for Transformers: Better Explanations Through Conservative Propagation." *International Conference on Machine Learning*, pages 435-451. PMLR.
        - **Relevance:** This citation highlights the challenges faced by existing LRP approaches for handling the softmax function, setting the stage for the introduction of AttnLRP's novel rule.

**2.7. Handling the Softmax Non-linearity**

- **Key Points:**
    - The paper proposes a novel rule for handling the softmax function, incorporating a bias term to address the issue of non-zero output even with zero input.
    - It provides a theoretical justification for this rule based on Taylor decomposition.
    - It discusses the implications of vanishing gradients and temperature scaling on attributing the softmax function.
- **Significant Citations:**
    - **Claim:** Previous attempts to apply LRP to the softmax function in transformers led to numerical instabilities.
        - **Citation:** Ali et al., 2022. "XAI for Transformers: Better Explanations Through Conservative Propagation." *International Conference on Machine Learning*, pages 435-451. PMLR.
        - **Relevance:** This citation highlights the challenges faced by existing LRP approaches for handling the softmax function, setting the stage for the introduction of AttnLRP's novel rule.
    - **Claim:** The Deep Taylor Decomposition framework (Montavon et al., 2017) is used to locally linearize and decompose neural network operations.
        - **Citation:** Montavon et al., 2017. "Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition." *Pattern Recognition*, 65:211-222.
        - **Relevance:** This citation introduces the theoretical framework upon which AttnLRP is built, providing a basis for understanding its derivation.

**2.8. Handling Matrix-Multiplication**

- **Key Points:**
    - The paper derives a novel rule for handling matrix multiplication within the attention layer, ensuring conservation property and numerical stability.
    - It decomposes the matrix multiplication into a summation and a bi-linear part, applying separate rules for each.
- **Significant Citations:**
    - **Claim:** Previous attempts to apply LRP to matrix multiplication in transformers led to violations of the conservation property.
        - **Citation:** Ali et al., 2022. "XAI for Transformers: Better Explanations Through Conservative Propagation." *International Conference on Machine Learning*, pages 435-451. PMLR.
        - **Relevance:** This citation highlights the challenges faced by existing LRP approaches for handling matrix multiplication, setting the stage for the introduction of AttnLRP's novel rule.
    - **Claim:** The Deep Taylor Decomposition framework (Montavon et al., 2017) is used to locally linearize and decompose neural network operations.
        - **Citation:** Montavon et al., 2017. "Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition." *Pattern Recognition*, 65:211-222.
        - **Relevance:** This citation introduces the theoretical framework upon which AttnLRP is built, providing a basis for understanding its derivation.

**2.9. Handling Normalization Layers**

- **Key Points:**
    - The paper derives a rule for handling normalization layers (LayerNorm and RMSNorm) in transformers, ensuring conservation property and numerical stability.
    - It demonstrates that the identity rule is sufficient for handling these layers.
- **Significant Citations:**
    - **Claim:** Previous attempts to apply LRP to normalization layers in transformers led to numerical instabilities or violations of the conservation property.
        - **Citation:** Ali et al., 2022. "XAI for Transformers: Better Explanations Through Conservative Propagation." *International Conference on Machine Learning*, pages 435-451. PMLR.
        - **Relevance:** This citation highlights the challenges faced by existing LRP approaches for handling normalization layers, setting the stage for the introduction of AttnLRP's novel rule.
    - **Claim:** The Deep Taylor Decomposition framework (Montavon et al., 2017) is used to locally linearize and decompose neural network operations.
        - **Citation:** Montavon et al., 2017. "Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition." *Pattern Recognition*, 65:211-222.
        - **Relevance:** This citation introduces the theoretical framework upon which AttnLRP is built, providing a basis for understanding its derivation.

**2.10. Understanding Latent Features**

- **Key Points:**
    - The paper proposes a method for understanding latent features in transformers using AttnLRP.
    - It involves identifying relevant neurons and layers, and then using ActMax to find representative reference samples for each neuron.
- **Significant Citations:**
    - **Claim:** Identifying relevant neurons and layers is crucial for understanding the reasoning process of the model.
        - **Citation:** Achtibat et al., 2023. "From Attribution Maps to Human-Understandable Explanations Through Concept Relevance Propagation." *Nature Machine Intelligence*, 5(9):1006–1019.
        - **Relevance:** This citation highlights the importance of understanding latent features, providing a context for the proposed method.
    - **Claim:** Activation Maximization (ActMax) is a common technique for finding representative reference samples for each neuron.
        - **Citation:** Nguyen et al., 2016. "Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks." *Advances in Neural Information Processing Systems*, 29.
        - **Relevance:** This citation introduces ActMax, providing a tool for understanding the concept encoded by each neuron.

**3. Key Insights and Supporting Literature**

- **Key Insight:** AttnLRP outperforms existing methods in terms of faithfulness and computational efficiency, enabling the understanding of latent representations.
    - **Supporting Citations:**
        - **Citation:** Blücher et al., 2024. "Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks." *arXiv preprint arXiv:2401.06654*.
        - **Citation:** Ali et al., 2022. "XAI for Transformers: Better Explanations Through Conservative Propagation." *International Conference on Machine Learning*, pages 435-451. PMLR.
        - **Citation:** Montavon et al., 2019. "Layer-wise Relevance Propagation: An Overview." *Explainable AI: interpreting, explaining and visualizing deep learning*, pages 193-209.
        - **Contribution:** These citations provide a basis for comparing AttnLRP with existing methods, highlighting its advantages in terms of faithfulness and efficiency.
- **Key Insight:** AttnLRP allows for concept-based explanations by identifying relevant neurons and their encodings.
    - **Supporting Citations:**
        - **Citation:** Achtibat et al., 2023. "From Attribution Maps to Human-Understandable Explanations Through Concept Relevance Propagation." *Nature Machine Intelligence*, 5(9):1006–1019.
        - **Citation:** Nguyen et al., 2016. "Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks." *Advances in Neural Information Processing Systems*, 29.
        - **Contribution:** These citations provide a theoretical foundation for understanding the concept encoded by each neuron, enabling concept-based explanations.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The paper evaluates AttnLRP on various models, including ViT-B-16, LLaMa 2-7b, Mixtral 8x7b, and Flan-T5-XL.
    - It uses ImageNet, IMDB movie review, Wikipedia, and SQUAD v2 datasets for evaluation.
    - It employs faithfulness metrics based on perturbation experiments and plausibility metrics based on ground truth masks.
- **Cited Works for Methodology:**
    - **Citation:** Samek et al., 2017. "Evaluating the Visualization of What a Deep Neural Network Has Learned." *IEEE Transactions on Neural Networks and Learning Systems*, 28(11):2660-2673.
    - **Citation:** Blücher et al., 2024. "Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks." *arXiv preprint arXiv:2401.06654*.
    - **Citation:** Rajpurkar et al., 2018. "Know What You Don't Know: Unanswerable Questions for SQuAD." *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)*, pages 784-789.
    - **Contribution:** These citations provide a foundation for the experimental methodology used in the paper, including the choice of datasets, metrics, and perturbation techniques.
- **Novel Aspects of Methodology:**
    - The paper introduces a novel approach for evaluating the faithfulness of attribution methods by quantifying the area between the least and most relevant order perturbation curves.
    - **Citation:** Blücher et al., 2024. "Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks." *arXiv preprint arXiv:2401.06654*.
    - **Justification:** This novel approach addresses the limitations of existing faithfulness metrics, providing a more robust and reliable measure.

**5. Results in Context**

- **Main Results:**
    - AttnLRP consistently outperforms existing methods in terms of faithfulness, particularly in models with a higher number of non-linearities.
    - AttnLRP is computationally efficient, requiring only a single backward pass.
    - AttnLRP enables the understanding of latent representations and concept-based explanations.
- **Comparison with Existing Literature:**
    - **Claim:** AttnLRP outperforms existing methods in terms of faithfulness, particularly in models with a higher number of non-linearities.
        - **Citation:** Ali et al., 2022. "XAI for Transformers: Better Explanations Through Conservative Propagation." *International Conference on Machine Learning*, pages 435-451. PMLR.
        - **Contribution:** This result confirms the findings of previous work on the limitations of CP-LRP in complex models, highlighting the advantage of AttnLRP.
    - **Claim:** AttnLRP is computationally efficient, requiring only a single backward pass.
        - **Citation:** Samek et al., 2017. "Evaluating the Visualization of What a Deep Neural Network Has Learned." *IEEE Transactions on Neural Networks and Learning Systems*, 28(11):2660-2673.
        - **Contribution:** This result highlights the advantage of AttnLRP over perturbation-based methods, which require multiple forward passes.
    - **Claim:** AttnLRP enables the understanding of latent representations and concept-based explanations.
        - **Citation:** Achtibat et al., 2023. "From Attribution Maps to Human-Understandable Explanations Through Concept Relevance Propagation." *Nature Machine Intelligence*, 5(9):1006–1019.
        - **Contribution:** This result extends the capabilities of attribution methods, enabling a deeper understanding of model behavior.

**6. Discussion and Related Work**

- **Key Papers Cited:**
    - **Citation:** Ali et al., 2022. "XAI for Transformers: Better Explanations Through Conservative Propagation." *International Conference on Machine Learning*, pages 435-451. PMLR.
    - **Citation:** Chefer et al., 2021b. "Transformer Interpretability Beyond Attention Visualization." *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 782-791.
    - **Citation:** Montavon et al., 2019. "Layer-wise Relevance Propagation: An Overview." *Explainable AI: interpreting, explaining and visualizing deep learning*, pages 193-209.
    - **Contribution:** These citations are used to highlight the limitations of existing methods and to emphasize the novelty and importance of AttnLRP.
- **Novelty and Importance:**
    - The authors emphasize the novelty of AttnLRP in addressing the limitations of existing methods, particularly in terms of faithfulness, computational efficiency, and the ability to explain latent representations.
    - They highlight the importance of their work for understanding and manipulating transformer models, particularly in critical domains such as healthcare and finance.

**7. Future Work and Open Questions**

- **Areas for Further Research:**
    - Investigating the impact of quantization on attributions and developing custom GPU kernels for LRP rules.
    - Exploring the use of AttnLRP for manipulating transformer models and reducing the impact of specific concepts.
    - Analyzing the effects of temperature scaling on attributing the softmax function.
- **Citations:**
    - **Citation:** Ali et al., 2022. "XAI for Transformers: Better Explanations Through Conservative Propagation." *International Conference on Machine Learning*, pages 435-451. PMLR.
    - **Citation:** Montavon et al., 2019. "Layer-wise Relevance Propagation: An Overview." *Explainable AI: interpreting, explaining and visualizing deep learning*, pages 193-209.
    - **Contribution:** These citations provide a context for the suggested areas of future research, highlighting the potential for further development and application of AttnLRP.

**8. Critical Analysis of Citation Usage**

- **Effectiveness:** The authors effectively use citations to support their arguments and findings, providing a strong foundation for their claims.
- **Areas for Improvement:**
    - The paper could benefit from additional citations to support the claim that AttnLRP is particularly important for critical domains such as healthcare and finance.
    - The paper could also benefit from a more comprehensive discussion of the ethical implications of using AttnLRP for manipulating transformer models.
- **Potential Biases:**
    - The paper primarily cites works from the authors' own research group, which could indicate a potential bias in the selection of cited works.

**9. Final Summary**

- **Contribution:** AttnLRP is a significant contribution to the field of explainable AI for transformers, offering a novel and effective method for understanding and manipulating these complex models.
- **Influential Works:**
    - **Citation:** Montavon et al., 2017. "Explaining Nonlinear Classification Decisions with Deep Taylor Decomposition." *Pattern Recognition*, 65:211-222.
    - **Citation:** Bach et al., 2015. "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation." *PLoS ONE*, 10(7):e0130140.
    - **Citation:** Ali et al., 2022. "XAI for Transformers: Better Explanations Through Conservative Propagation." *International Conference on Machine Learning*, pages 435-451. PMLR.
    - **Contribution:** These works provide the theoretical foundation for LRP and its application to transformers, highlighting the importance of these concepts for understanding and explaining complex models.
- **Integration of Existing Literature:** The paper effectively integrates existing literature to support its claims and findings, providing a comprehensive overview of the challenges and opportunities in explainable AI for transformers. It builds upon previous work on LRP and attention-based methods, highlighting the limitations of these approaches and demonstrating the advantages of AttnLRP.

Overall, the paper presents a valuable contribution to the field of explainable AI for transformers, offering a novel and effective method for understanding and manipulating these complex models. The authors provide a strong theoretical foundation for their approach, supported by extensive experimental results. The paper effectively integrates existing literature, highlighting the limitations of previous work and demonstrating the advantages of AttnLRP. However, the paper could benefit from additional citations to support its claims about the importance of AttnLRP for critical domains and a more comprehensive discussion of the ethical implications of using AttnLRP for manipulating transformer models.
