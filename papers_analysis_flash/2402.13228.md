## Analysis of "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"

**1. Introduction:**

- **Title:** Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive
- **Authors:** Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, Colin White
- **Publication Date:** 3 Jul 2024
- **Objective:** The paper aims to address a failure mode in Direct Preference Optimisation (DPO), a popular method for fine-tuning large language models (LLMs) using preference data. The authors propose a new loss function, DPO-Positive (DPOP), to mitigate this failure mode and improve the performance of DPO.
- **References:** The paper cites a total of 57 references.

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:** The paper introduces DPO as a method for improving LLM performance on downstream tasks like reasoning, summarization, and alignment. It highlights the use of preference data (pairs of preferred and dispreferred completions) in DPO. The authors mention that DPO models the relative probability of picking one response over another, based on a preference-ranking model with an implicit reward function.
- **Significant Citations:**
    - **Claim:** DPO is effective at improving the performance of pretrained LLMs on downstream tasks such as reasoning, summarisation, and alignment.
    - **Citation:** [Wang et al., 2023, Tunstall et al., 2023]
    - **Relevance:** These citations support the claim by providing examples of DPO's successful application in improving LLM performance on specific downstream tasks.
    - **Claim:** The theoretical motivation for DPO is based on a preference-ranking model with an implicit reward function that models the relative probability of picking the preferred completion over the dispreferred.
    - **Citation:** [Rafailov et al., 2023]
    - **Relevance:** This citation provides the theoretical foundation for DPO, explaining the underlying mechanism of the method.

**2.2 Background and Related Work:**

- **Key Points:** The section discusses two main approaches for learning from preference data: Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimisation (DPO). It provides a brief overview of both methods and their applications.
- **Significant Citations:**
    - **Claim:** RLHF uses a dataset of pairwise-preference ranked data to learn a parameterised estimate of a latent reward function.
    - **Citation:** [Ouyang et al., 2022, Bai et al., 2022, Ziegler et al., 2020]
    - **Relevance:** These citations provide examples of RLHF's application in LLM alignment and highlight its use in learning from human feedback.
    - **Claim:** DPO optimizes the same KL-constrained reward function as RLHF without having to learn an explicit reward function.
    - **Citation:** [Rafailov et al., 2023]
    - **Relevance:** This citation introduces DPO and contrasts it with RLHF, highlighting its key advantage of not requiring an explicit reward function.

**2.3 Failure Mode of DPO:**

- **Key Points:** The section presents a theoretical analysis of DPO's failure mode, showing that the standard DPO loss can lead to a reduction in the model's likelihood of the preferred completions. This occurs when the edit distance between preferred and dispreferred completions is small, especially when the differing tokens are located at the beginning of the sequences.
- **Significant Citations:**
    - **Claim:** The standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases.
    - **Citation:** [Rafailov et al., 2023]
    - **Relevance:** This citation provides the theoretical foundation for DPO, explaining the underlying mechanism of the method.
    - **Claim:** The DPO loss function decreases the probability of producing dispreferred completions at a faster rate than it increases the probability of producing preferred completions.
    - **Citation:** [Feng et al., 2024]
    - **Relevance:** This citation supports the authors' analysis of DPO's failure mode, providing further evidence for the phenomenon.

**2.4 DPOP:**

- **Key Points:** The section introduces DPO-Positive (DPOP), a new loss function that addresses the failure mode of DPO. DPOP adds a penalty term to the loss function that incentivizes maintaining a high log-likelihood of the preferred completions. This penalty term is 0 when the ratio of log-probabilities of preferred to dispreferred completions is greater than or equal to 1, and increases as the ratio goes below 1.
- **Significant Citations:**
    - **Claim:** The DPOP loss function mitigates the failure mode of DPO by ensuring that the model cannot minimize the loss by significantly reducing the log-likelihood of the dispreferred examples more than it reduces the log-likelihood of the preferred examples.
    - **Citation:** [Rafailov et al., 2023]
    - **Relevance:** This citation provides the theoretical foundation for DPO, explaining the underlying mechanism of the method.

**2.5 DPOP Datasets & Experiments:**

- **Key Points:** The section describes the creation of new preference-based datasets for ARC, HellaSwag, and MetaMath. It presents empirical results comparing the performance of DPOP with DPO, IPO, and SLiC on these datasets. The results show that DPOP consistently outperforms other methods, especially on datasets with low edit distances.
- **Significant Citations:**
    - **Claim:** The authors create new preference-based versions of ARC, HellaSwag, and MetaMath.
    - **Citation:** [Clark et al., 2018, Zellers et al., 2019, Yu et al., 2023]
    - **Relevance:** These citations provide the source of the original datasets used for creating the new preference-based versions.
    - **Claim:** DPOP outperforms DPO, IPO, and SLiC on both MetaMath and ARC.
    - **Citation:** [Gao et al., 2021]
    - **Relevance:** This citation provides the methodology for evaluating the performance of the models on the datasets.

**2.6 Smaug:**

- **Key Points:** The section introduces the Smaug series of models, trained using DPOP on the new preference-based datasets. The authors present results showing that Smaug-72B achieves an average accuracy of 80.48% on the HuggingFace Open LLM Leaderboard, becoming the first open-source LLM to surpass an average accuracy of 80%. They also compare Smaug-72B with other top open-source models on MT-Bench and find that it achieves the top MMLU score and third-best MT-bench score.
- **Significant Citations:**
    - **Claim:** Smaug-72B achieves an average accuracy of 80.48% on the HuggingFace Open LLM Leaderboard, becoming the first open-source LLM to surpass an average accuracy of 80%.
    - **Citation:** [Beeching et al., 2023, Gao et al., 2021]
    - **Relevance:** These citations provide the source of the HuggingFace Open LLM Leaderboard and its evaluation methodology.
    - **Claim:** Smaug-72B achieves the top MMLU score and third-best MT-bench score out of the open-source models.
    - **Citation:** [Zheng et al., 2023]
    - **Relevance:** This citation provides the source of the MT-Bench benchmark and its evaluation methodology.

**2.7 Contamination check:**

- **Key Points:** The section addresses the issue of data contamination in LLM training and evaluation. The authors use an open-source contamination checker to compare the contamination levels of Smaug-72B with other open-source models on ARC, TruthfulQA, and GSM8K. They find that Smaug-72B achieves scores similar to MoMo-72B-lora-1.8.7-DPO and Llama-2-70B.
- **Significant Citations:**
    - **Claim:** Data contamination remains notoriously challenging to measure and mitigate.
    - **Citation:** [Roberts et al., 2024, Jain et al., 2024, bench authors, 2023]
    - **Relevance:** These citations highlight the challenges associated with data contamination in LLM training and evaluation.
    - **Claim:** The authors use an open-source contamination checker to compare the contamination levels of Smaug-72B with other open-source models.
    - **Citation:** [Shi, 2023]
    - **Relevance:** This citation provides the source of the contamination checker used in the analysis.

**2.8 Conclusions and Limitations:**

- **Key Points:** The section summarizes the paper's main findings, highlighting the discovery of a failure mode in DPO and the development of DPOP as a solution. The authors discuss the potential of DPOP for further advancing LLMs, especially in mathematical reasoning and specific downstream tasks. They also acknowledge the limitations of their work, including the need for further research on larger models and non-English datasets.
- **Significant Citations:**
    - **Claim:** DPOP overcomes the failure mode of DPO and can outperform DPO even outside this failure mode.
    - **Citation:** [Pang et al., 2024, Feng et al., 2024, Rafailov et al., 2024]
    - **Relevance:** These citations provide evidence for the effectiveness of DPOP in addressing the failure mode of DPO and its potential for improving LLM performance.
    - **Claim:** Using DPOP on additional mathematical datasets is an exciting area for future work, as it has the potential to further advance LLMs' abilities in mathematical reasoning.
    - **Citation:** [OpenAI, 2023]
    - **Relevance:** This citation highlights the potential of DPOP for improving LLM performance on specific downstream tasks, drawing a comparison with the capabilities of GPT-4.

**2.9 Related Work Continued:**

- **Key Points:** The appendix provides further discussion of related work, focusing on methods like Alignment Fine-Tuning (AFT) and Human-Centred Loss Functions (HALOs). It highlights the differences between these methods and DPOP, emphasizing the unique contributions of DPOP.
- **Significant Citations:**
    - **Claim:** AFT seeks to align LLMs to correctly 'score' (in terms of perplexity) their own generations.
    - **Citation:** [Wang et al., 2023]
    - **Relevance:** This citation introduces AFT and its objective of aligning LLMs to correctly score their own generations.
    - **Claim:** HALO seeks to understand alignment methods, including DPO, in the context of 'Human-Centred Loss Functions (HALOs)'.
    - **Citation:** [Ethayarajh et al., 2023]
    - **Relevance:** This citation introduces HALOs and its approach to understanding alignment methods, including DPO, in the context of human-centered loss functions.

**2.10 Derivation of logit gradients:**

- **Key Points:** The appendix provides a detailed mathematical derivation of the gradients for DPO and DPOP with respect to the logits. This derivation helps to understand the theoretical basis of the failure mode of DPO and how DPOP addresses it.
- **Significant Citations:**
    - **Claim:** The standard DPO loss can lead to a reduction of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases.
    - **Citation:** [Rafailov et al., 2023]
    - **Relevance:** This citation provides the theoretical foundation for DPO, explaining the underlying mechanism of the method.

**2.11 Motivation: Contrastive Loss:**

- **Key Points:** The appendix discusses the connection between DPOP and contrastive loss, a popular technique in embedding learning. It shows that DPOP can be viewed as a refined version of contrastive loss, addressing its shortcomings by incorporating the missing similar points term and margin.
- **Significant Citations:**
    - **Claim:** Contrastive learning is widely used [Wang and Liu, 2021, Wang and Isola, 2020, Saunshi et al., 2019, Oord et al., 2018, Chen et al., 2020, He et al., 2020], often for embedding learning applications.
    - **Citation:** [Wang and Liu, 2021, Wang and Isola, 2020, Saunshi et al., 2019, Oord et al., 2018, Chen et al., 2020, He et al., 2020]
    - **Relevance:** These citations provide examples of contrastive loss's application in embedding learning and highlight its key components.
    - **Claim:** DPOP fixes the shortcomings of contrastive training when one constituent term is absent by adding the absent term and the margin.
    - **Citation:** [Hadsell et al., 2006]
    - **Relevance:** This citation introduces contrastive loss and its key components, providing a framework for understanding DPOP's connection to contrastive loss.

**2.12 Details of Datasets:**

- **Key Points:** The appendix provides detailed descriptions of the three new preference-based datasets created for the paper: MetaMath, ARC, and HellaSwag. It explains the process of creating these datasets and highlights their key characteristics, including edit distance and intended use.
- **Significant Citations:**
    - **Claim:** The authors create new preference-based versions of ARC, HellaSwag, and MetaMath.
    - **Citation:** [Clark et al., 2018, Zellers et al., 2019, Yu et al., 2023]
    - **Relevance:** These citations provide the source of the original datasets used for creating the new preference-based versions.

**2.13 Additional Experiments and Details:**

- **Key Points:** The appendix provides additional details about the training process for the Smaug models, including hyperparameter choices and model licenses. It also presents additional results, such as the log-probabilities of preferred completions during training on MetaMath and ARC.
- **Significant Citations:**
    - **Claim:** The authors chose β = 0.3, similar to prior work [Rafailov et al., 2023], and they chose λ = 50 without trying other values.
    - **Citation:** [Rafailov et al., 2023]
    - **Relevance:** This citation provides the source of the hyperparameter choice for β, highlighting the authors' approach to hyperparameter tuning.

**2.14 Example Completions:**

- **Key Points:** The appendix provides examples of Smaug-72B completions for questions in MT-Bench, showcasing the model's capabilities in different categories of LLM performance.
- **Significant Citations:**
    - **Claim:** The authors provide examples of Smaug-72B completions for questions in MT-Bench.
    - **Citation:** [Zheng et al., 2023]
    - **Relevance:** This citation provides the source of the MT-Bench benchmark, highlighting the context for the provided examples.

**3. Key Insights and Supporting Literature:**

- **Insight:** DPO can suffer from a failure mode where it reduces the likelihood of preferred completions, especially when the edit distance between preferred and dispreferred completions is small.
    - **Supporting Citations:** [Rafailov et al., 2023, Feng et al., 2024]
    - **Contribution:** These citations provide theoretical and empirical evidence for the failure mode of DPO, highlighting its potential limitations.
- **Insight:** DPOP, a new loss function proposed by the authors, effectively mitigates the failure mode of DPO and improves its performance.
    - **Supporting Citations:** [Rafailov et al., 2023, Pang et al., 2024]
    - **Contribution:** These citations provide the theoretical foundation for DPOP and demonstrate its effectiveness in addressing the failure mode of DPO.
- **Insight:** Smaug-72B, a model trained using DPOP, achieves an average accuracy of 80.48% on the HuggingFace Open LLM Leaderboard, becoming the first open-source LLM to surpass an average accuracy of 80%.
    - **Supporting Citations:** [Beeching et al., 2023, Gao et al., 2021]
    - **Contribution:** These citations provide the context for Smaug-72B's achievement, highlighting its significance in the field of open-source LLMs.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:** The authors fine-tune various LLM models (7B, 34B, and 72B) using DPO and DPOP on their newly created preference-based datasets (MetaMath, ARC, and HellaSwag). They evaluate the performance of these models on multiple benchmarks, including the HuggingFace Open LLM Leaderboard and MT-Bench.
- **Methodology Foundations:**
    - **DPO:** [Rafailov et al., 2023]
    - **HuggingFace Open LLM Leaderboard:** [Beeching et al., 2023, Gao et al., 2021]
    - **MT-Bench:** [Zheng et al., 2023]
- **Novel Aspects:** The authors introduce DPOP as a novel loss function to address the failure mode of DPO. They also create new preference-based datasets for ARC, HellaSwag, and MetaMath.
    - **Justification:** The authors provide theoretical and empirical evidence to justify the need for DPOP and the creation of new datasets.

**5. Results in Context:**

- **Main Results:**
    - DPOP consistently outperforms DPO, IPO, and SLiC on both MetaMath and ARC, especially on datasets with low edit distances.
    - Smaug-72B, trained using DPOP, achieves an average accuracy of 80.48% on the HuggingFace Open LLM Leaderboard, becoming the first open-source LLM to surpass an average accuracy of 80%.
    - Smaug-72B achieves the top MMLU score and third-best MT-bench score out of the open-source models.
- **Comparison with Existing Literature:**
    - The authors compare the performance of Smaug-72B with other top open-source models on the HuggingFace Open LLM Leaderboard and MT-Bench, highlighting its competitive performance.
    - The authors compare the contamination levels of Smaug-72B with other open-source models on ARC, TruthfulQA, and GSM8K, finding that it achieves scores similar to MoMo-72B-lora-1.8.7-DPO and Llama-2-70B.
- **Confirmation, Contradiction, or Extension:**
    - The authors' results confirm the existence of the failure mode in DPO, as previously reported by [Rafailov et al., 2023, Feng et al., 2024].
    - The authors' results demonstrate the effectiveness of DPOP in mitigating the failure mode of DPO and improving its performance, extending the findings of [Pang et al., 2024].
    - The authors' results showcase the potential of DPOP for further advancing LLMs, especially in mathematical reasoning and specific downstream tasks, extending the findings of [OpenAI, 2023].

**6. Discussion and Related Work:**

- **Situating the Work:** The authors situate their work within the existing literature by discussing related methods like AFT and HALOs. They highlight the differences between these methods and DPOP, emphasizing the unique contributions of DPOP.
- **Key Papers Cited:**
    - [Wang et al., 2023]: This paper introduces AFT and its objective of aligning LLMs to correctly score their own generations.
    - [Ethayarajh et al., 2023]: This paper introduces HALOs and its approach to understanding alignment methods, including DPO, in the context of human-centered loss functions.
- **Novelty and Importance:** The authors emphasize the novelty of DPOP in addressing the failure mode of DPO and its potential for improving LLM performance. They highlight the importance of their work in advancing the field of open-source LLMs.

**7. Future Work and Open Questions:**

- **Areas for Further Research:**
    - The authors suggest further research on larger models (beyond 72B) and non-English datasets to verify the effectiveness of DPOP.
    - They also suggest exploring the potential of DPOP for improving LLM performance on specific downstream tasks, especially in mathematical reasoning.
- **Citations:**
    - [OpenAI, 2023]: This citation highlights the potential of DPOP for improving LLM performance on specific downstream tasks, drawing a comparison with the capabilities of GPT-4.

**8. Critical Analysis of Citation Usage:**

- **Effectiveness:** The authors effectively use citations to support their arguments and findings. They provide relevant citations to support their claims about DPO, RLHF, and other related methods. They also cite relevant works to justify their methodology and experimental setup.
- **Areas for Improvement:**
    - The authors could have provided more citations to support their claims about the connection between DPOP and contrastive loss.
    - They could have also provided more citations to support their claims about the potential of DPOP for further advancing LLMs, especially in mathematical reasoning and specific downstream tasks.
- **Potential Biases:**
    - The authors primarily cite works from OpenAI, Google, and Microsoft, potentially reflecting a bias towards these organizations.
    - They could have included more citations from other research groups working on preference-based learning and LLM alignment.

**9. Final Summary:**

- **Contribution:** The paper makes a significant contribution to the field of LLM alignment by identifying a failure mode in DPO and proposing a new loss function, DPOP, to address it. The authors demonstrate the effectiveness of DPOP in improving LLM performance and showcase its potential for further advancing LLMs, especially in mathematical reasoning and specific downstream tasks.
- **Influential Works:**
    - [Rafailov et al., 2023]: This paper introduces DPO and provides the theoretical foundation for the authors' work.
    - [Beeching et al., 2023, Gao et al., 2021]: These papers provide the context for the HuggingFace Open LLM Leaderboard and its evaluation methodology.
    - [Zheng et al., 2023]: This paper introduces MT-Bench and its evaluation methodology.
- **Integration of Existing Literature:** The paper effectively integrates existing literature to support its claims and findings. It provides a comprehensive overview of related methods like RLHF, AFT, and HALOs, highlighting the differences between these methods and DPOP. The authors also cite relevant works to justify their methodology and experimental setup.

Overall, the paper presents a valuable contribution to the field of LLM alignment by identifying a critical failure mode in DPO and proposing a novel solution, DPOP. The authors provide strong theoretical and empirical evidence to support their claims, demonstrating the effectiveness of DPOP in improving LLM performance. The paper effectively integrates existing literature to support its arguments and findings, providing a comprehensive overview of the research landscape. However, the authors could have included more citations to support their claims about the connection between DPOP and contrastive loss and the potential of DPOP for further advancing LLMs. Additionally, they could have included more citations from other research groups working on preference-based learning and LLM alignment to mitigate potential biases in their citation selection.
