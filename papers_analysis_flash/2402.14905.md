## MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases

**1. Introduction:**

- **Title:** MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases
- **Authors:** Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra
- **Publication Date:** 2024 (Proceedings of the 41st International Conference on Machine Learning)
- **Objective:** The paper aims to design efficient and high-quality large language models (LLMs) with fewer than a billion parameters, suitable for deployment on mobile devices.
- **Number of References:** 58

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1. Introduction:**

- **Key Points:**
    - The paper addresses the growing need for efficient LLMs on mobile devices due to increasing cloud costs and latency concerns.
    - It challenges the prevailing belief that data and parameter quantity are the primary factors determining model quality, emphasizing the importance of model architecture for sub-billion scale LLMs.
- **Significant Citations:**
    - **Claim:** "Leading models such as ChatGPT4 exceed 1 trillion parameters."
        - **Citation:** [¹] "https://the-decoder.com/gpt-4-has-a-trillion-parameters"
        - **Relevance:** This citation provides evidence for the increasing size and computational demands of LLMs, highlighting the need for smaller models for mobile deployment.
    - **Claim:** "This computation scale, excluding communication and data transfer, entails the deployment of around one hundred million H100 GPUs, each capable of 60 TFLOPs/s."
        - **Citation:** [²] "Detailed calculation can be found in the appendix."
        - **Relevance:** This citation supports the claim about the massive computational resources required for large LLMs, further emphasizing the need for smaller models.
    - **Claim:** "By utilizing a sub-billion model, such as a 350M 8-bit model consuming only 0.035 J/token, an iPhone can support conversational use an entire day."
        - **Citation:** [⁵] "https://llm.mlc.ai"
        - **Relevance:** This citation provides a concrete example of the potential benefits of using smaller models for on-device applications, highlighting the improved performance and energy efficiency compared to larger models.

**2.2. Improving Sub-billion Scale LLM Design:**

- **Key Points:**
    - The authors present a series of design choices for building efficient sub-billion scale LLMs, focusing on both 125M and 350M models.
    - They introduce a strong baseline model, MobileLLM, and further enhance it with a layer-sharing approach, resulting in MobileLLM-LS.
- **Significant Citations:**
    - **Claim:** "Contradictory to the scaling law (Kaplan et al., 2020), we demonstrate that depth is more important than width for small LLMs."
        - **Citation:** (Kaplan et al., 2020)
        - **Relevance:** This citation introduces the scaling law, which suggests that model performance is primarily determined by the number of parameters, data size, and training iterations. The authors challenge this notion by demonstrating the importance of depth for smaller models.
    - **Claim:** "We revisit embedding sharing methods (Zhang et al., 2022) and implement grouped query attention (Ainslie et al., 2023) in small LLMs to maximize weight utilization."
        - **Citation:** (Zhang et al., 2022), (Ainslie et al., 2023)
        - **Relevance:** These citations highlight the authors' use of existing techniques for weight sharing and grouped query attention, demonstrating their understanding of the current state-of-the-art in model design.
    - **Claim:** "We propose a new family of models, MobileLLM, showcasing SOTA performance."
        - **Citation:** None
        - **Relevance:** This claim introduces the authors' novel contribution, the MobileLLM model family, which is a key focus of the paper.

**2.3. Training Setup:**

- **Key Points:**
    - The authors describe their training setup, including the hardware used (32 A100 GPUs), batch size, and training iterations.
    - They mention the datasets used for evaluation, including zero-shot common sense reasoning tasks, question answering, and reading comprehension.
- **Significant Citations:**
    - **Claim:** "We evaluate the pre-trained model on zero-shot common sense reasoning tasks, including ARC-easy, ARC-challenge (Clark et al., 2018), BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), OBQA (Mihaylov et al., 2018), WinoGrande (Sakaguchi et al., 2021), as well as question answering and reading comprehension tasks using TQA (Joshi et al., 2017) and RACE dataset (Lai et al., 2017)."
        - **Citation:** (Clark et al., 2018), (Clark et al., 2019), (Bisk et al., 2020), (Sap et al., 2019), (Zellers et al., 2019), (Mihaylov et al., 2018), (Sakaguchi et al., 2021), (Joshi et al., 2017), (Lai et al., 2017)
        - **Relevance:** This citation provides a comprehensive list of the datasets used for evaluation, demonstrating the authors' thoroughness in assessing the performance of their models.

**2.4. Building a Strong Baseline:**

- **Key Points:**
    - The authors discuss their choices for the feed-forward network (FFN) and investigate the impact of model depth versus width.
    - They highlight the importance of embedding sharing and grouped query attention for sub-billion scale LLMs.
- **Significant Citations:**
    - **Claim:** "We first investigate activation functions commonly used in feed-forward networks (FFNs) and find that the state-of-the-art SwiGLU (Dauphin et al., 2017) is also beneficial for small models."
        - **Citation:** (Dauphin et al., 2017)
        - **Relevance:** This citation highlights the authors' use of a well-established technique, SwiGLU, for improving FFN performance in their models.
    - **Claim:** "A prevalent belief (Kaplan et al., 2020) in the field suggests that the performance of transformer models is primarily determined by the number of parameters, the size of the training dataset, and the number of training iterations."
        - **Citation:** (Kaplan et al., 2020)
        - **Relevance:** This citation introduces the prevailing belief about the scaling law, which the authors challenge in their investigation of depth versus width.
    - **Claim:** "We revisit the concept of input-output embedding sharing. The input embedding in LLM models maps the token ID in the vocabulary to the corresponding token embedding and has a dimension of (vocab_size, embedding_dim)."
        - **Citation:** (Zhang et al., 2022)
        - **Relevance:** This citation highlights the authors' understanding of existing techniques for embedding sharing, which they revisit and apply to their models.
    - **Claim:** "We experiment on a 30-layer 125M model. In Table 1, we demonstrate that sharing the input and output embeddings reduces the number of parameters by 16M, approximately 11.8% of total parameters with a 0.2 points drop in average accuracy."
        - **Citation:** None
        - **Relevance:** This claim presents the authors' experimental results on the effectiveness of embedding sharing, demonstrating its potential for reducing model size while maintaining reasonable accuracy.
    - **Claim:** "In sub-billion scale language models, the embedding layers constitute a significant portion of the parameter count. For instance, with an embedding dimension of 512 and a vocabulary size of 32k, the input and output embedding layers each comprise 16 million parameters."
        - **Citation:** None
        - **Relevance:** This claim provides context for the importance of embedding sharing in sub-billion scale models, highlighting the significant proportion of parameters dedicated to embedding layers.
    - **Claim:** "Grouped query attention (GQA) and meanwhile increasing the embedding dimension to maintain the model size, the accuracy of 125M further increases by 0.4 points, indicating GQA as a favorable method to further squeeze out small model's potential."
        - **Citation:** (Chowdhery et al., 2023), (Ainslie et al., 2023)
        - **Relevance:** This citation highlights the authors' use of grouped query attention, a technique designed for reducing key-value cache size in LLMs, to further improve the performance of their models.

**2.5. Layer Sharing:**

- **Key Points:**
    - The authors propose a layer-sharing approach to increase the number of hidden layers without increasing model size.
    - They compare different layer-sharing strategies and choose immediate block-wise sharing as the most effective approach.
- **Significant Citations:**
    - **Claim:** "The findings in Section 2.2.2 on the impact of layer depth versus width suggest deeper layers are favorable for small transformer models."
        - **Citation:** None
        - **Relevance:** This claim connects the authors' previous findings about the importance of depth to their motivation for exploring layer sharing.
    - **Claim:** "This approach is particularly helpful in on-device scenarios where model size is a major constraint."
        - **Citation:** None
        - **Relevance:** This claim highlights the practical relevance of layer sharing for on-device applications, where model size is a critical factor.
    - **Claim:** "We have opted for the immediate block-wise sharing strategy in our model design. We denote the proposed model with layer sharing as MobileLLM-LS."
        - **Citation:** None
        - **Relevance:** This claim introduces the authors' final model, MobileLLM-LS, which incorporates layer sharing for improved performance.

**3. Experiments:**

- **Key Points:**
    - The authors describe their experimental setup, including the training settings, evaluation metrics, and datasets used.
    - They present the main results of their experiments, comparing MobileLLM and MobileLLM-LS to other sub-billion scale models.
- **Significant Citations:**
    - **Claim:** "We train MobileLLM from scratch using Adam optimizer (Kingma & Ba, 2014) with a weight decay of 0.1."
        - **Citation:** (Kingma & Ba, 2014)
        - **Relevance:** This citation highlights the authors' use of a well-established optimization algorithm, Adam, for training their models.
    - **Claim:** "We compare the final performance on zero-shot common sense reasoning tasks, question answering, and reading comprehension tasks. The results of baseline methods were evaluated using their open-source Hugging Face models to ensure consistent evaluation procedures."
        - **Citation:** None
        - **Relevance:** This claim describes the authors' approach to evaluating their models, including the use of open-source models for comparison.

**4. Results in Context:**

- **Key Points:**
    - MobileLLM and MobileLLM-LS consistently outperform previous sub-billion scale models on zero-shot common sense reasoning tasks, question answering, and reading comprehension.
    - MobileLLM-1.5B achieves an average accuracy of 59.4 points on zero-shot commonsense reasoning tasks, outperforming the previous state-of-the-art model, Qwen1.5-1.8B, by 2.9 points despite having fewer parameters.
- **Significant Citations:**
    - **Claim:** "MobileLLM-1.5B achieves an average accuracy of 59.4 points on zero-shot commonsense reasoning tasks, outperforming the previous state-of-the-art model, Qwen1.5-1.8B, by 2.9 points despite having fewer parameters."
        - **Citation:** (Zhang et al., 2022), (Scao et al., 2022), (Black et al., 2022), (Dey et al., 2023), (Taylor et al., 2022), (Timiryasov & Tastet, 2023), (Biderman et al., 2023), (Peng et al., 2023), (Wu et al., 2023), (Bai et al., 2023), (Thawakar et al., 2024)
        - **Relevance:** This citation highlights the authors' comparison of their model to other sub-billion scale models, demonstrating the superiority of MobileLLM.

**5. Discussion and Related Work:**

- **Key Points:**
    - The authors discuss the limitations of existing model compression techniques, such as pruning, sparsity, and quantization, for sub-billion scale LLMs.
    - They highlight the novelty of their approach, focusing on model architecture design and weight sharing for improving performance and efficiency.
    - They compare their work to other studies on small model design, neural architecture search, and weight sharing.
- **Significant Citations:**
    - **Claim:** "Numerous model compression methods are developed for LLMs, including pruning(Xia et al., 2023b), sparsity (Sun et al., 2023; Xia et al., 2023a; Frantar & Alistarh, 2023), and quantization (Liu et al., 2023a; Dettmers et al., 2022; Kim et al., 2023; Frantar et al., 2022; Xiao et al., 2023; Yao et al., 2022; Liu et al., 2023c;b; Frantar et al., 2022). Our research is complementary to these techniques."
        - **Citation:** (Xia et al., 2023b), (Sun et al., 2023), (Xia et al., 2023a), (Frantar & Alistarh, 2023), (Liu et al., 2023a), (Dettmers et al., 2022), (Kim et al., 2023), (Frantar et al., 2022), (Xiao et al., 2023), (Yao et al., 2022), (Liu et al., 2023c;b), (Frantar et al., 2022)
        - **Relevance:** This citation provides a comprehensive overview of existing model compression techniques, highlighting the authors' understanding of the current state-of-the-art.
    - **Claim:** "A limited number of studies have explored compact model architectures, such as TinyLLaMA (Timiryasov & Tastet, 2023). However, even the smallest TinyLLaMA exceeds 1 billion parameters, making them still prohibitive for many on-device applications."
        - **Citation:** (Timiryasov & Tastet, 2023)
        - **Relevance:** This citation highlights the limitations of existing small model architectures, emphasizing the need for models with fewer parameters for on-device applications.
    - **Claim:** "Our current investigation, focusing on the interplay between depth and width, can be conceptualized as a meticulous grid search within the depth space. The outcomes of that study challenge the prevalent orthodoxy surrounding scaling laws, proposing that deep and thin architectures demonstrate higher performance for compact LLMs."
        - **Citation:** (Kaplan et al., 2020)
        - **Relevance:** This citation highlights the authors' challenge to the prevailing belief about scaling laws, demonstrating the importance of their findings about depth versus width for sub-billion scale LLMs.
    - **Claim:** "While the OPT family (Zhang et al., 2022) and subsequent works (Black et al., 2022) leverage weight sharing between input and output embeddings, limited research has explored weight sharing for intermediate layers in transformers (Shen et al., 2022; Reid et al., 2021)."
        - **Citation:** (Zhang et al., 2022), (Black et al., 2022), (Shen et al., 2022), (Reid et al., 2021)
        - **Relevance:** This citation highlights the authors' contribution to the field of weight sharing, demonstrating the novelty of their approach for applying layer sharing to intermediate layers in transformers.

**6. Future Work and Open Questions:**

- **Key Points:**
    - The authors suggest exploring the use of knowledge distillation for further improving the performance of their models.
    - They acknowledge the limitations of their current approach, such as the increased training time associated with knowledge distillation.
- **Significant Citations:**
    - **Claim:** "The results of integrating knowledge distillation (KD) (Hinton et al., 2015) into small model pre-training are presented in Table 16. LLaMA-v2 7B models serve as the teacher, and the KD loss is computed using cross-entropy between the logits from the large pre-trained teacher model (i.e., LLaMA-v2 7B) and the small student network (i.e., 125M or 350M models)."
        - **Citation:** (Hinton et al., 2015)
        - **Relevance:** This citation highlights the authors' interest in exploring knowledge distillation as a potential technique for further improving their models.

**7. Critical Analysis of Citation Usage:**

- **Effectiveness:** The authors effectively use citations to support their arguments and findings, providing a strong foundation for their claims.
- **Areas for Improvement:**
    - While the authors cite a wide range of relevant works, they could have provided more specific citations for some of their claims, particularly in the discussion and related work section.
    - The authors could have included more citations from recent works on sub-billion scale LLMs, particularly those focusing on model architecture design and weight sharing.
- **Potential Biases:**
    - The authors primarily cite works from major research labs, such as Meta, Google, and OpenAI, which may reflect a bias towards these institutions.
    - The authors could have included more citations from independent researchers and smaller labs working on sub-billion scale LLMs.

**8. Final Summary:**

- **Contribution:** The paper makes a significant contribution to the field of efficient LLM design, demonstrating the effectiveness of using deep and thin architectures, embedding sharing, grouped query attention, and layer sharing for building high-quality sub-billion scale LLMs suitable for on-device applications.
- **Influential Works:** (Kaplan et al., 2020), (Zhang et al., 2022), (Ainslie et al., 2023), (Kingma & Ba, 2014), (Hinton et al., 2015)
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings, providing a strong foundation for its arguments and demonstrating a thorough understanding of the current state-of-the-art in the field.

This analysis provides a comprehensive overview of the paper's content, highlighting its key findings, supporting citations, and its place within the broader research context. It also identifies potential areas for improvement in the authors' citation usage and highlights potential biases in their selection of cited works. This information can be valuable for readers seeking to understand the paper's contribution to the field and its relationship to existing literature.
