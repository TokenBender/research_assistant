## Keyformer: KV Cache Reduction Through Key Tokens Selection for Efficient Generative Inference

**1. Introduction**

- **Title:** Keyformer: KV Cache Reduction Through Key Tokens Selection for Efficient Generative Inference
- **Authors:** Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant J. Nair, Ilya Soloveychik, Purushotham Kamath
- **Publication Date:** 2024 (arXiv preprint)
- **Objective:** To mitigate the challenges associated with KV cache size in generative language models by identifying and retaining only "key" tokens, thereby reducing memory bandwidth usage and improving inference latency and throughput.
- **Total References:** 54

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Point:** Transformers are the dominant architecture for LLMs, but inference latency and throughput are hindered by the sequential autoregressive nature of token generation, especially when handling long contexts.
    - **Citation:** (Lewis et al., 2019; Brown et al., 2020; Raffel et al., 2020; Doso-vitskiy et al., 2020; Sun et al., 2019; de Souza Pereira Moreira et al., 2021; Adnan et al., 2023; Zhao et al., 2023; Bai et al., 2023; Li et al., 2023; Chen et al., 2023; Huang et al., 2021a)
    - **Relevance:** This citation establishes the context of LLMs and their growing importance, while highlighting the challenges associated with inference efficiency.
- **Key Point:** The paper introduces "Keyformer," an inference-time approach that leverages the observation that a significant portion of attention weight focuses on a subset of "key" tokens.
    - **Citation:** (Vaswani et al., 2017; Sukhbaatar et al., 2019; Dao et al., 2022; Choromanski et al., 2020; Ott et al., 2019)
    - **Relevance:** This citation explains the role of the attention mechanism in transformers and the need for KV cache to mitigate computational overhead.
- **Key Point:** Keyformer identifies key tokens using a novel score function and retains only these tokens in the KV cache, reducing both KV cache size and memory bandwidth usage without compromising accuracy.
    - **Citation:** None
    - **Relevance:** This is a novel contribution of the paper, not directly supported by existing citations.

**2.2 Related Work**

- **Key Point:** Previous work has focused on mitigating attention mechanism's memory and computation requirements for longer sequences, but often overlooks the challenge of expanding KV cache size.
    - **Citation:** (Zaheer et al., 2020; Kitaev, 2020; Wang et al., 2020; Beltagy et al., 2020; Sheng et al., 2023; Dao et al., 2022; Kwon et al., 2023; Pope et al., 2023; Shazeer, 2019; Ainslie et al., 2023)
    - **Relevance:** This citation provides a background on existing approaches to address LLM efficiency, highlighting the limitations of these methods and setting the stage for Keyformer's novel approach.
- **Key Point:** Existing techniques for KV cache reduction often require resource-intensive model retraining or fine-tuning, which is not feasible in many deployment scenarios.
    - **Citation:** (Shazeer, 2019; Ainslie et al., 2023)
    - **Relevance:** This citation emphasizes the need for inference-time techniques that do not require model retraining, highlighting the practical significance of Keyformer's approach.
- **Key Point:** Keyformer aims to address the challenge of expanding KV cache size during inference while maintaining model accuracy, a crucial requirement for real-world applications.
    - **Citation:** (Reddi et al., 2020)
    - **Relevance:** This citation highlights the importance of maintaining accuracy in LLM optimization, setting the bar for Keyformer's performance.

**2.3 Background and Motivation**

- **Key Point:** The inference process in LLMs involves two phases: prompt processing and token generation.
    - **Citation:** None
    - **Relevance:** This is a basic concept in LLM inference, not directly supported by existing citations.
- **Key Point:** The KV cache stores key-value pairs for tokens in the context, reducing computational overhead during token generation.
    - **Citation:** (Strati et al., 2024)
    - **Relevance:** This citation explains the role of KV cache in LLM inference, providing a foundation for understanding Keyformer's approach.
- **Key Point:** The attention mechanism exhibits inherent sparsity, with a small subset of tokens receiving the most attention.
    - **Citation:** None
    - **Relevance:** This is an observation made by the authors, not directly supported by existing citations.
- **Key Point:** Identifying key tokens is crucial for improving inference performance, but it is challenging to determine these tokens dynamically, especially when dealing with unknown or unseen tokens.
    - **Citation:** None
    - **Relevance:** This is a challenge identified by the authors, not directly supported by existing citations.

**2.4 Reducing KV Cache Size by Exploiting Sparsity**

- **Key Point:** Keyformer leverages the inherent sparsity in the attention mechanism to reduce KV cache size by identifying and retaining only key tokens.
    - **Citation:** None
    - **Relevance:** This is a novel contribution of the paper, not directly supported by existing citations.
- **Key Point:** The authors propose using a skewed distribution to model the distribution of maximum values (key tokens), favoring initial tokens while maintaining an asymmetric profile.
    - **Citation:** (Xiao et al., 2023; Zhang et al., 2023)
    - **Relevance:** This citation provides a theoretical basis for Keyformer's approach, highlighting the bias towards initial tokens in LLMs.
- **Key Point:** The Gumbel distribution is used for logits regularization, as it characterizes the distribution of maximum values and is skewed towards initial tokens.
    - **Citation:** (Cooray, 2010)
    - **Relevance:** This citation provides a theoretical justification for the use of the Gumbel distribution in Keyformer.

**2.5 Keyformer: Intuition and Design**

- **Key Point:** Keyformer strategically removes tokens from the context during prompt processing to maintain a constant KV cache size during token generation.
    - **Citation:** None
    - **Relevance:** This is a novel aspect of Keyformer's design, not directly supported by existing citations.
- **Key Point:** Logits regularization is used to identify key tokens even in the presence of unknown contexts, adding noise to the unnormalized logits derived from the query-key-value (QKV) operation.
    - **Citation:** None
    - **Relevance:** This is a novel aspect of Keyformer's design, not directly supported by existing citations.
- **Key Point:** The choice of distribution for regularization impacts key token identification and model quality.
    - **Citation:** None
    - **Relevance:** This is a general observation, not directly supported by existing citations.
- **Key Point:** The Gumbel distribution is used for regularization, as it is skewed towards initial tokens and captures the essence of the Gumbel limit theorem.
    - **Citation:** (Cooray, 2010)
    - **Relevance:** This citation provides a theoretical justification for the use of the Gumbel distribution in Keyformer.
- **Key Point:** The authors propose a novel score function for Keyformer that integrates the Gumbel noise distribution into the unnormalized logits and accounts for discarded tokens.
    - **Citation:** (Jang et al., 2016; Maddison et al., 2016)
    - **Relevance:** This citation provides a theoretical basis for the use of the Gumbel softmax in Keyformer.
- **Key Point:** The temperature parameter in the score function regulates the smoothness of the probabilistic distribution, with higher values yielding uniform probabilities.
    - **Citation:** None
    - **Relevance:** This is a novel aspect of Keyformer's design, not directly supported by existing citations.

**2.6 Keyformer Algorithm**

- **Key Point:** Keyformer identifies key tokens using a mixture of recent and key tokens, discarding tokens based on a Gumbel softmax-based score function.
    - **Citation:** None
    - **Relevance:** This is a novel aspect of Keyformer's design, not directly supported by existing citations.
- **Key Point:** The score function accumulates over decoding steps for each layer and head, ensuring consistent behavior of key tokens across decoding iterations.
    - **Citation:** None
    - **Relevance:** This is a novel aspect of Keyformer's design, not directly supported by existing citations.
- **Key Point:** The temperature parameter is dynamically adjusted during decoding iterations, increasing randomness as more tokens are discarded.
    - **Citation:** None
    - **Relevance:** This is a novel aspect of Keyformer's design, not directly supported by existing citations.

**3. Key Insights and Supporting Literature**

- **Key Insight:** Keyformer effectively reduces KV cache size without compromising model accuracy, achieving significant improvements in inference latency and token generation throughput.
    - **Citation:** None
    - **Relevance:** This is the primary finding of the paper, not directly supported by existing citations.
- **Key Insight:** Keyformer's approach of identifying and retaining only key tokens is more effective than existing methods like Window Attention and H2O, which rely solely on recent tokens or heavy hitters.
    - **Citation:** (Zhang et al., 2023)
    - **Relevance:** This citation provides a comparison point for Keyformer's performance, highlighting its superiority.
- **Key Insight:** The Gumbel distribution used for logits regularization effectively models the distribution of maximum values (key tokens) and contributes to the improved accuracy of Keyformer.
    - **Citation:** (Cooray, 2010)
    - **Relevance:** This citation provides a theoretical basis for the effectiveness of the Gumbel distribution in Keyformer.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:** The authors evaluated Keyformer across three foundational models: GPT-J, Cerebras-GPT, and MPT, using various positional embedding algorithms. They assessed performance on summarization and conversation tasks involving extended contexts.
    - **Citation:** (Wang & Komatsuzaki, 2021; Dey et al., 2023; Team et al., 2023; Su et al., 2022; Press et al., 2021)
    - **Relevance:** This citation provides a detailed description of the models and tasks used in the evaluation, establishing the context for the results.
- **Novel Aspects of Methodology:** The authors introduced a novel score function for Keyformer that integrates the Gumbel noise distribution into the unnormalized logits and accounts for discarded tokens.
    - **Citation:** (Jang et al., 2016; Maddison et al., 2016)
    - **Relevance:** This citation provides a theoretical basis for the use of the Gumbel softmax in Keyformer.

**5. Results in Context**

- **Main Result:** Keyformer reduces inference latency by 2.1× and improves token generation throughput by 2.4× while preserving model accuracy, compared to full attention.
    - **Citation:** None
    - **Relevance:** This is the primary result of the paper, not directly supported by existing citations.
- **Result:** Keyformer consistently outperforms existing methods like Window Attention and H2O across various KV cache budgets, achieving baseline accuracy with only 70% of the KV cache size.
    - **Citation:** (Zhang et al., 2023)
    - **Relevance:** This result confirms the superiority of Keyformer compared to existing methods.
- **Result:** Keyformer maintains desired accuracy even with a 50% KV cache reduction for long context summarization tasks, outperforming H2O.
    - **Citation:** (Zhang et al., 2023)
    - **Relevance:** This result demonstrates the effectiveness of Keyformer for handling long contexts, extending its applicability beyond shorter sequences.

**6. Discussion and Related Work**

- **Key Papers Cited:** (Zaheer et al., 2020; Kitaev, 2020; Wang et al., 2020; Beltagy et al., 2020; Sheng et al., 2023; Dao et al., 2022; Kwon et al., 2023; Pope et al., 2023; Shazeer, 2019; Ainslie et al., 2023; Zhang et al., 2023; Liu et al., 2023; Anagnostidis et al., 2023; Mu et al., 2023; Mohtashami & Jaggi, 2023; Xiao et al., 2023; Yan et al., 2021)
- **Novelty:** The authors highlight the novelty of Keyformer's inference-time approach, which does not require model retraining or fine-tuning, unlike existing methods.
    - **Citation:** (Shazeer, 2019; Ainslie et al., 2023; Zhang et al., 2023; Liu et al., 2023; Anagnostidis et al., 2023; Mu et al., 2023; Mohtashami & Jaggi, 2023)
    - **Relevance:** This citation emphasizes the practical significance of Keyformer's approach, contrasting it with existing methods that require retraining.
- **Importance:** The authors emphasize the importance of Keyformer's ability to maintain accuracy while reducing KV cache size, a crucial requirement for real-world applications.
    - **Citation:** (Reddi et al., 2020)
    - **Relevance:** This citation highlights the importance of maintaining accuracy in LLM optimization, setting the bar for Keyformer's performance.

**7. Future Work and Open Questions**

- **Future Work:** The authors suggest integrating Keyformer into the LLM's attention block by replacing the standard softmax with a Keyformer-based softmax, addressing the quadratic computational and memory complexities of transformers.
    - **Citation:** None
    - **Relevance:** This is a suggestion for future research, not directly supported by existing citations.
- **Open Question:** The authors acknowledge that a detailed evaluation of Keyformer with multi-query attention (MQA) and group-query attention (GQA) models is deferred to future work.
    - **Citation:** (Shazeer, 2019; Ainslie et al., 2023)
    - **Relevance:** This highlights an area for further research, acknowledging the limitations of the current study.

**8. Critical Analysis of Citation Usage**

- **Effectiveness:** The authors generally use citations effectively to support their arguments and findings, providing a strong foundation for their claims.
- **Areas for Improvement:** While the authors cite a wide range of relevant works, they could have provided more specific citations to support some of their novel contributions, such as the design of the score function and the dynamic adjustment of the temperature parameter.
- **Potential Biases:** The authors primarily cite works related to attention mechanisms and KV cache reduction, potentially overlooking other relevant research areas, such as model compression and quantization.

**9. Final Summary**

- **Contribution:** Keyformer is a novel inference-time approach that effectively reduces KV cache size without compromising model accuracy, achieving significant improvements in inference latency and token generation throughput.
- **Influential Works:** (Vaswani et al., 2017; Sukhbaatar et al., 2019; Dao et al., 2022; Choromanski et al., 2020; Ott et al., 2019; Zaheer et al., 2020; Kitaev, 2020; Wang et al., 2020; Beltagy et al., 2020; Sheng et al., 2023; Reddi et al., 2020)
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings, providing a strong foundation for its novel contributions. However, the authors could have provided more specific citations to support some of their novel contributions.

Overall, this paper presents a valuable contribution to the field of LLM efficiency, offering a novel approach to address the challenges associated with KV cache size. Keyformer's ability to reduce memory bandwidth usage and improve inference latency and throughput without compromising accuracy makes it a promising technique for real-world applications. The authors provide a comprehensive evaluation of Keyformer, demonstrating its effectiveness across various models and tasks. However, further research is needed to explore the full potential of Keyformer and its integration with other LLM optimization techniques.
