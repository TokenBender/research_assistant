## DiPaCo: Distributed Path Composition - Citation Analysis

**1. Introduction**

- **Title:** DiPaCo: Distributed Path Composition
- **Authors:** Arthur Douillard, Qixuan Feng, Andrei A. Rusu, Adhiguna Kuncoro, Yani Donchev, Rachita Chhaparia, Ionel Gog, Marc'Aurelio Ranzato, Jiajun Shen, and Arthur Szlam
- **Publication Date:** 2023-07-01
- **Objective:** The paper proposes a novel modular architecture and training approach for ML models, called DiPaCo, which aims to distribute computation across paths through shared modules, enabling efficient training on large-scale, heterogeneous, and poorly connected computing resources.
- **Total References:** 55

**2. Section-by-Section Analysis with Citation Extraction**

**2.1. Introduction**

- **Key Points:** The paper highlights the challenges of scaling ML models due to the need for high bandwidth communication between parallel devices. It argues that current training paradigms are not fundamentally designed for distributed training and suffer from limitations in terms of engineering, organization, and model reusability.
- **Citations:**
    - **Claim:** "Progress in machine learning (ML) has been fueled by scaling neural network models."
        - **Citation:** (Dean et al., 2012)
        - **Relevance:** This citation supports the claim that scaling ML models has been a driving force in the field's progress.
    - **Claim:** "This scaling has been accomplished via data and model parallelism (Dean et al., 2012) and pipelining (Narayanan et al., 2020) to distribute computation, enabling the concurrent use of a large number of devices (Anil et al., 2023; OpenAI et al., 2023; Touvron et al., 2023)."
        - **Citation:** (Dean et al., 2012), (Narayanan et al., 2020), (Anil et al., 2023), (OpenAI et al., 2023), (Touvron et al., 2023)
        - **Relevance:** These citations provide examples of techniques used for scaling ML models, including data and model parallelism, and pipelining.
    - **Claim:** "Although model architectures (Lepikhin et al., 2021; OpenAI et al., 2023) have also been used to allow computational parallelism, and optimization procedures to prefer larger batches (Goyal et al., 2017) (again allowing more data parallelism), the current training paradigm has not fundamentally changed model architecture or optimization procedure to facilitate distributed training."
        - **Citation:** (Lepikhin et al., 2021), (OpenAI et al., 2023), (Goyal et al., 2017)
        - **Relevance:** These citations highlight the limitations of current training paradigms, which still rely on monolithic models and require extensive communication between devices.
    - **Claim:** "This approach incurs engineering and infrastructure challenges associated with provisioning and managing the large number of tightly interconnected devices required for the lengthy training process."
        - **Citation:** (Barham et al., 2022), (Borzunov et al., 2022), (Raffel, 2023b), (Ryabinin and Gusev, 2020)
        - **Relevance:** These citations provide context for the challenges associated with scaling ML models, particularly in terms of infrastructure and organization.

**2.2. Overview of System**

- **Key Points:** The paper introduces the core idea of DiPaCo, which involves training a sparsely-activated modular system where data and computation are distributed by the choice of path through the modules. This approach relies on two key ideas: coarse routing and DiLoCo.
- **Citations:**
    - **Claim:** "Sparsely routed Mixture of Experts (MoE) have shown great results in language modeling (Lepikhin et al., 2021)."
        - **Citation:** (Lepikhin et al., 2021)
        - **Relevance:** This citation introduces the concept of MoE, which is a key component of DiPaCo.
    - **Claim:** "We use DiLoCo (Douillard et al., 2023) for low communication data parallelism."
        - **Citation:** (Douillard et al., 2023)
        - **Relevance:** This citation introduces DiLoCo, a distributed optimization algorithm that is crucial for DiPaCo's training process.

**2.3. Notation**

- **Key Points:** This section introduces the notation used throughout the paper, defining terms like "module," "expert," and "path."
- **Citations:**
    - **Claim:** "We will call a set of parameters associated to a Bį a “module" or, as in (Jacobs et al., 1991; Jordan and Jacobs, 1994), an “expert”."
        - **Citation:** (Jacobs et al., 1991), (Jordan and Jacobs, 1994)
        - **Relevance:** These citations provide the original definitions of "module" and "expert" in the context of mixture of experts models.
    - **Claim:** "Any choice of module from B₁ and module form B2 defines a neural network; as in Dean (2021), we call each of these 9 possible networks a “path", see Figure 2."
        - **Citation:** (Dean, 2021)
        - **Relevance:** This citation provides the definition of "path" as used in the paper, which is consistent with the terminology used in Dean (2021).

**2.4. Coarse Routing**

- **Key Points:** This section describes the coarse routing mechanism used in DiPaCo, which involves assigning each sequence to a specific path based on the first 32 tokens of the sequence. The paper discusses three routing approaches: generative routing, discriminative routing, and routing more frequently at test time.
- **Citations:**
    - **Claim:** "In generative routing, the choice is based on minimizing feature reconstruction error. Given a representation z of the first 32 tokens of a sequence (context), we perform k-Means on the features z of each sequence, and then we use the k-means assignment algorithm to shard the data into k shards."
        - **Citation:** (Gross et al., 2017), (Gururangan et al., 2023)
        - **Relevance:** These citations provide context for the generative routing approach, which is based on k-Means clustering.
    - **Claim:** "In discriminative routing the sharding takes into account how well experts perform on each sequence."
        - **Citation:** (Dempster et al., 1977)
        - **Relevance:** This citation introduces the concept of Expectation Maximization (EM), which is used as a basis for the discriminative routing approach.

**2.5. DiLoCo: Review**

- **Key Points:** This section provides a brief overview of DiLoCo, a distributed optimization algorithm that is used as a foundation for DiPaCo's training process.
- **Citations:**
    - **Claim:** "DiLoCo optimizes a dense model across k workers."
        - **Citation:** (Douillard et al., 2023)
        - **Relevance:** This citation introduces DiLoCo and its key features.
    - **Claim:** "In language modeling applications using transformers, the inner and outer optimizers that have been shown to be most effective are respectively AdamW (Kingma and Ba, 2014) and Nesterov momentum (Sutskever et al., 2013)."
        - **Citation:** (Kingma and Ba, 2014), (Sutskever et al., 2013)
        - **Relevance:** These citations highlight the specific optimizers used in DiLoCo, which are AdamW for inner optimization and Nesterov momentum for outer optimization.
    - **Claim:** "Note that other alternatives, such as FedOpt (Reddi et al., 2021), are compatible with this framework."
        - **Citation:** (Reddi et al., 2021)
        - **Relevance:** This citation mentions FedOpt as an alternative optimization approach that could be used with DiLoCo.

**2.6. DiPaCo**

- **Key Points:** This section describes the DiPaCo architecture and training algorithm, which combines coarse routing and DiLoCo to train a composable mixture of experts model. The paper discusses the concept of increasing capacity by adding more paths and the scaling of the modular architecture.
- **Citations:**
    - **Claim:** "In the toy illustration of Figure 4 there are three levels, B1, B2, and B3. There is only one module (equivalently, one set of parameters) in B1, and it is shared across all paths."
        - **Citation:** (Douillard et al., 2023)
        - **Relevance:** This citation provides context for the DiPaCo architecture, which is based on the DiLoCo framework.
    - **Claim:** "The resulting 2 × 2 DiPaCo has 4 paths in total (as shown on the middle panel of Figure 4). However, the full model need never be fully instantiated, neither during training nor testing."
        - **Citation:** (Dean, 2021)
        - **Relevance:** This citation highlights the key feature of DiPaCo, which allows for training and testing without instantiating the full model.
    - **Claim:** "The more paths go through a module, the more opportunities for transfer learning across paths, but also the more constrained learning is and the less capacity the overall mixture has."
        - **Citation:** (Gross et al., 2017), (Gururangan et al., 2023)
        - **Relevance:** These citations provide context for the trade-off between increasing capacity and transfer learning in DiPaCo.
    - **Claim:** "The extreme form of capacity increase as in subsubsection 2.6.1 would be to have each path be a completely independent network."
        - **Citation:** (Gross et al., 2017), (Gururangan et al., 2023)
        - **Relevance:** These citations introduce the concept of flat MoE, which is used as a baseline for comparison with DiPaCo.

**2.7. Advanced Optimization Techniques**

- **Key Points:** This section discusses several optimization techniques used in DiPaCo, including outer gradient norm rescaling, loss reweighing, early stopping, and asynchronous checkpoints gathering.
- **Citations:**
    - **Claim:** "In language modeling applications using transformers, the inner and outer optimizers that have been shown to be most effective are respectively AdamW (Kingma and Ba, 2014) and Nesterov momentum (Sutskever et al., 2013)."
        - **Citation:** (Kingma and Ba, 2014), (Sutskever et al., 2013)
        - **Relevance:** These citations highlight the specific optimizers used in DiPaCo, which are AdamW for inner optimization and Nesterov momentum for outer optimization.
    - **Claim:** "Note that other alternatives, such as FedOpt (Reddi et al., 2021), are compatible with this framework."
        - **Citation:** (Reddi et al., 2021)
        - **Relevance:** This citation mentions FedOpt as an alternative optimization approach that could be used with DiLoCo.

**3. Infrastructure**

- **Key Points:** This section describes the infrastructure designed to implement DiPaCo, which includes a worker pool, a task queue system, and an outer optimization executor. The paper highlights the importance of fault tolerance and scalability in the infrastructure.
- **Citations:**
    - **Claim:** "When a training worker in the worker pool (in orange) becomes available, it fetches the next training task from the train task scheduler and performs inner optimization (L5-9 of Algorithm 1) on accelerators."
        - **Citation:** (Ghemawat et al., 2003)
        - **Relevance:** This citation introduces the Google's distributed file system (GFS), which is used for storing checkpoints in DiPaCo.
    - **Claim:** "The outer optimizer task scheduler (indicated in light blue) distributes outer optimization tasks to sharded outer optimization executors (highlighted in red), each of which is responsible for the outer optimization of a shard of modules (e.g., a single module or a collection of modules)."
        - **Citation:** (Corbett et al., 2012)
        - **Relevance:** This citation introduces Spanner, a globally-distributed database used for storing checkpoints and metadata in DiPaCo.
    - **Claim:** "In the background an Effingo process (Google, 2023) to bring the checkpoint to a closer location."
        - **Citation:** (Google, 2023)
        - **Relevance:** This citation introduces Effingo, a Google service for moving data at scale, which is used for efficiently loading checkpoints in DiPaCo.

**4. Experiments**

- **Key Points:** This section presents the experimental results of DiPaCo on a language modeling task using the C4 dataset. The paper compares DiPaCo's performance to dense transformer models of different sizes and investigates the impact of scaling the number of paths and parameter sharing.
- **Citations:**
    - **Claim:** "We consider a language modeling task on the C4 dataset, derived from Common Crawl (Raffel et al., 2020), tokenized with a SentencePiece tokenizer (Kudo and Richardson, 2018) with a vocabulary size of 32,000."
        - **Citation:** (Raffel et al., 2020), (Kudo and Richardson, 2018)
        - **Relevance:** These citations provide context for the experimental setup, including the dataset and tokenizer used.
    - **Claim:** "We report perplexity on the validation set against number of weight update steps used at training time, which is a close proxy for wall-clock time if all computations are done on the same accelerator type."
        - **Citation:** (Douillard et al., 2023)
        - **Relevance:** This citation provides context for the evaluation metric used in the experiments, which is perplexity on the validation set.
    - **Claim:** "We again warn the reader that this comparison is not standard in the literature, as weight updates for DiPaCo see more tokens and use more FLOPS when the number of paths is larger."
        - **Citation:** (Douillard et al., 2023)
        - **Relevance:** This citation acknowledges the limitations of the comparison between DiPaCo and dense models, as they use different amounts of FLOPs and see different numbers of tokens.

**5. Related Work**

- **Key Points:** This section discusses related work in the areas of modularity, mixture of experts, and distributed training. The paper highlights the similarities and differences between DiPaCo and other approaches.
- **Citations:**
    - **Claim:** "As mentioned in the introduction, this work shares the same motivation and intuitions expressed in Pathways (Dean, 2021)."
        - **Citation:** (Dean, 2021)
        - **Relevance:** This citation highlights the connection between DiPaCo and the Pathways framework, which also aims to enable distributed training of modular models.
    - **Claim:** "Our approach also shares motivations and intuitions with Borzunov et al. (2022); Ryabinin and Gusev (2020)."
        - **Citation:** (Borzunov et al., 2022), (Ryabinin and Gusev, 2020)
        - **Relevance:** These citations highlight other works that share similar motivations with DiPaCo, particularly in terms of enabling collaboration and distributed training.
    - **Claim:** "The key difference in this work is that each worker trains a path through modules, rather than a module."
        - **Citation:** (Alayrac et al., 2022), (Dalmia et al., 2023)
        - **Relevance:** These citations provide examples of approaches that use pre-trained modules, which is different from DiPaCo's approach of training paths through modules.
    - **Claim:** "In their seminal work, Shazeer et al. (2017) proposed a very large mixture of experts LSTM model for sequence modeling tasks."
        - **Citation:** (Shazeer et al., 2017)
        - **Relevance:** This citation introduces the seminal work of Shazeer et al. (2017), which proposed a large mixture of experts model for sequence modeling.
    - **Claim:** "Most works MoE for sequence-modeling works that followed (Artetxe et al., 2021; Clark et al., 2022; Fedus et al., 2021; Lepikhin et al., 2021) have used a recipe whereby FFN layers of transformers are replaced by mixtures."
        - **Citation:** (Artetxe et al., 2021), (Clark et al., 2022), (Fedus et al., 2021), (Lepikhin et al., 2021)
        - **Relevance:** These citations provide examples of works that followed Shazeer et al. (2017) and used MoE for sequence modeling.
    - **Claim:** "In contrast, (Gururangan et al., 2023) trains experts independently using a document level router; this approach had been used in computer vision by Gross et al. (2017), and it also appeared in the federated learning literature (Reisser et al., 2021)."
        - **Citation:** (Gururangan et al., 2023), (Gross et al., 2017), (Reisser et al., 2021)
        - **Relevance:** These citations highlight other works that use document-level routing, which is different from DiPaCo's approach of using a mixture of experts model.

**6. Limitations**

- **Key Points:** The paper acknowledges the limitations of DiPaCo, particularly in terms of FLOP efficiency and scaling laws.
- **Citations:**
    - **Claim:** "The most salient limitation to DiPaCo is with respect to FLOP efficiency."
        - **Citation:** (Douillard et al., 2023)
        - **Relevance:** This citation highlights the limitation of DiPaCo in terms of FLOP efficiency, which is a key concern for large-scale models.

**7. Conclusions and Future Work**

- **Key Points:** The paper concludes by summarizing the contributions of DiPaCo and outlining areas for future work, including improving FLOP efficiency, exploring more sophisticated sharding approaches, and applying DiPaCo to continual learning settings.
- **Citations:**
    - **Claim:** "Our long-term dream is to further refine this approach and produce a never-ending, community-driven, modular learning system that can be used by everyone to compose new predictors out of existing modules, and thus efficiently develop entirely new models and capabilities in a positive feedback loop."
        - **Citation:** (Dean, 2021), (Raffel, 2023a)
        - **Relevance:** These citations highlight the long-term vision of the authors, which is to create a modular learning system that can be used for developing new models and capabilities.

**8. Critical Analysis of Citation Usage**

- **Effectiveness:** The authors effectively use citations to support their arguments and findings. They provide relevant citations to introduce key concepts, explain their methodology, and compare their results with existing literature.
- **Areas for Improvement:** While the authors cite a wide range of relevant works, there are a few areas where additional citations might have been beneficial. For example, the paper could have provided more citations to support the claims about the limitations of current training paradigms and the benefits of modularity.
- **Potential Biases:** The authors primarily cite works from Google DeepMind and OpenAI, which might suggest a potential bias towards these organizations. However, they also cite works from other research institutions, demonstrating a broader understanding of the field.

**9. Final Summary**

- **Contribution:** DiPaCo is a novel modular architecture and training approach for ML models that aims to distribute computation across paths through shared modules, enabling efficient training on large-scale, heterogeneous, and poorly connected computing resources. The paper demonstrates the effectiveness of DiPaCo on a language modeling task, showing that it can achieve comparable performance to dense models while using significantly fewer parameters and requiring less compute.
- **Influential Works:** The paper frequently cites works from Google DeepMind and OpenAI, highlighting the influence of these organizations in the field of large-scale ML.
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. It provides relevant citations to introduce key concepts, explain its methodology, and compare its results with existing literature. However, there are a few areas where additional citations might have been beneficial to provide a more comprehensive overview of the field.

Overall, DiPaCo is a promising approach for scaling ML models to larger sizes and more complex architectures. The paper provides a strong foundation for future research in this area, particularly in terms of improving FLOP efficiency, exploring more sophisticated sharding approaches, and applying DiPaCo to continual learning settings. The authors' effective use of citations helps readers understand the factual basis of the research, its relationship to existing literature, and the broader context of the work.