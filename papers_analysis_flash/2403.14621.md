Okay, here's a comprehensive analysis of the paper "GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation" in Markdown format, following the structure you provided:


# GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation - Paper Analysis

## 1. Introduction

- **Title:** GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation
- **Authors:** Yinghao Xu, Zifan Shi, Wang Yifan, Hansheng Chen, Ceyuan Yang, Sida Peng, Yujun Shen, and Gordon Wetzstein
- **Publication Date:** March 21, 2024 (arXiv preprint)
- **Main Objective:** The research aims to introduce GRM, a novel feed-forward 3D generative model that efficiently reconstructs 3D scenes from sparse-view images and generates high-fidelity 3D assets from text or single images.
- **Total Number of References:** 114


## 2. Section-by-Section Analysis with Citation Extraction

### 2.1 Introduction

- **Key Points:** Introduces the importance of high-quality and diverse 3D assets in various domains. Highlights the challenges of traditional manual creation and the emergence of 3D generative models. Discusses the limitations of optimization-based and existing state-of-the-art (SOTA) feed-forward 3D generative models, particularly their reliance on the triplane representation and inefficient volume rendering.
- **Significant Citations:**

    a. **Claim:** "Emerging 3D generative models offer the ability to easily create diverse 3D assets from simple text prompts or single images."
    b. **Citation:** [70] Po, R., Yifan, W., Golyanik, V., Aberman, K., Barron, J.T., Bermano, A.H., Chan, E.R., Dekel, T., Holynski, A., Kanazawa, A., et al.: State of the art on diffusion models for visual computing. arXiv preprint arXiv:2310.07204 (2023)
    c. **Relevance:** This citation provides context for the growing field of 3D generative models and their potential to address the challenges of creating 3D assets.

    a. **Claim:** "Optimization-based 3D generative methods can produce high-quality assets, but they often require a long time—often hours—to produce a single 3D asset."
    b. **Citation:** [50, 71, 93, 98, 101]  (Specific citations within these are:  [50]  Schwarz, K., Liao, Y., Niemeyer, M., Geiger, A.: Graf: Generative radiance fields for 3d-aware image synthesis. In: Adv. Neural Inform. Process. Syst. (2020), [71] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. In: The Eleventh International Conference on Learning Representations (2022), [93] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023), [98] Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z., Zhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024 (2023), [101] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213 (2023))
    c. **Relevance:** These citations highlight the limitations of optimization-based methods in terms of speed, which motivates the need for faster alternatives like feed-forward methods.

    a. **Claim:** "These state-of-the-art (SOTA) models, however, typically build on the triplane representation, which requires inefficient volume rendering."
    b. **Citation:** [5] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo, O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d generative adversarial networks. In: IEEE Conf. Comput. Vis. Pattern Recog. (2022)
    c. **Relevance:** This citation introduces the triplane representation, a common approach in 3D generation, and points out its drawbacks in terms of efficiency, which GRM aims to address.


### 2.2 Related Work

- **Key Points:** Reviews prior work on sparse-view reconstruction and 3D generation. Discusses the use of neural representations and neural rendering in novel-view synthesis. Highlights the challenges of sparse-view reconstruction, including capturing multiple modes in large-scale datasets and the limitations of neural volume-based scene representations. Introduces the concept of 3D Gaussians and their advantages for efficient scene representation and rendering.
- **Significant Citations:**

    a. **Claim:** "Neural representations, as highlighted in prior works, present a promising foundation for scene representation and neural rendering."
    b. **Citation:** [9, 62-64, 69, 84, 86] (Specific citations within these are: [9] Chen, A., Xu, Z., Geiger, A., Yu, J., Su, H.: Tensorf: Tensorial radiance fields. In: European Conference on Computer Vision (ECCV) (2022), [62] Mescheder, L., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy networks: Learning 3d reconstruction in function space. In: IEEE Conf. Comput. Vis. Pattern Recog. (2019), [63] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: Nerf: Representing scenes as neural radiance fields for view synthesis. In: Eur. Conf. Comput. Vis. (2020), [64] Müller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph. 41(4), 102:1–102:15 (Jul 2022), [69] Park, J.J., Florence, P., Straub, J., Newcombe, R., Lovegrove, S.: Deepsdf: Learning continuous signed distance functions for shape representation. In: IEEE Conf. Comput. Vis. Pattern Recog. (2019), [84] Sitzmann, V., Martel, J., Bergman, A., Lindell, D., Wetzstein, G.: Implicit neural representations with periodic activation functions. Advances in neural information processing systems 33, 7462-7473 (2020), [86] Sitzmann, V., Zollhöfer, M., Wetzstein, G.: Scene representation networks: Continuous 3d-structure-aware neural scene representations. Advances in Neural Information Processing Systems 32 (2019))
    c. **Relevance:** These citations establish the foundation of neural representations and their application in scene understanding and rendering, which are relevant to the paper's approach.

    a. **Claim:** "Notably, recent advancements have extended these techniques to operate with a sparse set of views, displaying improved generalization to unseen scenes."
    b. **Citation:** [10, 33, 51, 59, 100, 109] (Specific citations within these are: [10] Chen, A., Xu, Z., Zhao, F., Zhang, X., Xiang, F., Yu, J., Su, H.: Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In: Int. Conf. Comput. Vis. (2021), [33] Jain, A., Tancik, M., Abbeel, P.: Putting nerf on a diet: Semantically consistent few-shot view synthesis. In: Int. Conf. Comput. Vis. (2021), [51] Lin, K.E., Yen-Chen, L., Lai, W.S., Lin, T.Y., Shih, Y.C., Ramamoorthi, R.: Vision transformer for nerf-based view synthesis from a single input image. In: IEEE Winter Conf. Appl. Comput. Vis. (2023), [59] Long, X., Lin, C., Wang, P., Komura, T., Wang, W.: Sparseneus: Fast generalizable neural surface reconstruction from sparse views. In: Eur. Conf. Comput. Vis. (2022), [100] Wang, Q., Wang, Z., Genova, K., Srinivasan, P.P., Zhou, H., Barron, J.T., Martin-Brualla, R., Snavely, N., Funkhouser, T.: Ibrnet: Learning multi-view image-based rendering. In: IEEE Conf. Comput. Vis. Pattern Recog. (2021), [109] Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from one or few images. In: IEEE Conf. Comput. Vis. Pattern Recog. (2021))
    c. **Relevance:** These citations demonstrate the progress in sparse-view reconstruction, highlighting the challenges and successes in generalizing to unseen scenes, which is a key aspect of GRM's capabilities.

    a. **Claim:** "But relying on neural volume-based scene representation proves inadequate for efficiently synthesizing high-resolution and high-fidelity images."
    b. **Citation:** [30, 99, 114] (Specific citations within these are: [30] Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023), [99] Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z., Zhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024 (2023), [114] Zou, Z.X., Yu, Z., Guo, Y.C., Li, Y., Liang, D., Cao, Y.P., Zhang, S.H.: Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers. arXiv preprint arXiv:2312.09147 (2023))
    c. **Relevance:** These citations highlight the limitations of neural volume-based methods in achieving high resolution and fidelity, which motivates the use of alternative representations like 3D Gaussians.

    a. **Claim:** "Our proposed solution involves the use of pixel-aligned 3D Gaussians combined with our effective transformer architecture."
    b. **Citation:** [8, 90] (Specific citations within these are: [8] Charatan, D., Li, S., Tagliasacchi, A., Sitzmann, V.: pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. arXiv preprint arXiv:2312.12337 (2023), [90] Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Splatter image: Ultra-fast single-view 3d reconstruction. arXiv preprint arXiv:2312.13150 (2023))
    c. **Relevance:** This statement introduces the core idea of GRM, which leverages pixel-aligned 3D Gaussians and a transformer architecture to address the limitations of existing methods.


### 2.3 3D Generation

- **Key Points:** Discusses the advancements in 3D GANs and Diffusion Models (DMs) for 3D scene generation. Highlights the strengths and limitations of different approaches, including direct training of 3D DMs, Score Distillation Sampling (SDS), and multi-view diffusion methods. Introduces the concept of generalizable Gaussians and their applications in various 3D generation tasks.
- **Significant Citations:**

    a. **Claim:** "The advances of 3D GANs have set the foundation of 3D scene generation."
    b. **Citation:** [5, 6, 22, 24, 65, 67, 77, 82, 87, 88, 104, 105] (Specific citations within these are: [5] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo, O., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d generative adversarial networks. In: IEEE Conf. Comput. Vis. Pattern Recog. (2022), [6] Chan, E.R., Monteiro, M., Kellnhofer, P., Wu, J., Wetzstein, G.: pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In: IEEE Conf. Comput. Vis. Pattern Recog. (2021), [22] Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z., Fidler, S.: Get3d: A generative model of high quality 3d textured shapes learned from images. Adv. Neural Inform. Process. Syst. (2022), [24] Gu, J., Liu, L., Wang, P., Theobalt, C.: Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis. arXiv preprint arXiv:2110.08985 (2021), [65] Nguyen-Phuoc, T., Li, C., Theis, L., Richardt, C., Yang, Y.L.: Hologan: Unsupervised learning of 3d representations from natural images. In: Int. Conf. Comput. Vis. (2019), [67] Niemeyer, M., Geiger, A.: Giraffe: Representing scenes as compositional generative neural feature fields. In: IEEE Conf. Comput. Vis. Pattern Recog. (2021), [77] Shen, B., Yan, X., Qi, C.R., Najibi, M., Deng, B., Guibas, L., Zhou, Y., Anguelov, D.: Gina-3d: Learning to generate implicit neural assets in the wild. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 4913-4926 (2023), [82] Shi, Z., Peng, S., Xu, Y., Andreas, G., Liao, Y., Shen, Y.: Deep generative models on 3d representations: A survey. arXiv preprint arXiv:2210.15663 (2022), [87] Skorokhodov, I., Siarohin, A., Xu, Y., Ren, J., Lee, H.Y., Wonka, P., Tulyakov, S.: 3d generation on imagenet. In: International Conference on Learning Representations (2023), [104] Xu, Y., Chai, M., Shi, Z., Peng, S., Skorokhodov, I., Siarohin, A., Yang, C., Shen, Y., Lee, H.Y., Zhou, B., et al.: Discoscene: Spatially disentangled generative radiance fields for controllable 3d-aware scene synthesis. In: IEEE Conf. Comput. Vis. Pattern Recog. (2023))
    c. **Relevance:** These citations provide a historical context for 3D GANs and their role in 3D scene generation, highlighting the evolution of techniques and the emergence of DMs as a powerful alternative.

    a. **Claim:** "With its extension in 3D being actively explored, we review the most relevant work and refer readers to [70] for a comprehensive review."
    b. **Citation:** [70] Po, R., Yifan, W., Golyanik, V., Aberman, K., Barron, J.T., Bermano, A.H., Chan, E.R., Dekel, T., Holynski, A., Kanazawa, A., et al.: State of the art on diffusion models for visual computing. arXiv preprint arXiv:2310.07204 (2023)
    c. **Relevance:** This citation acknowledges the growing research in 3D diffusion models and directs readers to a comprehensive review of the topic.

    a. **Claim:** "One research line seeks to directly train 3D DMs using 3D or 2D supervision."
    b. **Citation:** [2, 12, 25, 26, 36, 38, 55, 66, 68, 78, 83] (Specific citations within these are: [2] Anciukevičius, T., Xu, Z., Fisher, M., Henderson, P., Bilen, H., Mitra, N.J., Guerrero, P.: Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation. In: IEEE Conf. Comput. Vis. Pattern Recog. (2023), [12] Chen, H., Gu, J., Chen, A., Tian, W., Tu, Z., Liu, L., Su, H.: Single-stage diffusion nerf: A unified approach to 3d generation and reconstruction. arXiv preprint arXiv:2304.06714 (2023), [25] Gu, J., Trevithick, A., Lin, K.E., Susskind, J.M., Theobalt, C., Liu, L., Ramamoorthi, R.: Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In: Int. Conf. Mach. Learn. (2023), [26] Gupta, A., Xiong, W., Nie, Y., Jones, I., Oğuz, B.: 3dgen: Triplane latent diffusion for textured mesh generation. arXiv preprint arXiv:2303.05371 (2023), [36] Jun, H., Nichol, A.: Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463 (2023), [38] Karnewar, A., Vedaldi, A., Novotny, D., Mitra, N.J.: Holodiffusion: Training a 3d diffusion model using 2d images. In: IEEE Conf. Comput. Vis. Pattern Recog. (2023), [55] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9298–9309 (2023), [66] Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., Chen, M.: Point-e: A system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751 (2022), [68] Ntavelis, E., Siarohin, A., Olszewski, K., Wang, C., Van Gool, L., Tulyakov, S.: Autodecoding latent 3d diffusion models. arXiv preprint arXiv:2307.05445 (2023), [78] Shen, B., Yan, X., Qi, C.R., Najibi, M., Deng, B., Guibas, L., Zhou, Y., Anguelov, D.: Gina-3d: Learning to generate implicit neural assets in the wild. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 4913-4926 (2023), [83] Sitzmann, V., Martel, J., Bergman, A., Lindell, D., Wetzstein, G.: Implicit neural representations with periodic activation functions. Advances in neural information processing systems 33, 7462-7473 (2020))
    c. **Relevance:** These citations showcase the different approaches to training 3D DMs, highlighting the challenges and successes in leveraging 3D and 2D supervision.

    a. **Claim:** "Other researchers propose to exploit 2D diffusion priors using an optimization procedure known as Score Distillation Sampling (SDS) and its variant."
    b. **Citation:** [13, 15, 27, 49, 50, 55, 71, 81, 93, 98, 101] (Specific citations within these are: [13] Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873 (2023), [15] Chung, J., Lee, S., Nam, H., Lee, J., Lee, K.M.: Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. arXiv preprint arXiv:2311.13384 (2023), [27] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems 30 (2017), [49] Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 300–309 (2023), [50] Lin, K.E., Yen-Chen, L., Lai, W.S., Lin, T.Y., Shih, Y.C., Ramamoorthi, R.: Vision transformer for nerf-based view synthesis from a single input image. In: IEEE Winter Conf. Appl. Comput. Vis. (2023), [55] Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-1-to-3: Zero-shot one image to 3d object. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9298–9309 (2023), [71] Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using 2d diffusion. In: The Eleventh International Conference on Learning Representations (2022), [81] Shi, Y., Wang, P., Ye, J., Mai, L., Li, K., Yang, X.: Mvdream: Multi-view diffusion for 3d generation. In: The Twelfth International Conference on Learning Representations (2023), [93] Tang, J., Ren, J., Zhou, H., Liu, Z., Zeng, G.: Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653 (2023), [98] Wang, P., Tan, H., Bi, S., Xu, Y., Luan, F., Sunkavalli, K., Wang, W., Xu, Z., Zhang, K.: Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024 (2023), [101] Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., Zhu, J.: Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213 (2023))
    c. **Relevance:** These citations introduce SDS, a key optimization-based technique for 3D generation, and highlight its limitations in terms of speed.

    a. **Claim:** "Recently, the Large Reconstruction Model (LRM) scales up both the model and the dataset to predict a neural radiance field (NeRF) from single-view images."
    b. **Citation:** [30] Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K., Bui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400 (2023)
    c. **Relevance:** This citation introduces LRM, a significant recent work in 3D reconstruction, and highlights its approach using NeRFs.

    a. **Claim:** "Our method also builds on a strong reconstruction model and uses pretrained 2D DMs to provide input images for 3D generation in a feed-forward fashion."
    b. **Citation:** [46, 106] (Specific citations within these are: [46] Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K., Shakhnarovich, G., Bi, S.: Instant3D: Fast text-to-3d with sparse-view generation and large reconstruction model. https://arxiv.org/abs/2311.06214 (2023), [106] Xu, Y., Tan, H., Luan, F., Bi, S., Wang, P., Li, J., Shi, Z., Sunkavalli, K., Wetzstein, G., Xu, Z., et al.: Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217 (2023))
    c. **Relevance:** This statement highlights GRM's approach of combining a strong reconstruction model with pretrained 2D DMs, which is a key aspect of its design.

    a. **Claim:** "However, we adopt highly efficient 3D Gaussians for representing and rendering a scene."
    b. **Citation:** [43] Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics 42(4) (2023)
    c. **Relevance:** This citation introduces the use of 3D Gaussians, a core component of GRM, and highlights their efficiency for scene representation and rendering.

    a. **Claim:** "3D Gaussians and differentiable splatting have gained broad popularity thanks to their ability to efficiently reconstruct high-fidelity 3D scenes from posed images using only a moderate number of 3D Gaussians."
    b. **Citation:** [43, 44] (Specific citations within these are: [43] Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics 42(4) (2023), [44] Keselman, L., Hebert, M.: Approximate differentiable rendering with algebraic surfaces. In: European Conference on Computer Vision. pp. 596–614. Springer (2022))
    c. **Relevance:** This citation establishes the popularity and effectiveness of 3D Gaussians as a representation for 3D scenes, providing a foundation for GRM's approach.

    a. **Claim:** "This representation has been quickly adopted for various applications, including image- or text-conditioned 3D and 4D generation, avatar reconstruction, dynamic scene reconstruction, among others."
    b. **Citation:** [1, 11, 14, 31, 47, 48, 49, 52, 61, 72, 74, 76, 97, 102, 107, 108, 113] (Specific citations within these are: [1] Abdal, R., Yifan, W., Shi, Z., Xu, Y., Po, R., Kuang, Z., Chen, Q., Yeung, D.Y., Wetzstein, G.: Gaussian shell maps for efficient 3d human generation. arXiv preprint arXiv:2311.17857 (2023), [11] Chen, G., Wang, W.: A survey on 3d gaussian splatting. arXiv preprint arXiv:2401.03890 (2024), [14] Chen, Z., Wang, F., Liu, H.: Text-to-3d using gaussian splatting. arXiv preprint arXiv:2309.16585 (2023), [31] Hu, L., Zhang, H., Zhang, Y., Zhou, B., Liu, B., Zhang, S., Nie, L.: Gaussiana-avatar: Towards realistic human avatar modeling from a single video via animatable 3d gaussians. arXiv preprint arXiv:2312.02134 (2023), [47] Li, X., Wang, H., Tseng, K.K.: Gaussiandiffusion: 3d gaussian splatting for denoising diffusion probabilistic models with structured noise. arXiv preprint arXiv:2311.11221 (2023), [48] Li, Z., Zheng, Z., Wang, L., Liu, Y.: Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling. arXiv preprint arXiv:2311.16096 (2023), [49] Liang, Y., Yang, X., Lin, J., Li, H., Xu, X., Chen, Y.: Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. arXiv preprint arXiv:2311.11284 (2023), [52] Ling, H., Kim, S.W., Torralba, A., Fidler, S., Kreis, K.: Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. arXiv preprint arXiv:2312.13763 (2023), [61] Luiten, J., Kopanas, G., Leibe, B., Ramanan, D.: Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713 (2023), [72] Qian, S., Kirschstein, T., Schoneveld, L., Davoli, D., Giebenhain, S., Nießner, M.: Gaussianavatars: Photorealistic head avatars with rigged 3d gaussians. arXiv preprint arXiv:2312.02069 (2023), [74] Ren, J., Pan, L., Tang, J., Zhang, C., Cao, A., Zeng, G., Liu, Z.: Dreamgaussian4d: Generative 4d gaussian splatting. arXiv preprint arXiv:2312.17142 (2023), [76] Saito, S., Schwartz, G., Simon, T., Li, J., Nam, G.: Relightable gaussian codec avatars. arXiv preprint arXiv:2312.03704 (2023), [97] Tosi, F., Zhang, Y., Gong, Z., Sandström, E., Mattoccia, S., Oswald, M.R., Poggi, M.: How nerfs and 3d gaussian splatting are reshaping slam: a survey. arXiv preprint arXiv:2402.13255 (2024), [102] Wu, G., Yi, T., Fang, J., Xie, L., Zhang, X., Wei, W., Liu, W., Tian, Q., Wang, X.: 4d gaussian splatting for real-time dynamic scene rendering. arXiv preprint arXiv:2310.08528 (2023), [107] Yang, Z., Yang, H., Pan, Z., Zhu, X., Zhang, L.: Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642 (2023), [108] Yang, Z., Gao, X., Zhou, W., Jiao, S., Zhang, Y., Jin, X.: Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101 (2023), [113] Zielonka, W., Bagautdinov, T., Saito, S., Zollhöfer, M., Thies, J., Romero, J.: Drivable 3d gaussian avatars. arXiv preprint arXiv:2311.08581 (2023))
    c. **Relevance:** These citations demonstrate the wide range of applications where 3D Gaussians have been successfully employed, highlighting the versatility of this representation.


### 2.4 Method

- **Key Points:** Details the architecture of GRM, a feed-forward sparse-view 3D reconstructor that leverages pixel-aligned 3D Gaussians and a transformer-based network. Explains the concept of pixel-aligned Gaussians and their role in establishing connections between input pixels and 3D space. Describes the transformer-based encoder and upsampler, highlighting the use of windowed self-attention for efficient upsampling. Explains the rendering process using Gaussian splatting. Details the training process and loss functions.
- **Significant Citations:**

    a. **Claim:** "GRM is a feed-forward sparse-view 3D reconstructor, utilizing four input images to efficiently infer underlying 3D Gaussians."
    b. **Citation:** [43] Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics 42(4) (2023)
    c. **Relevance:** This citation establishes the foundation of GRM's approach, which utilizes 3D Gaussians for scene representation and reconstruction.

    a. **Claim:** "Supplied with a multi-view image generator head, GRM can be utilized to generate 3D from text or a single image."
    b. **Citation:** [46, 79] (Specific citations within these are: [46] Li, J., Tan, H., Zhang, K., Xu, Z., Luan, F., Xu, Y., Hong, Y., Sunkavalli, K., Shakhnarovich, G., Bi, S.: Instant3D: Fast text-to-3d with sparse-view generation and large reconstruction model. https://arxiv.org/abs/2311.06214 (2023), [79] Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng