Okay, here's the analysis of the provided paper in Markdown format, following the specified guidelines:


# Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs

**1. Introduction**

- **Title:** Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs
- **Authors:** Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, Jiawei Han
- **Publication Date:** July 15, 2024 (v2)
- **Main Objective:** The research aims to address the hallucination problem in large language models (LLMs) by proposing a novel framework, GRAPH-COT, that leverages graph structures as external knowledge sources for improved reasoning and factual accuracy.
- **Total Number of References:** 62


**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:** Introduces the challenge of hallucinations in LLMs, especially on knowledge-intensive tasks. Discusses existing approaches like retrieval augmentation (RAG) that utilize individual text units from external corpora but fail to capture the interconnectedness of knowledge within graphs. Highlights the need for a benchmark dataset and a framework to effectively leverage graph structures for LLM augmentation.
- **Significant Citations:**

    a. "Large language models (LLMs) (Touvron et al., 2023; Jiang et al., 2024) have demonstrated their exceptional language understanding and text generation capability in real-world scenarios (Zhao et al., 2023)."
    b. **Touvron et al., 2023.** *Llama 2: Open foundation and fine-tuned chat models*. arXiv preprint arXiv:2307.09288.
    c. **Jiang et al., 2024.** *Mixtral of experts*. arXiv preprint arXiv:2401.04088.
    d. **Zhao et al., 2023.** *A comprehensive survey of large language models on graphs*. arXiv preprint arXiv:2312.02783.
    e. "However, LLMs suffer from hallucination problems and sometimes tend to generate content that appears plausible but is ungrounded (Tonmoy et al., 2024)."
    f. **Tonmoy et al., 2024.** *A comprehensive survey of hallucination mitigation techniques in large language models*. arXiv preprint arXiv:2401.01313.
    g. "This is because they memorize world knowledge parametrically and fail to refer to concrete knowledge sources (Zhang et al., 2023b)."
    h. **Zhang et al., 2023b.** *Siren's song in the ai ocean: A survey on hallucination in large language models*. arXiv preprint arXiv:2309.01219.
    i. "To alleviate the hallucination issues, existing works propose to augment LLMs with external text corpora as knowledge sources (Shuster et al., 2021; Wu et al., 2023) and treat every single document as a knowledge unit."
    j. **Shuster et al., 2021.** *Retrieval augmentation reduces hallucination in conversation*. arXiv preprint arXiv:2104.07567.
    k. **Wu et al., 2023.** *Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models*. arXiv preprint arXiv:2401.00396.
    l. "Retrieval augmentation (RAG) (Lewis et al., 2020; Gao et al., 2023) is then proposed to enable LLMs to interact with external knowledge sources, where relevant texts are retrieved and serve as contexts to improve the factuality of LLMs (shown in Figure 1 (a))."
    m. **Lewis et al., 2020.** *Retrieval-augmented generation for knowledge-intensive nlp tasks*. Advances in Neural Information Processing Systems, 33:9459–9474.
    n. **Gao et al., 2023.** *Retrieval-augmented generation for large language models: A survey*. arXiv preprint arXiv:2312.10997.
    o. "However, retrieval augmentation assumes that knowledge is well represented in individual text units and ignores the correlations among multiple text units."
    p. "In real-world scenarios, text units are generally interconnected, forming a (text-attributed) graph."
    q. "The knowledge of such graphs is reflected not only in the form of texts but also in the structure of their connections."
    r. "For example, academic papers in a bibliographic graph are linked by citation links (Wang et al., 2020)."
    s. **Wang et al., 2020.** *Microsoft academic graph: When experts are not enough*. Quantitative Science Studies, 1(1):396-413.
    t. "We can trace the source of a research direction (Bai et al., 2019) by traversing such a graph."
    u. **Bai et al., 2019.** *Scientific paper recommendation: A survey*. IEEE Access, 7:9324-9339.
    v. "Cases and opinions in a legal graph are interconnected by reference edges (Sadeghian et al., 2018)."
    w. **Sadeghian et al., 2018.** *Automatic semantic edge labeling over legal citation graphs*. Artificial Intelligence and Law, 26:127-144.
    x. "We can verify the judgment for a case by looking up its citations on such a graph (Chen et al., 2019)."
    y. **Chen et al., 2019.** *Learning to predict charges for judgment with legal graph*. In Artificial Neural Networks and Machine Learning–ICANN 2019: Text and Time Series: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings, Part IV 28, pages 240–252. Springer.
    z. "Although widely used for text corpora as external knowledge sources, retrieval-augmentation cannot be readily used to augment LLMs with graphs for two reasons: 1) Structure Context: Retrieval augmentation can find individual nodes/texts from the graphs which can serve as context to augment the LLMs. However, knowledge on the graph also lies in the structure which can not be captured by single nodes/texts. 2) Graph Size Explosion: Although it is feasible to convert local subgraph structures into text descriptions as the input contexts to LLMs, the size of the local subgraph increases exponentially as the hop number increases, resulting in an excessively long context sequence. This could cause LLMs to be lost in the middle (Liu et al., 2023)."
    aa. **Liu et al., 2023.** *Lost in the middle: How language models use long contexts*. arXiv preprint arXiv:2307.03172.
    bb. "Therefore, it is an important research topic to augment LLMs with such graph information."
    cc. "Unfortunately, there has been a lack of benchmark datasets to support the development of methodology and facilitate the evaluation of the proposed models."
    dd. "To this end, we first construct a Graph Reasoning benchmark dataset called GRBENCH."
    ee. "GRBENCH includes ten real-world graphs that can serve as external knowledge sources for LLMs from five domains including academic, e-commerce, literature, healthcare, and legal domains."
    ff. "Each sample in GRBENCH consists of a manually designed question and an answer, which can be directly answered by referring to the graphs or retrieving the information from the graphs as context."
    gg. "To make the dataset comprehensive, we include samples of different difficulty levels: easy questions (which can be answered with single-hop reasoning on graphs), medium questions (which necessitate multi-hop reasoning on graphs), and hard questions (which call for inductive reasoning with information on graphs as context)."
    hh. "We propose a simple and effective framework called Graph Chain-of-thought (GRAPH-COT)."
    ii. "The main idea is to enable LLMs to traverse the graph step-by-step to figure out the key information needed, rather than directly feeding the whole subgraph as context into the LLMs (shown in Figure 1 (b))."
    jj. "GRAPH-COT is an iterative framework, where one iteration corresponds to one step on the graph."
    kk. "Each iteration in GRAPH-COT consists of three sub-steps: 1) Reasoning: LLMs propose what conclusion we can make with the current information and what further information is needed from the graph; 2) Interaction: LLMs generate the interactions needed to fetch information from the graph (e.g., finding the nodes, checking the neighbors, etc); 3) Execution: The requests from the interaction step are executed on the graph and the corresponding information is returned."
    ll. "In this way, LLMs can conduct chain-based reasoning on the graph and find the key information on the graph."
    mm. "This process will be iterated until LLMs conclude the final answer in the reasoning sub-step."


**2.2 Preliminaries**

- **Key Points:** Defines the basic concepts of graphs, including node sets, edge sets, and features associated with nodes. Introduces the concept of text-attributed graphs, where features are represented as text. Defines the concepts of neighbors and degree within a graph.
- **Significant Citations:** None


**2.3 GRBENCH Dataset**

- **Key Points:** Describes the GRBENCH dataset, a manually constructed benchmark dataset for evaluating LLMs' ability to reason on graphs. Explains the dataset's structure, including the domains covered (academia, e-commerce, literature, healthcare, and legal), the types of questions included (easy, medium, and hard), and the process of dataset creation.
- **Significant Citations:**

    a. "We create the GRBENCH dataset to evaluate how effectively LLMs can interact with domain-specific graphs containing rich knowledge to solve the desired problem."
    b. "GRBENCH contains 10 graphs from 5 general domains (academia, e-commerce, literature, healthcare, and legal)."
    c. "Each data sample in GRBENCH is a question-answer pair."
    d. "The questions are designed to simulate the real-world use cases in specific domains."
    e. "However, it is hard for LLMs to answer those questions directly with their internal knowledge stored in model parameters; they need to interact with external domain-specific graphs."
    f. "To curate high-quality and diverse data without heavy human effort, the construction of GRBENCH contains four steps: 1) We first collect large reference graph data from real-world scenarios which can serve as the context for data generation. 2) Then, we manually design question templates which can be answered on the reference graph data. 3) After that, we call GPT-4 to generate diverse question expressions for each question template. 4) Finally, we automatically generate ground truth answers from the domain-specific graphs."
    g. "We collect data from five domains where the knowledge lies in the format of graphs: academia, e-commerce, literature, healthcare, and legal."
    h. "In the academic domain, papers, authors, and venues are naturally interconnected by citation, “written-by”, and “publish-in" relations."
    i. "We obtain academic graphs across six disciplines including Biology, Computer Science, Chemistry, Material Science, Medicine, and Physics from DBLP (Tang et al., 2008) and Microsoft Academic Graph (MAG) (Wang et al., 2020; Zhang et al., 2023a)."
    j. **Tang et al., 2008.** *Arnetminer: Extraction and mining of academic social networks*. In KDD’08, pages 990-998.
    k. **Wang et al., 2020.** *Microsoft academic graph: When experts are not enough*. Quantitative Science Studies, 1(1):396-413.
    l. **Zhang et al., 2023a.** *Exploring the potential of large language models (llms) in learning on graphs*. arXiv preprint arXiv:2307.03393.
    m. "Nodes on such graphs are papers, authors, and venues, while edges include citation edges, authorship edges, and venueship edges."
    n. "In the e-commerce domain, a single product is assigned a brand, and different products are interlinked through “also-viewed" or "also-bought" relationships, which naturally embody graph-like structures."
    o. "We use Amazon product datasets (He and McAuley, 2016), which provides the metadata information of items across a myriad of product categories."
    p. **He and McAuley, 2016.** *Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering*. In proceedings of the 25th international conference on world wide web, pages 507-517.
    q. "Nodes on this graph are items and brands, while edges include "also-viewed”, “also-bought", "buy-after-viewing”, “bought-together", and "item-brand"."
    r. "In the literature domain, the inherent graph structure exists with interconnections between books, authors, publishers, and series."
    s. "The Goodreads dataset (Wan and McAuley, 2018) offers an extensive collection of books with their metadata."
    t. **Wan and McAuley, 2018.** *Item recommendation on monotonic behavior chains*. In Proceedings of the 12th ACM conference on recommender systems, pages 86-94.
    u. "Nodes on this graph are books, authors, publishers, and series, while edges include "written-by", "publish-in”, “book-series" and so on."
    v. "In the healthcare domain, we can construct a graph by considering the diseases with their associated properties."
    w. "We adopt the biological disease graph Hetionet (Himmelstein et al., 2017), which comprehensively summarizes existing disease and their symptoms, with the aim of repurposing drugs."
    x. **Himmelstein et al., 2017.** *Systematic integration of biomedical knowledge prioritizes drugs for repurposing*. Elife, 6:e26726.
    y. "Nodes on this graph include diseases, symptoms, side effects, compounds, and so on, while edges include "disease-present-symptom”, “compound-cause-side effect" and so on."
    z. "In the legal domain, there are rich citation links between cases and opinions (since judges rely on citing opinions from previous cases to write for the current case) which naturally form a graph."
    aa. "We use the data from CourtListener."
    bb. "Nodes on this graph are opinion, opinion-cluster, docket, and court, while edges include “opinion-citation”, “opinion-cluster", "cluster-docket", and "docket-court"."
    cc. "The question generation phase aims to generate questions that can be answered by LLMs after referring to the domain graphs."
    dd. "Considering that the generated questions should be accurate and meaningful, we ask four well-trained computer science Ph.D. students to write potential questions that can be answered given the graphs as context."
    ee. "To comprehensively evaluate the LLMs and their capability to interact with graphs, we ask the annotators to design question templates of three different difficulties:"
    ff. "Easy: These questions can be answered by looking up the feature/degree of only one node or travel on the graph within one hop."
    gg. "Medium: These questions require reasoning on the graphs for more than one hop and involve returning the feature/degree of nodes."
    hh. "Hard: These questions cannot be directly answered by looking up the graph, but the graph can be useful by providing informative context."
    ii. "It is worth noting that the easy-level and medium-level questions can be answered from the given graph, while the ground truth for hard questions cannot be directly found in the graph."
    jj. "Once the question templates are manually designed, we extract values from the graph to transform the templates into actual questions."
    kk. "Following the previous steps, we obtain question samples for each graph."
    ll. "However, all samples pertaining to the same template will share the same expressions."
    mm. "To this end, we propose to use GPT-4 to paraphrase each question template into five different expressions so that we can have more diverse question samples regarding the same type of question."
    nn. "The final step is to obtain the ground truth answer from the graph for each generated question."
    oo. "To achieve this goal, we first implement graph functions (e.g., neighbor check, degree check), which can be utilized to reason on the graph."
    pp. "Then we implement function chains which can serve as a combination of graph functions in order to fetch the ground truth answer from the graph."
    qq. "The function chains are manually written by annotators for each type of question."


**2.4 Graph Chain-of-Thought**

- **Key Points:** Introduces the GRAPH-COT framework, an iterative approach that enables LLMs to interact with graphs and reason step-by-step. Explains the three sub-steps within each iteration: reasoning with LLMs, interaction between LLMs and graphs, and execution on graphs. Details the four pre-defined graph functions (RetrieveNode, NodeFeature, NeighborCheck, NodeDegree) that facilitate LLM-graph interaction. Discusses the connection of GRAPH-COT to LLM agents and environments.
- **Significant Citations:**

    a. "The straightforward solution to let LLMs interact with the graph is through retrieval-augmentation generation (RAG) (Lewis et al., 2020; Gao et al., 2023), where a retriever fetches related information from graphs as context for LLM generation."
    b. **Lewis et al., 2020.** *Retrieval-augmented generation for knowledge-intensive nlp tasks*. Advances in Neural Information Processing Systems, 33:9459–9474.
    c. **Gao et al., 2023.** *Retrieval-augmented generation for large language models: A survey*. arXiv preprint arXiv:2312.10997.
    d. "However, different from text corpus as the external knowledge source, the information in graphs also lies in the complex interconnection between the text units, which poses a potential requirement for traversing and reasoning on graphs."
    e. "To enable LLMs to reason, Chain-of-thought (Wei et al., 2022) is proposed to encourage LLMs to decompose complex tasks into several steps."
    f. **Wei et al., 2022.** *Chain-of-thought prompting elicits reasoning in large language models*. Advances in Neural Information Processing Systems, 35:24824–24837.
    g. "However, it is designed for reasoning on texts and leaves reasoning on graphs with LLMs an open question."
    h. "To this end, we design a simple solution named Graph Chain-of-Thought (GRAPH-COT) to tackle the complex graph reasoning problem with LLMs (shown in Figure 2)."
    i. "GRAPH-COT is an iterative framework, with three steps in each iteration: reasoning, interaction, and execution."
    j. "Given the question or the previous iteration context, the first step is to let the LLMs conduct reasoning on what further external information from graphs is needed to answer the question, or if the question is answerable with the current contexts from graphs."
    k. "Based on the output results from the previous LLM reasoning step, the next step is to let LLMs know how to interact with the graphs and fetch relevant information from the graphs."
    l. "Inspired by (Yao et al., 2022), we pre-define four graph functions to cover both the semantic information and structure information on the graphs:"
    m. **Yao et al., 2022.** *React: Synergizing reasoning and acting in language models*. arXiv preprint arXiv:2210.03629.
    n. "RetrieveNode(Text): Identify related nodes in the graph with semantic search."
    o. "NodeFeature (NodeID, FeatureName): Extract the textual feature information from the graph for a specific node."
    p. "NeighborCheck(NodeID, NeighborType): Return the neighboring information in the graph for a specific node."
    q. "NodeDegree(NodeID, NeighborType): Return the degree of a specific neighbor type for a specific node in the graph."
    r. "The task at hand requires LLMs to generate accurate graph function calls, based on their previous reasoning results, to effectively interact with the graph."
    s. "The final step is to call those functions given by the previous step and fetch the relevant information from the graph."
    t. "The whole framework will be iterated until the LLM finishes the reasoning and outputs the final answer."
    u. "In this work, we enable LLMs to learn how to conduct GRAPH-COT with in-context learning (Dong et al., 2022)."
    v. **Dong et al., 2022.** *A survey for in-context learning*. arXiv preprint arXiv:2301.00234.
    w. "The prompts and demonstrations can be found in Appendix E."
    x. "It is worth mentioning that GRAPH-COT can be seen as an agent framework (Xi et al., 2023), where the LLM backbones are the agents and the graphs are the environments."
    y. **Xi et al., 2023.** *The rise and potential of large language model based agents: A survey*. arXiv preprint arXiv:2309.07864.
    z. "The agents (LLMs) can interact with the environment (graphs) with some predefined functions (defined in this section above)."


**2.5 Experiments**

- **Key Points:** Describes the experimental setup, including the baseline methods (Base LLMs, Text RAG LLMs, Graph RAG LLMs) and the LLM backbones used (LLaMA-2-13b-chat, Mixtral-8x7b-Instruct, GPT-3.5-turbo). Explains the evaluation metrics (Rouge-L, GPT4score) and the implementation settings.
- **Significant Citations:**

    a. "Base LLMs: We test if the LLMs can answer the given question with their knowledge without interacting with external data."
    b. "Text RAG LLMS (Gao et al., 2023): We treat the external graphs as pure text corpora and utilize a retriever to retrieve relevant text information from them."
    c. **Gao et al., 2023.** *Retrieval-augmented generation for large language models: A survey*. arXiv preprint arXiv:2312.10997.
    d. "Graph RAG LLMs: This is an extension of text RAG, where not only the retrieved text/node but also the subgraph associated with it is linearized into a text sequence (Ye et al., 2023) and serves as the context."
    e. **Ye et al., 2023.** *Natural language is all a graph needs*. arXiv preprint arXiv:2308.07134.
    f. "For all categories of baselines, we explore three LLM backbones, including LLaMA-2-13b-chat (Touvron et al., 2023), Mixtral-8x7b-Instruct (Jiang et al., 2024), and GPT-3.5-turbo (Ouyang et al., 2022)."
    g. **Touvron et al., 2023.** *Llama 2: Open foundation and fine-tuned chat models*. arXiv preprint arXiv:2307.09288.
    h. **Jiang et al., 2024.** *Mixtral of experts*. arXiv preprint arXiv:2401.04088.
    i. **Ouyang et al., 2022.** *Training language models to follow instructions with human feedback*. Advances in Neural Information Processing Systems, 35:27730–27744.
    j. "We use both rule-based metrics and model-based metrics to comprehensively evaluate the model results."
    k. "For the former, we use Rouge-L(R-L), which measures the longest common subsequence of words between the responses and the ground truth answers."
    l. "For the latter, we call GPT-4 to measure if the model output and ground truth are the same."
    m. "We calculate the percentage of "correct" predicted by GPT-4 as GPT4score."
    n. "All experiments are conducted on NVIDIA GeForce RTX A6000 GPUs with Python 3.8 and Huggingface 4.36.2."
    o. "We use Mpnet-v2 as the retriever for all the baselines and our method and implement the indexing with FAISS (Johnson et al., 2019)."
    p. **Johnson et al., 2019.** *Billion-scale similarity search with GPUs*. IEEE Transactions on Big Data, 7(3):535-547.
    q. "In GRAPH-COT, we adopt GPT-3.5-turbo-16k (Jan 2024) as the backbone LLM in the main results and set the temperature t to 0 for consistent responses."


**2.6 Overall Performance**

- **Key Points:** Presents the main results of the experiments, showing that GRAPH-COT consistently outperforms the baselines across different domains and LLM backbones. Discusses the performance of different baseline methods and highlights the limitations of the absolute scores achieved.
- **Significant Citations:** None


**2.7 Ablation Study**

- **Key Points:** Investigates the importance of demonstrations in GRAPH-COT through zero-shot and cross-domain experiments. Finds that demonstrations are crucial for performance and that GRAPH-COT is relatively robust to domain shifts in demonstrations.
- **Significant Citations:**

    a. "How Important are the Demonstrations for GRAPH-COT? To answer this question, we conduct experiments from two aspects: zero-shot study (no demonstrations) and cross-domain study (demonstrations from other domains (Ding et al., 2018))."
    b. **Ding et al., 2018.** *Graph adaptive knowledge transfer for unsupervised domain adaptation*. In Proceedings of the European Conference on Computer Vision (ECCV), pages 37-52.
    c. "The results are shown in Figure 3, where the columns and rows correspond to the source domain and target domain respectively."
    d. "For the zero-shot study, no demonstrations are given (right-est column in Figure 3)."
    e. "We empirically find that given no reasoning demonstrations, GRAPH-COT cannot work in all the datasets (nearly 0 performance)."
    f. "This implies that the LLMs suffer if given insufficient instructions (only graph definition and interaction function definitions)."
    g. "For the cross-domain study, we provide demonstrations from the source domain graphs and test on the target domain graphs."
    h. "From the result (left five columns in Figure 3), in-domain demonstrations (diagonal) perform quite well and GRAPH-COT is overall robust to demonstration domain-shift."


**2.8 How Different LLMs Perform in GRAPH-COT?**

- **Key Points:** Explores the impact of different LLM backbones on GRAPH-COT's performance. Finds that LLMs with stronger instruction-following and reasoning abilities lead to better results.
- **Significant Citations:** None


**2.9 RAG vs GRAPH-COT**

- **Key Points:** Compares the effectiveness of GRAPH-COT with retrieval-augmented LLMs (RAG) that utilize subgraphs as context. Shows that GRAPH-COT outperforms RAG, particularly when dealing with larger subgraphs due to the limitations of LLMs' input length.
- **Significant Citations:** None


**2.10 GRAPH-COT on Questions of Different Difficulty Levels in GRBENCH**

- **Key Points:** Analyzes GRAPH-COT's performance on questions of varying difficulty levels (easy, medium, and hard). Finds that GRAPH-COT performs well on easy questions but struggles with medium and hard questions that require more complex reasoning.
- **Significant Citations:** None


**2.11 Case Studies of GRAPH-COT**

- **Key Points:** Presents two failure cases of GRAPH-COT to illustrate its limitations. Highlights potential issues related to LLMs' understanding of semantic meaning and graph structure.
- **Significant Citations:** None


**2.12 Related Work**

- **Key Points:** Reviews related work in two main areas: LLMs on graphs and augmenting LLMs with external knowledge. Discusses various approaches for leveraging LLMs in graph-related tasks, including feature extraction, prediction, and graph-nested language models. Highlights the limitations of existing work in addressing complex graph reasoning and the focus on traditional graph tasks. Discusses the existing literature on augmenting LLMs with external knowledge sources, primarily text corpora, and the retrieval augmentation framework. Positions the current work as a novel approach to augmenting LLMs with graph-structured knowledge.
- **Significant Citations:**

    a. "Inspired by the recent success of LLMs on natural language processing tasks, researchers are exploring solving graph tasks with LLMs (Jin et al., 2023a)."
    b. **Jin et al., 2023a.** *Large language models on graphs: A comprehensive survey*. arXiv preprint arXiv:2312.02783.
    c. "The main idea is to serve LLMs as the feature extractor (Chen et al., 2023) or final predictor (Jin et al., 2023b)."
    d. **Chen et al., 2023.** *Exploring the potential of large language models (llms) in learning on graphs*. arXiv preprint arXiv:2307.03393.
    e. **Jin et al., 2023b.** *Patton: Language model pretraining on text-rich networks*. arXiv preprint arXiv:2305.12268.
    f. "For the former, many methods adopt a LLM-GNN cascaded structure (Chien et al., 2021), where LLMs extract node features for graph neural networks (GNNs) (Wu et al., 2020)."
    g. **Chien et al., 2021.** *Node feature extraction by self-supervised multi-scale neighborhood prediction*. arXiv preprint arXiv:2111.00064.
    h. **Wu et al., 2020.** *A comprehensive survey on graph neural networks*. IEEE transactions on neural networks and learning systems, 32(1):4-24.
    i. "For example, SimTeG (Duan et al., 2023) proposes to first warm up the LLM feature extractor before training the whole pipeline."
    j. **Duan et al., 2023.** *Simteg: A frustratingly simple approach improves textual graph learning*. arXiv preprint arXiv:2308.02565.
    k. "GLEM (Zhao et al., 2022) introduces an iterative pipeline where GNNs can provide feedback for LLM feature extractors."
    l. **Zhao et al., 2022.** *Learning on large-scale text-attributed graphs via variational inference*. arXiv preprint arXiv:2210.14709.
    m. "For the latter, existing works transfer the structure information into a sequence to feed into LLMs (Tian et al., 2023; Xiong et al., 2024) or design advanced graph-empowered LLMs (Yang et al., 2021)."
    n. **Tian et al., 2023.** *Graph neural prompting with large language models*. arXiv preprint arXiv:2309.15427.
    o. **Xiong et al., 2024.** *Large language models can learn temporal reasoning*. arXiv preprint arXiv:2401.06853.
    p. **Yang et al., 2021.** *Graphformers: Gnn-nested transformers for representation learning on textual graph*. Advances in Neural Information Processing Systems, 34:28798-28810.
    q. "For example, InstructGLM (Ye et al., 2023) utilizes natural language to describe graph structure."
    r. **Ye et al., 2023.** *Tree of thoughts: Deliberate problem solving with large language models*. arXiv preprint arXiv:2305.10601.
    s. "Heterformer (Jin et al., 2023c) proposes a graph-nested language model architecture."
    t. **Jin et al., 2023c.** *Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks*. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1020-1031.
    u. "However, most existing works mainly focus on traditional graph tasks such as node classification (Xiao et al., 2022) and link prediction (Zhang and Chen, 2018)."
    v. **Xiao et al., 2022.** *Graph neural networks in node classification: survey and evaluation*. Machine Vision and Applications, 33:1-19.
    w. **Zhang and Chen, 2018.** *Link prediction based on graph neural networks*. Advances in neural information processing systems, 31.
    x. "On the other hand, Graph-of-thought (Besta et al., 2023) proposes to conduct LLM reasoning with graph-structured thinking."
    y. **Besta et al., 2023.** *Graph of thoughts: Solving elaborate problems with large language models*. arXiv preprint arXiv:2308.09687.
    z. "Nevertheless, it mainly focuses on text-based reasoning rather than referring to external graphs."
    aa. "In our work, we research the question of augmenting LLMs with external graphs by conducting graph reasoning with LLMs."
    bb. "Although LLMs (Touvron et al., 2023; Jiang et al., 2024) have shown their superb language understanding and generation capability (Zhao et al., 2023), they encounter issues with generating misleading information that seems credible but lacks factual basis, a phenomenon known as hallucination (Tonmoy et al., 2024; Rawte et al., 2023)."
    cc. **Touvron et al., 2023.** *Llama 2: Open foundation and fine-tuned chat models*. arXiv preprint arXiv:2307.09288.
    dd. **Jiang et al., 2024.** *Mixtral of experts*. arXiv preprint arXiv:2401.04088.
    ee. **Zhao et al., 2023.** *A comprehensive survey of large language models on graphs*. arXiv preprint arXiv:2312.02783.
    ff. **Tonmoy et al., 2024.** *A comprehensive survey of hallucination mitigation techniques in large language models*. arXiv preprint arXiv:2401.01313.
    gg. **Rawte et al., 2023.** *A survey of hallucination in large foundation models*. arXiv preprint arXiv:2309.05922.
    hh. "To alleviate such an issue, existing works (Shuster et al., 2021) propose to augment LLMs with text corpora as external knowledge sources, with the retrieval-augmentation framework proposed (Lewis et al., 2020; Gao et al., 2023)."
    ii. **Shuster et al., 2021.** *Retrieval augmentation reduces hallucination in conversation*. arXiv preprint arXiv:2104.07567.
    jj. **Lewis et al., 2020.** *Retrieval-augmented generation for knowledge-intensive nlp tasks*. Advances in Neural Information Processing Systems, 33:9459–9474.
    kk. **Gao et al., 2023.** *Retrieval-augmented generation for large language models: A survey*. arXiv preprint arXiv:2312.10997.
    ll. "Before LLMs