## Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models

**1. Introduction**

- **Title:** Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models
- **Authors:** Yang Liu, Melissa Xiaohui Qin, Hongming Li, Chao Huang
- **Publication Date:** May 5, 2024
- **Objective:** The paper introduces LEXBENCH, a comprehensive benchmark for evaluating language models (LLMs) on semantic phrase processing tasks. It aims to bridge the gap in existing research by providing a unified framework for assessing LLM performance on various semantic phrase types, including idiomatic expressions, noun compounds, verbal constructions, and lexical collocations.
- **Number of References:** 73

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:** The introduction defines semantic phrases (SPs) and their importance in NLP, highlighting the challenges they pose for NLP systems. It discusses the existing research on SP processing, focusing on the four representative phenomena: idiomatic expressions, noun compounds, verbal constructions, and lexical collocations. The authors emphasize the need for a comprehensive evaluation framework to assess LLM capabilities in SP processing.
- **Significant Citations:**
    - **Claim:** Semantic phrases (SPs), also known as multiword expressions (MwE), are word combinations idiosyncratic concerning syntax or semantics.
    - **Citation:** Pasquer et al., 2020. They have been well explored in taxonomy and categorized into different types by their semantic relation of lexical combination, i.e., the lexical function (LF) (Mel'čuk, 1998).
    - **Relevance:** This citation establishes the definition and importance of SPs in the context of NLP research.
    - **Claim:** A fine-grained taxonomy of semantic phrases from a compositional perspective includes idiomatic expressions, noun compounds, and verbal constructions.
    - **Citation:** Ramisch, 2023. The issue of semantic phrase handling is crucial for NLP systems, where it raises many challenges (Constant et al., 2017b), making them "a pain in the neck" for NLP applications in a long time (Sag et al., 2002; Shwartz and Dagan, 2019).
    - **Relevance:** This citation highlights the specific types of SPs that the paper focuses on and emphasizes the challenges associated with their processing.
    - **Claim:** Relevant tasks of semantic phrase processing have been the focus of numerous research works.
    - **Citation:** Ramisch et al., 2023a; Wada et al., 2023; Tanner and Hoffman, 2023.
    - **Relevance:** This citation provides context for the paper's focus on evaluating LLM performance on SP processing tasks.

**2.2 Related Work**

- **Key Points:** This section reviews existing research on semantic phrase processing, focusing on the evaluation of semantic phrase processing in context, different tasks, and the development of resources for SP research. It highlights the growing interest in using LLMs for SP processing but notes the lack of a comprehensive benchmark for evaluating their performance.
- **Significant Citations:**
    - **Claim:** An extensive body of work exists on the evaluation of semantic phrase processing in context and also increasingly on different tasks.
    - **Citation:** Vacareanu et al., 2020; Arase and Tsujii, 2020; Klubička et al., 2023; Wada et al., 2023.
    - **Relevance:** This citation provides a broad overview of existing research on SP processing, highlighting the diversity of tasks and approaches.
    - **Claim:** (Ponkiya et al., 2020; Shwartz, 2021; Coil and Shwartz, 2023) shown that both few-shot LLMs and supervised fine-tuned T5 models can be well-performing in noun compound paraphrasing.
    - **Relevance:** This citation highlights the potential of LLMs for SP processing, but also points out the need for further research to evaluate their performance on a wider range of tasks.
    - **Claim:** Meanwhile, as pointed out recently by (Miletić and Walde, 2024), there currently exists a need for directly comparable evaluation framework to encompass comprehensive assessment of different semantic phrase phenomena.
    - **Relevance:** This citation emphasizes the need for a unified benchmark to compare different LLMs on SP processing tasks.

**2.3 LEXBENCH: Semantic Phrase Processing Benchmark**

- **Key Points:** This section introduces LEXBENCH, the proposed benchmark for evaluating LLM performance on SP processing tasks. It describes the framework's design, including the ten tasks, ten datasets, and five evaluation metrics. Each task is explained in detail, outlining its definition, data source, and evaluation metrics.
- **Significant Citations:**
    - **Claim:** We formalize the problem of semantic phrase processing as follows: In light of the input prompt template P and a semantic phrase together with their associated context S, the models are tasked with generating an output denoted as O.
    - **Citation:** For example, in extraction tasks, the model input would be I := POS, and O denotes the extracted phrase followed the task instruction described in P.
    - **Relevance:** This citation defines the general framework for evaluating LLM performance on SP processing tasks.
    - **Claim:** We use the test split of dataset from (Harish et al., 2021), which consists of 1,688 instances with different interpretation to 273 idioms.
    - **Relevance:** This citation provides the data source for the Idiomatic Expression Detection (IED) task.
    - **Claim:** Initially, we use the English subset of idiom data from ID10M (Tedeschi et al., 2022) as the data source.
    - **Relevance:** This citation provides the data source for the Idiomatic Expression Extraction (IEE) task.
    - **Claim:** We use the annotated idiom paraphrase data from (Zhou et al., 2021) and (Chakrabarty et al., 2022).
    - **Relevance:** This citation provides the data source for the Idiomatic Expression Interpretation (IEI) task.
    - **Claim:** We use the data of NCTTI (Garcia et al., 2021) and convert each phrase-level instance to the same MCQ problem format as IED.
    - **Relevance:** This citation provides the data source for the Noun Compound Compositionality (NCC) task.
    - **Claim:** We compiled and sampled 720 examples from the dataset PRONCI (Kolluru et al., 2022).
    - **Relevance:** This citation provides the data source for the Noun Compound Extraction (NCE) task.
    - **Claim:** We use the revised dataset of (Coil and Shwartz, 2023) based on (Hendrickx et al., 2013), which consists of 298 noun compounds with 11, 730 annotated paraphrases in total.
    - **Relevance:** This citation provides the data source for the Noun Compound Interpretation (NCI) task.
    - **Claim:** We construct the test set sampled from the English part of collocation identification data of (Fisas et al., 2020; Espinosa-Anke et al., 2022) but for extraction task usage.
    - **Relevance:** This citation provides the data source for the Lexical Collocation Extraction (LCE) task.
    - **Claim:** We use an in-context collocation dataset, the expanded LEXFUNC (Espinosa-Anke et al., 2021), as our initiation and sample 40 data points for each relation category.
    - **Relevance:** This citation provides the data source for the Lexical Collocation Interpretation (LCI) task.
    - **Claim:** We use a well-known annotated VMWE dataset, PARSEME-corpus-release-1.3 (Savary et al., 2023), and we process the data to make sure each data point only contains one VC in context.
    - **Relevance:** This citation provides the data source for the Verbal MWE Extraction (VMWE) task.

**2.4 Experimental Setup**

- **Key Points:** This section describes the experimental setup used in the paper, including the datasets, models, and evaluation metrics. It explains the rationale for selecting specific models and the implementation details for conducting the experiments.
- **Significant Citations:**
    - **Claim:** We introduced our curated datasets illustrated in §3 and §B.
    - **Relevance:** This statement introduces the datasets used in the experiments, which are described in detail in the Appendix.
    - **Claim:** As our experimental baselines, we adopt fifteen currently popular NLP systems across different architectures and model scales, with strong semantic understanding ability.
    - **Relevance:** This statement introduces the models used in the experiments, which are listed in Table 7.
    - **Claim:** We probe the zero-shot and few-shot (three- and five-shot) performance for the inference-only models.
    - **Relevance:** This statement describes the experimental settings used for evaluating the models.
    - **Claim:** We utilized temperature with T = 0 on decoding parameters and used top-p decoding (Holtzman et al., 2019) with p = 1.0 for all the models.
    - **Relevance:** This citation provides the specific settings used for decoding the models' outputs.

**2.5 Benchmarking Results**

- **Key Points:** This section presents the main results of the experiments, comparing the performance of different models on the ten tasks in LEXBENCH. It highlights the key findings, including the superior performance of GPT-4, the impact of model scale on performance, and the effects of in-context learning.
- **Significant Citations:**
    - **Claim:** Significantly, GPT-4 (gpt-4-1106-preview) demonstrates comprehensive and superior performance compared to other models across various phrase types in the categorization, extraction, and interpretation tasks exhibiting notably higher average scores and achieving the top tier in six out of twelve sub tasks.
    - **Relevance:** This statement highlights the superior performance of GPT-4 compared to other models.
    - **Claim:** In most of the tasks, the current state-of-the-art open-source models (e.g., Mixtral-8x7B-inst and Llama-70B-chat) still lag behind several proprietary models in roughly the same level of model size, indicating that there remains a considerable performance gap between the two, in terms of the domain of semantic phrase processing.
    - **Relevance:** This statement highlights the performance gap between open-source and proprietary models.
    - **Claim:** The significant accuracy increase with 7B, 13B, and 70B model scales in IED is 25.6% → 37.7%→ 47.9%.
    - **Relevance:** This statement highlights the impact of model scale on performance, supporting the Scaling Law (Kaplan et al., 2020).
    - **Claim:** However, as reported in Table 5, it does not exhibit a significant gap between large and smaller models in semantic similarity-based measurement for the three interpretation tasks.
    - **Relevance:** This statement highlights the limitations of using similarity-based metrics for evaluating interpretation tasks.
    - **Claim:** The results of experiments (cf. Table 3 and Table 5) highlight the effectiveness of ICL.
    - **Relevance:** This statement highlights the impact of in-context learning on model performance.

**2.6 Semantic Category Scaling with In-Context Learning**

- **Key Points:** This section investigates the impact of semantic category scaling on LLM performance in the Lexical Collocation Categorization (LCC) task. It shows that the accuracy of LLMs decreases as the number of categories increases, but they still outperform the random baseline.
- **Significant Citations:**
    - **Claim:** We also run the one-class classification to ablate the impact of the instruction-following capacity of models.
    - **Relevance:** This statement describes the experimental setup used to investigate the impact of instruction-following capacity.

**2.7 VMWE Extraction via ORACLE PROMPTING**

- **Key Points:** This section explores the effectiveness of using ORACLE PROMPTING, a strategy that provides the specific definition of the target phrase type in the prompt, for improving LLM performance on the Verbal MWE Extraction (VMWE) task. It shows that ORACLE PROMPTING significantly improves the accuracy of LLMs.
- **Significant Citations:**
    - **Claim:** As an additional analysis, we explore the strategies of prompting methods with in-context learning in the VMWE extraction of LEXBENCH.
    - **Relevance:** This statement introduces the focus of this section.

**2.8 Discussion and Takeaways**

- **Key Points:** This section discusses the key findings of the paper, highlighting the strengths and limitations of LLMs in SP processing. It raises questions for future research, focusing on the need for specialized models for specific SP tasks, the challenges of handling discontinuous semantic phrases, and the potential of LLMs as a general phrase processing system.
- **Significant Citations:**
    - **Claim:** Meanwhile, as pointed out recently by (Miletić and Walde, 2024), there currently exists a need for directly comparable evaluation framework to encompass comprehensive assessment of different semantic phrase phenomena.
    - **Relevance:** This citation highlights the need for further research on SP processing.

**2.9 Conclusions**

- **Key Points:** This section summarizes the paper's contributions, highlighting the introduction of LEXBENCH, the comprehensive evaluation of LLMs on SP processing tasks, and the insights gained from the experimental results. It emphasizes the need for further research to improve LLM performance on SP processing tasks and to explore the potential of LLMs as a general phrase processing system.
- **Significant Citations:**
    - **Claim:** In this work, we introduced LEXBENCH, the first benchmark tailored for semantic phrase processing with diverse LMs.
    - **Relevance:** This statement summarizes the paper's main contribution.

**3. Key Insights and Supporting Literature**

- **Insight:** LLMs exhibit significant progress in SP processing, particularly in interpretation tasks, but still lag behind fine-tuned models in categorization and extraction tasks.
    - **Supporting Citations:** (Espinosa-Anke et al., 2021; Shvets and Wanner, 2022; Coil and Shwartz, 2023; Chakrabarty et al., 2022; Zhang et al., 2024)
    - **Explanation:** These citations highlight the advancements in LLM capabilities for SP processing, particularly in interpretation tasks. However, they also acknowledge the limitations of LLMs in categorization and extraction tasks, suggesting the need for further research to improve their performance in these areas.
- **Insight:** Model scale significantly impacts LLM performance on SP processing tasks, supporting the Scaling Law (Kaplan et al., 2020).
    - **Supporting Citations:** (Kaplan et al., 2020)
    - **Explanation:** This citation provides theoretical support for the observed relationship between model scale and performance on SP processing tasks.
- **Insight:** In-context learning can improve LLM performance on SP processing tasks, but its effectiveness varies across tasks and models.
    - **Supporting Citations:** (Chen et al., 2024; Zhou et al., 2023; Agrawal et al., 2022; Wadhwa et al., 2023; Coil and Shwartz, 2023; Chakrabarty et al., 2022; Zhang et al., 2024)
    - **Explanation:** These citations highlight the potential of in-context learning for improving LLM performance on SP processing tasks. However, they also acknowledge the need for further research to understand the factors that influence its effectiveness and to develop strategies for optimizing its use.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:** The paper uses a comprehensive set of ten tasks, ten datasets, and five evaluation metrics to assess LLM performance on SP processing. It evaluates fifteen models, including both open-source and proprietary models, across different architectures and model scales. The experiments are conducted using both zero-shot and few-shot prompting settings.
- **Foundations:** The authors cite several works to justify their methodology, including:
    - **Dataset Creation:** (Harish et al., 2021; Tedeschi et al., 2022; Zhou et al., 2021; Chakrabarty et al., 2022; Garcia et al., 2021; Kolluru et al., 2022; Coil and Shwartz, 2023; Fisas et al., 2020; Espinosa-Anke et al., 2019, 2021; Savary et al., 2023)
    - **Model Selection:** (Devlin et al., 2019; Raffel et al., 2020; Touvron et al., 2023b; Bi et al., 2024; Jiang et al., 2023; OpenAI, 2022, 2023; Anthropic, 2023, 2024; Google, 2023)
    - **Evaluation Metrics:** (Lin, 2004; Zhang et al., 2019; Jelinek et al., 1977; Holtzman et al., 2019; Kwon et al., 2023)
- **Novel Aspects:** The paper introduces ORACLE PROMPTING, a novel prompting strategy that provides the specific definition of the target phrase type in the prompt, to improve LLM performance on the VMWE extraction task. The authors do not cite any specific works to justify this novel approach.

**5. Results in Context**

- **Main Results:** The paper finds that GPT-4 outperforms other models on most tasks, demonstrating the significant impact of model scale on performance. In-context learning improves LLM performance on some tasks, but its effectiveness varies across tasks and models. The authors also observe a performance gap between open-source and proprietary models, suggesting that further research is needed to improve the capabilities of open-source models.
- **Comparison with Existing Literature:** The authors compare their findings with existing literature on SP processing, highlighting the advancements in LLM capabilities for SP processing, particularly in interpretation tasks. However, they also acknowledge the limitations of LLMs in categorization and extraction tasks, suggesting the need for further research to improve their performance in these areas.
- **Confirmation, Contradiction, or Extension:** The paper's findings confirm the importance of model scale for LLM performance, as suggested by the Scaling Law (Kaplan et al., 2020). However, the paper also highlights the limitations of using similarity-based metrics for evaluating interpretation tasks, which contradicts the findings of some previous studies. The paper extends the existing research on SP processing by introducing a comprehensive benchmark for evaluating LLM performance on a wider range of tasks and by exploring the effectiveness of ORACLE PROMPTING, a novel prompting strategy.

**6. Discussion and Related Work**

- **Situating the Work:** The authors situate their work within the existing literature by highlighting the growing interest in using LLMs for SP processing but noting the lack of a comprehensive benchmark for evaluating their performance. They emphasize the need for further research to understand the strengths and limitations of LLMs in SP processing and to develop strategies for optimizing their use.
- **Key Papers Cited:** (Miletić and Walde, 2024; Pasquer et al., 2020; Espinosa-Anke et al., 2021; Shvets and Wanner, 2022; Coil and Shwartz, 2023; Chakrabarty et al., 2022; Zhang et al., 2024; Kaplan et al., 2020; Chen et al., 2024; Zhou et al., 2023; Agrawal et al., 2022; Wadhwa et al., 2023)
- **Novelty and Importance:** The authors highlight the novelty of their work by introducing LEXBENCH, the first comprehensive benchmark for evaluating LLM performance on SP processing tasks. They also emphasize the importance of their work by addressing the need for a unified framework to compare different LLMs on SP processing tasks and by exploring the effectiveness of ORACLE PROMPTING, a novel prompting strategy.

**7. Future Work and Open Questions**

- **Areas for Further Research:** The authors suggest several areas for further research, including:
    - Developing a more comprehensive benchmark that includes a wider range of SP types and tasks.
    - Investigating the use of specialized models for specific SP tasks.
    - Exploring the challenges of handling discontinuous semantic phrases.
    - Investigating the potential of LLMs as a general phrase processing system.
- **Citations:** (Espinosa-Anke et al., 2021; Pham et al., 2023; Miletić and Walde, 2024)

**8. Critical Analysis of Citation Usage**

- **Effectiveness:** The authors effectively use citations to support their arguments and findings. They provide a comprehensive overview of existing research on SP processing, highlighting the key challenges and opportunities in this field. They also cite relevant works to justify their methodology and to support their claims about the performance of different models.
- **Areas for Improvement:** The authors could have provided more citations to support their claims about the limitations of LLMs in SP processing. They could also have provided more citations to justify their novel approach to prompting, ORACLE PROMPTING.
- **Potential Biases:** The authors primarily cite works from the NLP community, with a particular focus on research related to SP processing. They could have included citations from other fields, such as linguistics and cognitive science, to provide a more comprehensive perspective on SP processing.

**9. Final Summary**

- **Contribution:** The paper makes a significant contribution to the field of SP processing by introducing LEXBENCH, a comprehensive benchmark for evaluating LLM performance on a wide range of SP processing tasks. The paper also provides valuable insights into the strengths and limitations of LLMs in SP processing, highlighting the need for further research to improve their capabilities and to explore their potential as a general phrase processing system.
- **Influential Works:** (Mel'čuk, 1998; Pasquer et al., 2020; Espinosa-Anke et al., 2021; Shvets and Wanner, 2022; Coil and Shwartz, 2023; Chakrabarty et al., 2022; Zhang et al., 2024; Kaplan et al., 2020; Chen et al., 2024; Zhou et al., 2023; Agrawal et al., 2022; Wadhwa et al., 2023; Miletić and Walde, 2024)
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. It provides a comprehensive overview of existing research on SP processing, highlighting the key challenges and opportunities in this field. It also cites relevant works to justify its methodology and to support its claims about the performance of different models.

**Overall Assessment:** The paper makes a valuable contribution to the field of SP processing by introducing LEXBENCH, a comprehensive benchmark for evaluating LLM performance on a wide range of SP processing tasks. The paper also provides valuable insights into the strengths and limitations of LLMs in SP processing, highlighting the need for further research to improve their capabilities and to explore their potential as a general phrase processing system. The authors effectively use citations to support their arguments and findings, providing a comprehensive overview of existing research on SP processing. However, they could have provided more citations to support their claims about the limitations of LLMs in SP processing and to justify their novel approach to prompting, ORACLE PROMPTING. The authors primarily cite works from the NLP community, with a particular focus on research related to SP processing. They could have included citations from other fields, such as linguistics and cognitive science, to provide a more comprehensive perspective on SP processing.