Okay, here's the comprehensive analysis of the paper "Parameter-Efficient Fine-Tuning with Discrete Fourier Transform" in Markdown format, following the structure you provided:


# Parameter-Efficient Fine-Tuning with Discrete Fourier Transform: Citation Analysis

## 1. Introduction

- **Title:** Parameter-Efficient Fine-Tuning with Discrete Fourier Transform
- **Authors:** Ziqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu, Bingzhe Wu, Liang Chen, Jia Li
- **Publication Date:** 2024 (Proceedings of the 41st International Conference on Machine Learning)
- **Main Objective:** The research aims to develop a parameter-efficient fine-tuning method for large foundation models (LFMs) by leveraging the expressiveness of the Discrete Fourier Transform, achieving significant parameter reduction compared to existing methods like LoRA.
- **Total Number of References:** 75


## 2. Section-by-Section Analysis with Citation Extraction

### 2.1 Introduction

**Summary:** The introduction highlights the increasing popularity and computational cost of fine-tuning large foundation models (LFMs) for various tasks. It introduces the concept of low-rank adaptation (LoRA) as a parameter-efficient approach and motivates the need for further compression of trainable parameters.

**Significant Citations:**

1. **Claim:** "Large foundation models (LFMs) have demonstrated exceptional performance on tasks of multiple domains, including natural language processing (NLP) (Liu et al., 2019; He et al., 2020; Radford et al., 2019; Brown et al., 2020; Li et al., 2022) and computer vision (CV) (Liu et al., 2023a;b; Singh et al., 2022; Rombach et al., 2022)."
   - **Citation:** 
     - Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
     - He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.
     - Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
     - Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020.
     - Li, Y., Shen, W., Gao, J., and Wang, Y. Community question answering entity linking via leveraging auxiliary data. arXiv preprint arXiv:2205.11917, 2022.
     - Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023a.
     - Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023b.
     - Singh, A., Hu, R., Goswami, V., Couairon, G., Galuba, W., Rohrbach, M., and Kiela, D. Flava: A foundational language and vision alignment model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15638–15650, 2022.
     - Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10684-10695, 2022.
   - **Relevance:** This citation establishes the context of LFMs and their widespread use in NLP and CV, highlighting the growing need for efficient fine-tuning methods due to their increasing size and complexity.


2. **Claim:** "Fine-tuning LFMs for a wide range of downstream tasks has become prevalent (Wang et al., 2022; Taori et al., 2023; Qiu et al., 2020)."
   - **Citation:**
     - Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.
     - Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model, 2023.
     - Qiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. Pre-trained models for natural language processing: A survey. Science China Technological Sciences, 63(10): 1872-1897, 2020.
   - **Relevance:** This citation emphasizes the growing trend of fine-tuning LFMs for specific tasks, setting the stage for the paper's focus on developing more efficient fine-tuning techniques.


3. **Claim:** "Under the full fine-tuning paradigm, the new model adapted to each customized task typically contains as many parameters as the original model (Qiu et al., 2020; Raffel et al., 2020; Chen et al., 2024; Gao et al., 2024)."
   - **Citation:**
     - Qiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., and Huang, X. Pre-trained models for natural language processing: A survey. Science China Technological Sciences, 63(10): 1872-1897, 2020.
     - Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.
     - Chen, N., Li, Y., Tang, J., and Li, J. Graphwiz: An instruction-following language model for graph problems. arXiv preprint arXiv:2402.16029, 2024.
     - Gao, Z., Sun, X., Liu, Z., Li, Y., Cheng, H., and Li, J. Protein multimer structure prediction via prompt learning. arXiv preprint arXiv:2402.18813, 2024.
   - **Relevance:** This citation highlights the major drawback of full fine-tuning, which is the significant increase in the number of trainable parameters, further emphasizing the need for parameter-efficient alternatives.


4. **Claim:** "As a popular way to address this issue, LoRA (Hu et al., 2021) represents the weight change with two low-rank matrices A and B, i.e., W0+AW = Wo+BA."
   - **Citation:**
     - Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
   - **Relevance:** This citation introduces LoRA, a key method in the field of parameter-efficient fine-tuning, which the paper aims to improve upon.


### 2.2 Related Works

**Summary:** This section reviews existing parameter-efficient fine-tuning (PEFT) methods, categorizing them into weight-based and non-weight-based approaches. It then focuses on weight-based methods, particularly LoRA and its variants, and introduces the concept of sparse Fourier transform (SFT) in deep learning.

**Significant Citations:**

1. **Claim:** "Existing PEFT methods are broadly partitioned into two categories: non-weight-based and weight-based methods."
   - **Citation:** (No specific citation is provided for this general categorization, but the following citations are used to exemplify the categories)
   - **Relevance:** This claim sets the stage for the discussion of different PEFT approaches, which is crucial for understanding the paper's contribution within the existing literature.


2. **Claim:** "Adapter tuning (He et al., 2021; Rebuffi et al., 2017; Pfeiffer et al., 2020; Houlsby et al., 2019; Rücklé et al., 2020; Lin et al., 2020) aims to introduce light-weighted neural modules, called adapters, between pre-trained layers of the base model."
   - **Citation:**
     - He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. Towards a unified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021.
     - Rebuffi, S.-A., Bilen, H., and Vedaldi, A. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017.
     - Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., and Gurevych, I. Adapterfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020.
     - Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790-2799. PMLR, 2019.
     - Rücklé, A., Geigle, G., Glockner, M., Beck, T., Pfeiffer, J., Reimers, N., and Gurevych, I. Adapterdrop: On the efficiency of adapters in transformers. arXiv preprint arXiv:2010.11918, 2020.
     - Lin, Z., Madotto, A., and Fung, P. Exploring versatile generative language model via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829, 2020.
   - **Relevance:** This citation provides examples of non-weight-based PEFT methods, specifically adapter tuning, which the paper contrasts with its own weight-based approach.


3. **Claim:** "Prompt tuning (Brown et al., 2020; Lester et al., 2021; Gao et al., 2020; Diao et al., 2022) and prefix tuning (Li & Liang, 2021) insert additional prompts or prefix tokens to the layers of the base model."
   - **Citation:**
     - Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020.
     - Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.
     - Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020.
     - Diao, S., Huang, Z., Xu, R., Li, X., Lin, Y., Zhou, X., and Zhang, T. Black-box prompt learning for pre-trained language models. arXiv preprint arXiv:2201.08531, 2022.
     - Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.
   - **Relevance:** This citation provides further examples of non-weight-based PEFT methods, specifically prompt and prefix tuning, further illustrating the landscape of PEFT techniques.


4. **Claim:** "Weight-based methods, represented by LoRA (Hu et al., 2021), introduce and then update weight changes that can be merged with the original weights to avoid inference latency."
   - **Citation:**
     - Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
   - **Relevance:** This citation introduces LoRA as a representative example of weight-based PEFT methods, which is the primary focus of the paper's comparison and improvement efforts.


5. **Claim:** "AdaLoRA (Zhang et al., 2023) extends the LoRA method by distributing the parameter budget across weight matrices with importance scores."
   - **Citation:**
     - Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023.
   - **Relevance:** This citation introduces a variant of LoRA, AdaLoRA, which demonstrates the ongoing development and refinement of LoRA-based methods.


6. **Claim:** "Additionally, Q-LoRA (Dettmers et al., 2023) proposes to back-propagate gradients upon LoRA through a quantized pre-trained model with 4-bit NormalFloat."
   - **Citation:**
     - Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.
   - **Relevance:** This citation introduces another LoRA variant, Q-LoRA, further illustrating the active research in this area.


7. **Claim:** "Sparse Fourier transform (SFT) has flourished in various fields of deep learning (DL). The SFT technique mainly involves using sparse spectral coefficients of significant (Xu et al., 2020; Ehrlich & Davis, 2019; Gueguen et al., 2018; Tang et al., 2022) or even random (Lin et al., 2014; Rawat et al., 2019; Herrmann, 2010) spectral entries, for representation learning."
   - **Citation:**
     - Xu, K., Qin, M., Sun, F., Wang, Y., Chen, Y.-K., and Ren, F. Learning in the frequency domain. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1740–1749, 2020.
     - Ehrlich, M. and Davis, L. S. Deep residual learning in the jpeg transform domain. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 3484–3493, 2019.
     - Gueguen, L., Sergeev, A., Kadlec, B., Liu, R., and Yosinski, J. Faster neural networks straight from jpeg. Advances in Neural Information Processing Systems, 31, 2018.
     - Tang, J., Li, J., Gao, Z., and Li, J. Rethinking graph neural networks for anomaly detection. In International Conference on Machine Learning, pp. 21076–21089. PMLR, 2022.
     - Lin, M., Weng, S., and Zhang, C. On the sample complexity of random fourier features for online learning: How many random fourier features do we need? ACM Transactions on Knowledge Discovery from Data (TKDD), 8(3):1–19, 2014.
     - Rawat, A. S., Chen, J., Yu, F. X. X., Suresh, A. T., and Kumar, S. Sampled softmax with random fourier features. Advances in Neural Information Processing Systems, 32, 2019.
     - Herrmann, F. J. Randomized sampling and sparsity: Getting more information from fewer samples. Geophysics, 75 (6):WB173-WB187, 2010.
   - **Relevance:** This citation introduces the concept of SFT in deep learning, which is a key element of the proposed FourierFT method. It highlights the potential of SFT for representation learning and matrix recovery, providing a foundation for the paper's novel approach.


8. **Claim:** "One important application of this technique is matrix recovery. Patel et al. (2011) designs a gradient-based compressed sensing method to recover images with their sparse Fourier information."
   - **Citation:**
     - Patel, V. M., Maleh, R., Gilbert, A. C., and Chellappa, R. Gradient-based image recovery methods from incomplete fourier measurements. IEEE Transactions on Image Processing, 21(1):94–105, 2011.
   - **Relevance:** This citation provides a specific example of SFT's application in matrix recovery, further supporting the paper's argument that SFT can be effectively used for weight matrix recovery in fine-tuning.


9. **Claim:** "Previous works (Chen & Chi, 2013; Yang & Xie, 2016; Gao et al., 2022) show that even when the original data is not frequency-sparse, SFT can effectively recover the data with extremely few parameters."
   - **Citation:**
     - Chen, Y. and Chi, Y. Spectral compressed sensing via structured matrix completion. In International conference on machine learning, pp. 414–422. PMLR, 2013.
     - Yang, Z. and Xie, L. Exact joint sparse frequency recovery via optimization methods. IEEE Transactions on Signal Processing, 64(19):5145–5157, 2016.
     - Gao, Z., Niu, Y., Cheng, J., Tang, J., Xu, T., Zhao, P., Li, L., Tsung, F., and Li, J. Handling missing data via max-entropy regularized graph autoencoder. arXiv preprint arXiv:2211.16771, 2022.
   - **Relevance:** This citation highlights a crucial aspect of SFT, its ability to recover data even when it's not inherently frequency-sparse, which is important for the paper's application of SFT to weight matrices in LFMs.


### 2.3 Method

**Summary:** This section details the proposed FourierFT method, explaining how it leverages the Discrete Fourier Transform to update weight changes in a parameter-efficient manner. It contrasts FourierFT with LoRA, emphasizing the use of sparse spectral coefficients instead of low-rank matrices.

**Significant Citations:**

1. **Claim:** "FourierFT follows the principle of only learning the change in the pre-trained weight, as proposed by LoRA (Hu et al., 2021)."
   - **Citation:**
     - Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
   - **Relevance:** This citation connects FourierFT to the core principle of LoRA, which is to update only the weight changes rather than the entire weight matrix, highlighting the shared foundation between the two methods.


2. **Claim:** "Formally, we define each pre-trained weight matrix as Wo ∈ Rd1×d2, and the weight change for fine-tuning as ∆W ∈ Rd1×d2. LoRA aims to parameterize ∆W in the form of low-rank decomposition in the forward pass: h = Wox + AWx = Wox + BAx."
   - **Citation:**
     - Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
   - **Relevance:** This citation provides the mathematical formulation of LoRA's weight update process, which is essential for understanding the contrast with FourierFT's approach.


3. **Claim:** "The advantage of FourierFT is that the orthogonal and expressive Fourier basis enables recovery of informative weight changes."
   - **Citation:** (No specific citation is provided for this general claim, but the following citations are used to support the concept of Fourier basis in data compression)
   - **Relevance:** This claim highlights the core idea behind FourierFT, which is to leverage the powerful expressiveness of the Fourier basis for representing weight changes, providing a theoretical justification for the method's effectiveness.


### 2.4 Experiments

**Summary:** This section describes the experimental setup and results of FourierFT on various NLP and CV tasks. It compares FourierFT's performance with LoRA and other baselines, demonstrating its effectiveness in reducing the number of trainable parameters while maintaining comparable or better performance.

**Significant Citations:**

1. **Claim:** "We compare our FourierFT method with popular parameter-efficient fine-tuning (PEFT) methods."
   - **Citation:** (No specific citation is provided for this general claim, but the following citations are used to exemplify the categories)
   - **Relevance:** This statement sets the stage for the experimental comparison, which is crucial for evaluating the paper's contribution.


2. **Claim:** "Bitfit (Zaken et al., 2021) - Only the bias vectors are fine-tuned while all other parameters are frozen."
   - **Citation:**
     - Zaken, E. B., Ravfogel, S., and Goldberg, Y. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021.
   - **Relevance:** This citation introduces BitFit, a baseline method used for comparison, highlighting the diversity of PEFT approaches.


3. **Claim:** "Adapter tuning - This research line was first investigated by Houlsby et al. (2019), which proposes the Adapter method."
   - **Citation:**
     - Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790-2799. PMLR, 2019.
   - **Relevance:** This citation introduces Adapter Tuning, another baseline method, further illustrating the range of PEFT techniques used for comparison.


4. **Claim:** "LORA (Hu et al., 2021) - LoRA is the state-of-the-art method for PEFT. It parameterizes incremental weight updates using trainable low-rank matrices."
   - **Citation:**
     - Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
   - **Relevance:** This citation introduces LoRA, a key baseline method and the primary target for comparison, highlighting its importance in the field of PEFT.


5. **Claim:** "DyLoRA (Valipour et al., 2022) - This method trains dynamic search-free LORA models for the best rank choice."
   - **Citation:**
     - Valipour, M., Rezagholizadeh, M., Kobyzev, I., and Ghodsi, A. Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation. arXiv preprint arXiv:2210.07558, 2022.
   - **Relevance:** This citation introduces DyLoRA, a variant of LoRA, further demonstrating the diversity of LoRA-based methods used for comparison.


6. **Claim:** "AdaLoRA (Zhang et al., 2023) - This method proposes the SVD-based fine-tuning and prunes redundant singular values with the importance-aware rank allocation."
   - **Citation:**
     - Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023.
   - **Relevance:** This citation introduces AdaLoRA, another LoRA variant, further illustrating the breadth of LoRA-based methods used for comparison.


7. **Claim:** "We implement FourierFT for fine-tuning (1) RoBERTa (Base & Large) on natural language understanding (GLUE, (Wang et al., 2018)), (2) GPT-2 (Medium & Large) on natural language generation (E2E, (Novikova et al., 2017)) and (3) LLaMA-family models (7B & 13B) on instruction tuning."
   - **Citation:**
     - Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.
     - Novikova, J., Dušek, O., and Rieser, V. The e2e dataset: New challenges for end-to-end generation. arXiv preprint arXiv:1706.09254, 2017.
     - Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.
     - Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.
   - **Relevance:** This citation specifies the NLP datasets and models used in the experiments, providing context for the evaluation of FourierFT's performance.


8. **Claim:** "For CV, we apply FourierFT to fine-tune the (4) vision transformers (Base & Large) on image classification."
   - **Citation:**
     - Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
   - **Relevance:** This citation specifies the CV model and task used in the experiments, providing context for the evaluation of FourierFT's performance in the CV domain.


### 2.5 Discussion and Related Work

**Summary:** The discussion section summarizes the key findings and insights of the paper, highlighting the advantages of FourierFT in terms of parameter efficiency and performance. It also discusses the limitations and potential future directions for research.

**Significant Citations:**

1. **Claim:** "Empirically, we compare our method with state-of-the-art LORA variants and other parameter-efficient fine-tuning methods on various tasks including (1) natural language understanding (on the GLUE benchmark), (2) natural language generation (on the E2E benchmark), (3) instruction tuning (with LLaMA-family models), and (4) image classification (with vision transformers)."
   - **Citation:** (The citations for these tasks and benchmarks are provided in the "Experiments" section, as discussed above)
   - **Relevance:** This claim summarizes the experimental scope of the paper, highlighting the diverse range of tasks and models used to evaluate FourierFT.


2. **Claim:** "FourierFT can always achieve comparable or even better performance than LoRA, with about 6.0%, 9.4%, 0.2% and 9.2% of LoRA's trainable parameters for these 4 tasks, respectively."
   - **Citation:** (The results are presented in Tables 2, 3, 4, and 5, with specific comparisons to LoRA in each table)
   - **Relevance:** This claim summarizes the key finding of the paper, demonstrating the significant parameter reduction achieved by FourierFT while maintaining comparable or better performance.


3. **Claim:** "The advantage of parameter efficiency in FourierFT becomes more pronounced as the model's scale (depth and width) increases."
   - **Citation:** (No specific citation is provided for this general claim, but the results in Table 1 are used to support it)
   - **Relevance:** This claim highlights a key advantage of FourierFT, its scalability to larger models, which is important for future applications of the method.


### 2.6 Future Work and Open Questions

**Summary:** The authors suggest several directions for future research, including exploring the impact of different frequency biases, investigating the scalability of the method to even larger models, and exploring the potential for combining FourierFT with other PEFT techniques.

**Significant Citations:**

1. **Claim:** "We believe that making fe trainable will be a promising new direction for improving FourierFT."
   - **Citation:** (The results of the frequency bias experiments in Figure 5 are used to support this claim)
   - **Relevance:** This claim suggests a specific direction for future work, exploring the potential of making the central frequency a trainable parameter.


2. **Claim:** "The expressive power of the orthogonal basis is much stronger than that of the random basis."
   - **Citation:** (The results of the basis expressiveness experiments in Table 6 are used to support this claim)
   - **Relevance:** This claim suggests a potential direction for future work, exploring the use of different orthogonal basis functions in FourierFT.


## 3. Key Insights and Supporting Literature

- **Key Insight:** FourierFT achieves comparable or better performance than LoRA with significantly fewer trainable parameters across various NLP and CV tasks.
   - **Supporting Citations:**
     - Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. (LoRA's foundational work)
     - The results presented in Tables 2, 3, 4, and 5, which demonstrate FourierFT's performance compared to LoRA.
   - **Contribution:** This insight highlights the core contribution of the paper, demonstrating the effectiveness of FourierFT in reducing the parameter overhead of fine-tuning LFMs.


- **Key Insight:** FourierFT's parameter efficiency becomes more pronounced as the model's scale (depth and width) increases.
   - **Supporting Citations:**
     - The results presented in Table 1, which shows the theoretical number of parameters for LoRA and FourierFT for different model sizes.
   - **Contribution:** This insight suggests that FourierFT is particularly well-suited for fine-tuning very large LFMs, where parameter efficiency is crucial.


- **Key Insight:** The Fourier basis provides a powerful and expressive representation for capturing weight changes in LFMs.
   - **Supporting Citations:**
     - Duarte, M. F. and Baraniuk, R. G. Spectral compressive sensing. Applied and Computational Harmonic Analysis, 35(1):111-129, 2013. (Fourier basis in compressive sensing)
     - Chen, Y. and Chi, Y. Spectral compressed sensing via structured matrix completion. In International conference on machine learning, pp. 414–422. PMLR, 2013. (Fourier basis in matrix completion)
     - The discussion in Section 3.3, which explains the rationale behind using the Fourier basis for weight updates.
   - **Contribution:** This insight provides a theoretical foundation for the effectiveness of FourierFT, suggesting that the Fourier basis is well-suited for representing the complex relationships within LFM weight matrices.


## 4. Experimental Methodology and Its Foundations

- **Experimental Setup:** The paper evaluates FourierFT on a variety of NLP and CV tasks, including natural language understanding (GLUE benchmark), natural language generation (E2E benchmark), instruction tuning (LLaMA and LLaMA2 models), and image classification (ViT models). For each task, the authors compare FourierFT's performance with LoRA and other baselines, such as full fine-tuning, BitFit, and Adapter Tuning. They use standard metrics for each task (e.g., accuracy, F1 score, BLEU, FID) to evaluate the performance of different methods.


- **Foundations in Cited Works:**
   - The authors use LoRA (Hu et al., 2021) as a primary baseline and inspiration for their work. They adopt the principle of only updating weight changes, as proposed by LoRA.
   - The authors draw upon the concept of sparse Fourier transform (SFT) from deep learning literature (Xu et al., 2020; Ehrlich & Davis, 2019; Gueguen et al., 2018; Tang et al., 2022; Lin et al., 2014; Rawat et al., 2019; Herrmann, 2010) to develop their novel FourierFT method.
   - The authors cite works on matrix recovery using SFT (Patel et al., 2011; Shechtman et al., 2014) to support the idea that SFT can be used for recovering weight matrices.
   - The authors also cite works on the expressiveness of Fourier basis in data compression (Duarte & Baraniuk, 2013; Rudelson & Vershynin, 2008) to justify their choice of using the Fourier basis for representing weight changes.


- **Novel Aspects of Methodology:**
   - The core novelty of the paper lies in the introduction of FourierFT, which treats weight changes as spatial-domain matrices and learns only a small fraction of their spectral coefficients.
   - The authors justify this novel approach by citing works on the expressiveness of Fourier basis and its ability to recover data even when it