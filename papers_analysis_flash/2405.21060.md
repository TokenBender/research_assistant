## Analysis of "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality"

**1. Introduction:**

- **Title:** Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality
- **Authors:** Tri Dao and Albert Gu
- **Publication Date:** 31 May 2024
- **Objective:** The paper aims to demonstrate a deep connection between structured state-space models (SSMs) and variants of attention, leading to the development of more efficient and generalized sequence models.
- **Number of References:** 114

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:**
    - Transformers have been highly successful in language modeling, but their quadratic scaling with sequence length poses challenges.
    - SSMs, such as Mamba, have shown promise in matching or exceeding Transformer performance at smaller scales, but their development has been largely separate from the Transformer community.
    - The paper aims to bridge this gap by establishing theoretical connections between SSMs and attention, enabling the transfer of optimizations and insights from Transformers to SSMs.
- **Significant Citations:**
    - **Claim:** Transformers have been highly successful in language modeling.
        - **Citation:** Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. “Language Models are Few-shot Learners”. In: Advances in Neural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877-1901.
        - **Explanation:** This citation highlights the success of Transformers in language modeling, setting the context for the paper's focus on improving their efficiency.
    - **Claim:** SSMs, such as Mamba, have shown promise in matching or exceeding Transformer performance at smaller scales.
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.
    - **Claim:** The paper aims to bridge the gap between SSMs and attention by establishing theoretical connections.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation references the "Linear Attention" framework, which established a connection between attention and RNNs, providing a foundation for the paper's approach.

**2.2 Background and Overview:**

- **Key Points:**
    - The paper introduces the concept of "structured state space duality" (SSD) as a framework for connecting SSMs and attention.
    - SSD is based on the abstraction of "structured matrices," which have subquadratic parameter and multiplication complexity.
    - The paper outlines key technical contributions, including:
        - Equivalence between SSMs and semiseparable matrices
        - Improved theoretical understanding of linear attention
        - Connection between SSMs and structured masked attention (SMA)
    - The framework leads to new efficient algorithms for computing SSMs and opens up new directions for understanding and improving sequence models.
- **Significant Citations:**
    - **Claim:** The paper introduces the concept of "structured state space duality" (SSD).
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation highlights the "Linear Attention" framework, which established a connection between attention and RNNs, providing a foundation for the paper's approach.
    - **Claim:** SSD is based on the abstraction of "structured matrices," which have subquadratic parameter and multiplication complexity.
        - **Citation:** Dao, Tri, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Ré. "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations". In: The International Conference on Machine Learning (ICML). 2019.
        - **Explanation:** This citation introduces the concept of "structured matrices," which are essential to the SSD framework and its efficient algorithms.
    - **Claim:** The paper outlines key technical contributions, including:
        - Equivalence between SSMs and semiseparable matrices
        - Improved theoretical understanding of linear attention
        - Connection between SSMs and structured masked attention (SMA)
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.

**2.3 Structured State Space Models:**

- **Key Points:**
    - SSMs are a class of sequence models inspired by continuous-time systems.
    - They are broadly related to RNNs, CNNs, and classical state space models.
    - The paper focuses on "structured SSMs," which have specific structures on their matrices to enable efficient computation.
    - The paper introduces the concept of "selective SSMs," which can selectively focus on or ignore inputs at each timestep.
    - The paper highlights the importance of viewing SSMs as "sequence transformations," which map input sequences to output sequences.
- **Significant Citations:**
    - **Claim:** SSMs are a class of sequence models inspired by continuous-time systems.
        - **Citation:** Gu, Albert, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”. In: The International Conference on Learning Representations (ICLR). 2022.
        - **Explanation:** This citation introduces the original "structured SSMs" (S4) and their connection to continuous-time systems.
    - **Claim:** The paper focuses on "structured SSMs," which have specific structures on their matrices to enable efficient computation.
        - **Citation:** Gu, Albert, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher Ré. “Combining Recurrent, Convolutional, and Continuous-time Models with the Linear State Space Layer”. In: Advances in Neural Information Processing Systems (NeurIPS). 2021.
        - **Explanation:** This citation highlights the importance of "structured SSMs" for efficient computation, setting the stage for the paper's focus on specific structures.
    - **Claim:** The paper introduces the concept of "selective SSMs," which can selectively focus on or ignore inputs at each timestep.
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.
    - **Claim:** The paper highlights the importance of viewing SSMs as "sequence transformations," which map input sequences to output sequences.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation highlights the "Linear Attention" framework, which established a connection between attention and RNNs, providing a foundation for the paper's approach.

**2.4 Attention:**

- **Key Points:**
    - Attention is a mechanism for assigning scores to pairs of positions in a sequence, allowing elements to "attend" to others.
    - Softmax self-attention is the most common variant, but its quadratic scaling with sequence length is a major challenge.
    - The paper focuses on "linear attention," which aims to achieve linear complexity by folding the softmax into a kernel feature map.
- **Significant Citations:**
    - **Claim:** Attention is a mechanism for assigning scores to pairs of positions in a sequence, allowing elements to "attend" to others.
        - **Citation:** Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need”. In: Advances in Neural Information Processing Systems (NeurIPS). 2017.
        - **Explanation:** This citation provides a foundational definition of attention and its role in sequence modeling.
    - **Claim:** Softmax self-attention is the most common variant, but its quadratic scaling with sequence length is a major challenge.
        - **Citation:** Tay, Yi, Mostafa Dehghani, Dara Bahri, and Donald Metzler. “Efficient Transformers: A Survey”. In: ACM Computing Surveys 55.6 (2022), pp. 1–28.
        - **Explanation:** This citation highlights the computational challenges of softmax self-attention, motivating the paper's exploration of alternative approaches.
    - **Claim:** The paper focuses on "linear attention," which aims to achieve linear complexity by folding the softmax into a kernel feature map.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.

**2.5 Structured Matrices:**

- **Key Points:**
    - Structured matrices have subquadratic parameter and multiplication complexity, making them suitable for efficient computation.
    - Examples of structured matrices include sparse, low-rank, Toeplitz, Cauchy, Vandermonde, and butterfly matrices.
    - The paper introduces a new class of structured matrices that are closely related to SSMs.
- **Significant Citations:**
    - **Claim:** Structured matrices have subquadratic parameter and multiplication complexity, making them suitable for efficient computation.
        - **Citation:** Dao, Tri, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Ré. "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations". In: The International Conference on Machine Learning (ICML). 2019.
        - **Explanation:** This citation introduces the concept of "structured matrices," which are essential to the SSD framework and its efficient algorithms.
    - **Claim:** Examples of structured matrices include sparse, low-rank, Toeplitz, Cauchy, Vandermonde, and butterfly matrices.
        - **Citation:** Dao, Tri, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher Ré. “Monarch: Expressive structured matrices for efficient and accurate training”. In: International Conference on Machine Learning. PMLR. 2022, pp. 4690-4721.
        - **Explanation:** This citation provides a broader context for structured matrices, highlighting their importance in machine learning.
    - **Claim:** The paper introduces a new class of structured matrices that are closely related to SSMs.
        - **Citation:** Gu, Albert, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”. In: The International Conference on Learning Representations (ICLR). 2022.
        - **Explanation:** This citation introduces the original "structured SSMs" (S4) and their connection to continuous-time systems.

**2.6 Overview: Structured State Space Duality:**

- **Key Points:**
    - The paper presents a framework for connecting SSMs and attention through structured matrices.
    - The framework highlights the duality between the "recurrent" and "dual" forms of SSMs and attention.
    - The recurrent form is based on a linear recurrence, while the dual form is based on a quadratic computation.
    - The paper introduces a new algorithm for computing SSD, which leverages both the linear and quadratic forms.
- **Significant Citations:**
    - **Claim:** The paper presents a framework for connecting SSMs and attention through structured matrices.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation highlights the "Linear Attention" framework, which established a connection between attention and RNNs, providing a foundation for the paper's approach.
    - **Claim:** The framework highlights the duality between the "recurrent" and "dual" forms of SSMs and attention.
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.
    - **Claim:** The recurrent form is based on a linear recurrence, while the dual form is based on a quadratic computation.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.
    - **Claim:** The paper introduces a new algorithm for computing SSD, which leverages both the linear and quadratic forms.
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.

**2.7 Notation:**

- **Key Points:**
    - The paper establishes a clear and consistent notation for matrices, vectors, indexing, dimensions, and tensor contractions.
    - This notation is designed to be precise and mappable to code.
- **Significant Citations:**
    - **Claim:** The paper establishes a clear and consistent notation for matrices, vectors, indexing, dimensions, and tensor contractions.
        - **Citation:** None.
        - **Explanation:** This section primarily focuses on defining notation, not citing existing works.

**2.8 State Space Models are Structured Matrices:**

- **Key Points:**
    - The paper demonstrates that SSMs can be represented as semiseparable matrices.
    - Semiseparable matrices have a specific structure that allows for efficient computation.
    - The paper introduces the "sequentially semiseparable" (SSS) representation of semiseparable matrices.
    - The paper proves that any N-SSS matrix is N-semiseparable and vice versa.
    - The paper highlights the importance of 1-SS matrices, which are equivalent to scalar SSMs.
- **Significant Citations:**
    - **Claim:** The paper demonstrates that SSMs can be represented as semiseparable matrices.
        - **Citation:** Pernet, Clément, Hippolyte Signargout, and Gilles Villard. “Exact computations with quasiseparable matrices". In: arXiv preprint arXiv:2302.04515 (2023).
        - **Explanation:** This citation introduces the concept of "semiseparable matrices," which are crucial to the paper's argument.
    - **Claim:** Semiseparable matrices have a specific structure that allows for efficient computation.
        - **Citation:** Pernet, Clément, and Arne Storjohann. "Time and space efficient generators for quasiseparable matrices". In: Journal of Symbolic Computation 85 (2018), pp. 224-246.
        - **Explanation:** This citation highlights the computational advantages of semiseparable matrices, setting the stage for the paper's focus on efficient algorithms.
    - **Claim:** The paper introduces the "sequentially semiseparable" (SSS) representation of semiseparable matrices.
        - **Citation:** Pernet, Clément, Hippolyte Signargout, and Gilles Villard. “Exact computations with quasiseparable matrices". In: arXiv preprint arXiv:2302.04515 (2023).
        - **Explanation:** This citation introduces the concept of "semiseparable matrices," which are crucial to the paper's argument.
    - **Claim:** The paper proves that any N-SSS matrix is N-semiseparable and vice versa.
        - **Citation:** Pernet, Clément, Hippolyte Signargout, and Gilles Villard. “Exact computations with quasiseparable matrices". In: arXiv preprint arXiv:2302.04515 (2023).
        - **Explanation:** This citation introduces the concept of "semiseparable matrices," which are crucial to the paper's argument.
    - **Claim:** The paper highlights the importance of 1-SS matrices, which are equivalent to scalar SSMs.
        - **Citation:** Gu, Albert, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”. In: The International Conference on Learning Representations (ICLR). 2022.
        - **Explanation:** This citation introduces the original "structured SSMs" (S4) and their connection to continuous-time systems.

**2.9 Computing State Space Models Through Structured Matrix Algorithms:**

- **Key Points:**
    - The paper demonstrates that efficient algorithms for computing SSMs can be derived from structured matrix multiplication algorithms.
    - The paper highlights the duality between the "linear" and "quadratic" modes of SSM computation.
    - The linear mode is based on a recurrent form, while the quadratic mode is based on a naive matrix multiplication.
    - The paper introduces a new hardware-efficient algorithm for computing SSD, which leverages both the linear and quadratic modes.
- **Significant Citations:**
    - **Claim:** The paper demonstrates that efficient algorithms for computing SSMs can be derived from structured matrix multiplication algorithms.
        - **Citation:** Pernet, Clément, Hippolyte Signargout, and Gilles Villard. “Exact computations with quasiseparable matrices". In: arXiv preprint arXiv:2302.04515 (2023).
        - **Explanation:** This citation introduces the concept of "semiseparable matrices," which are crucial to the paper's argument.
    - **Claim:** The paper highlights the duality between the "linear" and "quadratic" modes of SSM computation.
        - **Citation:** Gu, Albert, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”. In: The International Conference on Learning Representations (ICLR). 2022.
        - **Explanation:** This citation introduces the original "structured SSMs" (S4) and their connection to continuous-time systems.
    - **Claim:** The linear mode is based on a recurrent form, while the quadratic mode is based on a naive matrix multiplication.
        - **Citation:** Gu, Albert, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”. In: The International Conference on Learning Representations (ICLR). 2022.
        - **Explanation:** This citation introduces the original "structured SSMs" (S4) and their connection to continuous-time systems.
    - **Claim:** The paper introduces a new hardware-efficient algorithm for computing SSD, which leverages both the linear and quadratic modes.
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.

**2.10 Notation:**

- **Key Points:**
    - The paper establishes a clear and consistent notation for matrices, vectors, indexing, dimensions, and tensor contractions.
    - This notation is designed to be precise and mappable to code.
- **Significant Citations:**
    - **Claim:** The paper establishes a clear and consistent notation for matrices, vectors, indexing, dimensions, and tensor contractions.
        - **Citation:** None.
        - **Explanation:** This section primarily focuses on defining notation, not citing existing works.

**3. State Space Duality:**

- **Key Points:**
    - The paper establishes a duality between structured state space models and structured masked attention (SMA).
    - The paper shows that a special case of SSMs (scalar-identity SSMs) is equivalent to a special case of SMA (1-SS SMA).
    - The paper demonstrates that the linear-time SSM algorithm and the quadratic-time kernel attention algorithm are dual forms of each other.
- **Significant Citations:**
    - **Claim:** The paper establishes a duality between structured state space models and structured masked attention (SMA).
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.
    - **Claim:** The paper shows that a special case of SSMs (scalar-identity SSMs) is equivalent to a special case of SMA (1-SS SMA).
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.
    - **Claim:** The paper demonstrates that the linear-time SSM algorithm and the quadratic-time kernel attention algorithm are dual forms of each other.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.

**3.1 Scalar-Identity Structured State Space Models:**

- **Key Points:**
    - The paper specializes state space models to scalar structure, where the naive quadratic computation can be seen as an instance of kernel attention.
    - The paper demonstrates that scalar-identity SSMs are equivalent to 1-SS structured masked attention.
- **Significant Citations:**
    - **Claim:** The paper specializes state space models to scalar structure, where the naive quadratic computation can be seen as an instance of kernel attention.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.
    - **Claim:** The paper demonstrates that scalar-identity SSMs are equivalent to 1-SS structured masked attention.
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.

**3.2 1-Semiseparable Structured Masked Attention:**

- **Key Points:**
    - The paper specializes structured masked attention to semiseparable SMA, which characterizes masked attention with efficient autoregression.
    - The paper demonstrates that 1-SS SMA is a special case of diagonal state space models.
    - The paper highlights the importance of 1-SS SMA for efficient autoregression.
- **Significant Citations:**
    - **Claim:** The paper specializes structured masked attention to semiseparable SMA, which characterizes masked attention with efficient autoregression.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.
    - **Claim:** The paper demonstrates that 1-SS SMA is a special case of diagonal state space models.
        - **Citation:** Gu, Albert, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”. In: The International Conference on Learning Representations (ICLR). 2022.
        - **Explanation:** This citation introduces the original "structured SSMs" (S4) and their connection to continuous-time systems.
    - **Claim:** The paper highlights the importance of 1-SS SMA for efficient autoregression.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.

**3.3 Structured State-Space Duality (SSD):**

- **Key Points:**
    - The paper summarizes the duality between SSMs and SMA, termed "structured state space duality" (SSD).
    - SSD highlights the close relationship between SSMs and SMA, demonstrating that they share a common underlying structure.
    - SSD provides a framework for understanding and developing new sequence models that leverage the strengths of both SSMs and attention.
- **Significant Citations:**
    - **Claim:** The paper summarizes the duality between SSMs and SMA, termed "structured state space duality" (SSD).
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.
    - **Claim:** SSD highlights the close relationship between SSMs and SMA, demonstrating that they share a common underlying structure.
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.
    - **Claim:** SSD provides a framework for understanding and developing new sequence models that leverage the strengths of both SSMs and attention.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.

**3.4 Computing State Space Models Through Structured Matrix Algorithms:**

- **Key Points:**
    - The paper demonstrates that efficient algorithms for computing SSMs can be derived from structured matrix multiplication algorithms.
    - The paper highlights the duality between the "linear" and "quadratic" modes of SSM computation.
    - The linear mode is based on a recurrent form, while the quadratic mode is based on a naive matrix multiplication.
    - The paper introduces a new hardware-efficient algorithm for computing SSD, which leverages both the linear and quadratic modes.
- **Significant Citations:**
    - **Claim:** The paper demonstrates that efficient algorithms for computing SSMs can be derived from structured matrix multiplication algorithms.
        - **Citation:** Pernet, Clément, Hippolyte Signargout, and Gilles Villard. “Exact computations with quasiseparable matrices". In: arXiv preprint arXiv:2302.04515 (2023).
        - **Explanation:** This citation introduces the concept of "semiseparable matrices," which are crucial to the paper's argument.
    - **Claim:** The paper highlights the duality between the "linear" and "quadratic" modes of SSM computation.
        - **Citation:** Gu, Albert, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”. In: The International Conference on Learning Representations (ICLR). 2022.
        - **Explanation:** This citation introduces the original "structured SSMs" (S4) and their connection to continuous-time systems.
    - **Claim:** The linear mode is based on a recurrent form, while the quadratic mode is based on a naive matrix multiplication.
        - **Citation:** Gu, Albert, Karan Goel, and Christopher Ré. “Efficiently Modeling Long Sequences with Structured State Spaces”. In: The International Conference on Learning Representations (ICLR). 2022.
        - **Explanation:** This citation introduces the original "structured SSMs" (S4) and their connection to continuous-time systems.
    - **Claim:** The paper introduces a new hardware-efficient algorithm for computing SSD, which leverages both the linear and quadratic modes.
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.

**4. Structured Masked Attention: Generalizing Linear Attention with Structured Matrices:**

- **Key Points:**
    - The paper revisits the linear attention framework from first principles.
    - The paper provides a simple tensor-contraction-based proof of linear attention.
    - The paper introduces a generalized abstraction of structured masked attention (SMA).
    - SMA allows for the use of any structured mask matrix, generalizing linear attention.
    - The paper highlights the duality between SSMs and SMA, demonstrating that they share a common underlying structure.
- **Significant Citations:**
    - **Claim:** The paper revisits the linear attention framework from first principles.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.
    - **Claim:** The paper provides a simple tensor-contraction-based proof of linear attention.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.
    - **Claim:** The paper introduces a generalized abstraction of structured masked attention (SMA).
        - **Citation:** None.
        - **Explanation:** This section primarily focuses on defining SMA, not citing existing works.
    - **Claim:** SMA allows for the use of any structured mask matrix, generalizing linear attention.
        - **Citation:** None.
        - **Explanation:** This section primarily focuses on defining SMA, not citing existing works.
    - **Claim:** The paper highlights the duality between SSMs and SMA, demonstrating that they share a common underlying structure.
        - **Citation:** Gu, Albert, and Tri Dao. “Mamba: Linear-Time Sequence Modeling with Selective State Spaces”. In: arXiv preprint arXiv:2312.00752 (2023).
        - **Explanation:** This citation introduces Mamba, a specific SSM that has achieved competitive results with Transformers, motivating the paper's exploration of SSMs.

**4.1 The Attention Framework:**

- **Key Points:**
    - The paper defines the general framework for attention, including its basic form and key components.
    - The paper highlights the importance of "self-attention," where the source and target sequences are the same.
    - The paper introduces the concept of "kernel attention," which is based on a kernel feature map.
- **Significant Citations:**
    - **Claim:** The paper defines the general framework for attention, including its basic form and key components.
        - **Citation:** Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need”. In: Advances in Neural Information Processing Systems (NeurIPS). 2017.
        - **Explanation:** This citation provides a foundational definition of attention and its role in sequence modeling.
    - **Claim:** The paper highlights the importance of "self-attention," where the source and target sequences are the same.
        - **Citation:** Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need”. In: Advances in Neural Information Processing Systems (NeurIPS). 2017.
        - **Explanation:** This citation provides a foundational definition of attention and its role in sequence modeling.
    - **Claim:** The paper introduces the concept of "kernel attention," which is based on a kernel feature map.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.

**4.2 Linear Attention:**

- **Key Points:**
    - The paper provides a simple and rigorous proof of linear attention through the lens of tensor contractions.
    - The paper demonstrates that linear attention can be computed efficiently using a recurrence.
- **Significant Citations:**
    - **Claim:** The paper provides a simple and rigorous proof of linear attention through the lens of tensor contractions.
        - **Citation:** Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention”. In: International Conference on Machine Learning. PMLR. 2020, pp. 5156-5165.
        - **Explanation:** This citation introduces "linear attention," a key concept in the paper's framework for connecting SSMs and attention.
    - **Claim:** The paper demonstrates that linear attention can be computed efficiently using a recurrence.
        -