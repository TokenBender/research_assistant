## Depth Anything V2: A Comprehensive Analysis

**1. Introduction**

- **Title:** Depth Anything V2
- **Authors:** Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao
- **Publication Date:** June 13, 2024
- **Objective:** To present Depth Anything V2, a monocular depth estimation model that aims to achieve robust and fine-grained depth predictions while maintaining efficiency and generalizability.
- **Total References:** 101

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - Monocular depth estimation (MDE) is gaining importance in various applications, including 3D reconstruction, navigation, and autonomous driving.
    - Existing MDE models can be categorized into discriminative and generative approaches.
    - Depth Anything V1 [89] is efficient and lightweight but struggles with transparent objects and reflections, while Marigold [31] excels in detail but lacks robustness.
    - Depth Anything V2 aims to combine the strengths of both approaches.
- **Significant Citations:**
    - **[47, 32, 93]:**  "Precise depth information is not only favorable in classical applications, such as 3D reconstruction [47, 32, 93], navigation [82], and autonomous driving [80], but is also preferable in modern scenarios..." - This citation highlights the importance of depth estimation in various downstream tasks.
    - **[56, 7, 6, 94, 26, 38, 31, 89, 88, 25, 20, 52, 28]:** "Therefore, there have been numerous MDE models [56, 7, 6, 94, 26, 38, 31, 89, 88, 25, 20, 52, 28] emerging recently, which are all capable of addressing open-world images." - This citation provides a broad overview of the existing literature on MDE models.
    - **[7, 6, 89, 28]:** "One group [7, 6, 89, 28] is based on discriminative models, e.g., BEiT [4] and DINOv2 [50], while the other [31, 20, 25] is based on generative models, e.g., Stable Diffusion (SD) [59]." - This citation categorizes existing MDE models into discriminative and generative approaches.
    - **[89]:** "Depth Anything [89] as a discriminative model and Marigold [31] as a generative model." - This citation introduces the two models that are compared in the paper.
    - **[31]:** "It can be easily observed that Marigold is superior in modeling the details, while Depth Anything produces more robust predictions for complex scenes." - This citation highlights the strengths and weaknesses of the two compared models.
    - **[82]:** "provide varied model scales and inference efficiency to support extensive applications [82]." - This citation emphasizes the importance of model efficiency and scalability.
    - **[89]:** "Since the nature of MDE is a discriminative task, we start from Depth Anything V1 [89], aiming to maintain its strengths and rectify its weaknesses." - This citation establishes the foundation of the paper's approach, building upon Depth Anything V1.

**2.2 Revisiting the Labeled Data Design of Depth Anything V1**

- **Key Points:**
    - The authors argue that relying heavily on real labeled images for training MDE models has drawbacks, including label noise and limited detail.
    - They discuss the limitations of real labeled data, such as inaccuracies stemming from depth sensor limitations, stereo matching algorithms, and SfM methods.
    - They highlight the advantages of synthetic images, including precise depth labels and the ability to generate diverse and detailed data.
- **Significant Citations:**
    - **[56, 7]:** "Building on the pioneering work of MiDaS [56, 7] in zero-shot MDE, recent studies tend to construct larger-scale training datasets in an effort to enhance estimation performance." - This citation introduces the work of MiDaS, which paved the way for zero-shot MDE.
    - **[89, 94, 26]:** "Notably, Depth Anything V1 [89], Metric3D V1 [94] and V2 [28], as well as ZeroDepth [26], have amassed 1.5M, 8M, 16M, and 15M labeled images from various sources for training, respectively." - This citation highlights the trend of using large-scale datasets for MDE training.
    - **[54]:** "For example, MiDaS and Depth Anything V1 obtain poor scores of 25.9% and 53.5% respectively in the Transparent Surface Challenge [54] (more details in Table 12: our V2 achieves a competitive score of 83.6% in a zero-shot manner)." - This citation demonstrates the limitations of existing models in handling transparent surfaces.
    - **[31, 20, 25]:** "Inspired by several recent SD-based studies [31, 20, 25], that exclusively utilize synthetic images with complete depth information for training, we extensively check the label quality of synthetic images and note their potential to mitigate the drawbacks discussed above." - This citation highlights the recent trend of using synthetic images for MDE training.
    - **[58, 63, 53]:** "In a word, the depth of synthetic images is truly "GT". In the right side of Figure 4c, we show the fine-grained prediction of a MDE model trained on synthetic images. Moreover, we can quickly enlarge synthetic training images by collecting from graphics engines [58, 63, 53], which would not cause any privacy or ethical concerns, as compared to real images." - This citation emphasizes the advantages of synthetic images in terms of data quality and ethical considerations.

**2.3 Challenges in Using Synthetic Data**

- **Key Points:**
    - The authors acknowledge two limitations of synthetic data: distribution shift between synthetic and real images and restricted scene coverage.
    - They discuss how these limitations can hinder the generalization of models trained solely on synthetic data.
- **Significant Citations:**
    - **[57, 9]:** "Such distribution shift makes models struggle to transfer from synthetic to real images, even if the two data sources share similar layouts [57, 9]." - This citation highlights the issue of distribution shift between synthetic and real images.
    - **[58, 9]:** "Consequently, despite the astonishing precision of Hypersim [58] or Virtual KITTI [9] (Figure 4b), we cannot expect models trained on them to generalize well in real-world scenes like â€œcrowded people". - This citation emphasizes the limited scene coverage of synthetic datasets.
    - **[83, 37]:** "In contrast, some real datasets constructed from web stereo images (e.g., HRWSI [83]) or monocular videos (e.g., MegaDepth [37]), can cover extensive real-world scenes." - This citation highlights the advantages of real datasets in terms of scene coverage.

**2.4 Key Role of Large-Scale Unlabeled Real Images**

- **Key Points:**
    - The authors propose a solution to address the limitations of synthetic data by incorporating unlabeled real images.
    - They argue that unlabeled real images can bridge the domain gap, enhance scene coverage, and facilitate knowledge transfer from a large teacher model to smaller student models.
- **Significant Citations:**
    - **[89]:** "Depth Anything V1 [89] has highlighted the importance of large-scale unlabeled real data." - This citation acknowledges the importance of unlabeled real data in previous work.
    - **[27]:** "But differently, our distillation is enforced at the label level via extra unlabeled real data, instead of at the feature or logit level with original labeled data." - This citation distinguishes the authors' approach from traditional knowledge distillation methods.
    - **[48]:** "Finally, as supported in Figure 16, unlabeled images boost the robustness of our smaller models tremendously." - This citation highlights the benefits of using unlabeled real images for improving model robustness.

**2.5 Depth Anything V2**

- **Key Points:**
    - The authors outline the overall framework of Depth Anything V2, which involves training a teacher model on synthetic images, generating pseudo labels on unlabeled real images, and training student models on the pseudo-labeled data.
    - They discuss the details of the training process, including the datasets used, loss functions, and model scales.
- **Significant Citations:**
    - **[89]:** "Same as V1 [89], for each pseudo-labeled sample, we ignore its top-n-largest-loss regions during training, where n is set as 10%." - This citation highlights the use of a similar approach to handling noisy pseudo labels as in Depth Anything V1.
    - **[56]:** "But differently, we find Lgm is super beneficial to the depth sharpness when using synthetic images (Section B.7)." - This citation highlights the importance of the gradient matching loss for improving depth sharpness.
    - **[33]:** "We use two loss terms for optimization on labeled images: a scale- and shift-invariant loss Lssi and a gradient matching loss Lgm. These two objective functions are not new, as they are proposed by MiDaS [56]." - This citation acknowledges the use of existing loss functions.

**2.6 A New Evaluation Benchmark: DA-2K**

- **Key Points:**
    - The authors discuss the limitations of existing MDE benchmarks, including noisy depth labels, limited diversity, and low resolution.
    - They introduce a new benchmark, DA-2K, which addresses these limitations by providing precise sparse depth annotations, covering diverse scenes, and using high-resolution images.
- **Significant Citations:**
    - **[70]:** "Here, we further argue that widely adopted test benchmarks are also noisy. Figure 8 illustrates incorrect annotations for mirrors and thin structures on NYU-D [70] despite using specialized depth sensors." - This citation highlights the issue of noisy depth labels in existing benchmarks.
    - **[70]:** "Most of them were originally proposed for a single scene. For example, NYU-D [70] focuses on a few indoor rooms, while KITTI [24] only contains several street scenes." - This citation highlights the limited diversity of existing benchmarks.
    - **[11]:** "Following DIW [11], we annotate sparse depth pairs for each image." - This citation acknowledges the inspiration for the authors' approach to annotating depth pairs.
    - **[33]:** "We use SAM [33] to automatically predict object masks." - This citation highlights the use of SAM for object segmentation in the annotation process.
    - **[41, 21, 3]:** "Lastly, we believe it is also a potential testbed for the 3D awareness of future multimodal LLMs [41, 21, 3]." - This citation suggests potential future applications of the DA-2K benchmark.

**2.7 Experiment**

- **Key Points:**
    - The authors describe the experimental setup used for training and evaluating Depth Anything V2, including the datasets, model architecture, training parameters, and evaluation metrics.
    - They present the results of zero-shot relative depth estimation on conventional benchmarks and on their proposed DA-2K benchmark.
    - They also discuss the performance of their model when fine-tuned for metric depth estimation and provide an ablation study on the importance of pseudo-labeled real images.
- **Significant Citations:**
    - **[89]:** "Follow Depth Anything V1 [89], we use DPT [55] as our depth decoder, built on DINOv2 encoders." - This citation highlights the use of a similar approach to model architecture as in Depth Anything V1.
    - **[55]:** "Follow Depth Anything V1 [89], we use DPT [55] as our depth decoder, built on DINOv2 encoders." - This citation introduces the DPT model used as the depth decoder.
    - **[89, 7]:** "For fairness, we compare with Depth Anything V1 [89] and MiDaS V3.1 [7] on five unseen test datasets." - This citation highlights the models used for comparison in zero-shot relative depth estimation.
    - **[54]:** "We find Lgm is super beneficial to the depth sharpness when using synthetic images (Section B.7)." - This citation highlights the importance of the gradient matching loss for improving depth sharpness.
    - **[6]:** "To validate the generalization ability of our model, we transfer its encoder to the downstream metric depth estimation task. First, same as V1 [89], we follow the ZoeDepth [6] pipeline, but replace its MiDaS [7] encoder with our pre-trained encoder." - This citation highlights the approach used for fine-tuning the model for metric depth estimation.
    - **[33]:** "This observation is indeed similar to SAM [33] that only releases its pseudo-labeled masks." - This citation highlights the similarity of the authors' approach to using pseudo-labeled data with SAM.

**2.8 Related Work**

- **Key Points:**
    - The authors discuss related work in monocular depth estimation, learning from unlabeled real images, and knowledge distillation.
    - They highlight the novelty of their approach, which combines the use of synthetic images, pseudo-labeled real images, and knowledge distillation to address the limitations of existing methods.
- **Significant Citations:**
    - **[18, 19, 5]:** "Early works [18, 19, 5] focus on the in-domain metric depth estimation, where training and test images must share the same domain [70, 24]." - This citation provides a brief overview of early work in monocular depth estimation.
    - **[31, 25, 20]:** "Some works address this task through better modeling manners, e.g., using Stable Diffusion [59] as a depth denoiser [31, 25, 20]." - This citation highlights the use of generative models for MDE.
    - **[56, 55, 7, 94]:** "For example, MiDaS [56, 55, 7] and Metric3D [94] collect 2M and 8M labeled images respectively." - This citation highlights the use of large-scale datasets for MDE training.
    - **[89]:** "Aware of the difficulty of scaling up labeled images, Depth Anything V1 [89] leverages 62M unlabeled images to enhance the model's robustness." - This citation highlights the use of unlabeled data in previous work.
    - **[36, 86, 71, 90]:** "Learning from unlabeled real images is widely studied in the field of semi-supervised learning [36, 86, 71, 90]." - This citation provides a broad overview of related work in semi-supervised learning.
    - **[34]:** "However, they focus on academic benchmarks [34] which only allow usage of small-scale labeled and unlabeled images." - This citation highlights the limitations of existing work in semi-supervised learning.
    - **[27]:** "This is similar to the core spirit of knowledge distillation (KD) [27]." - This citation acknowledges the connection to knowledge distillation.
    - **[2, 73, 98]:** "But we are also fundamentally different in that we perform distillation at the prediction level through extra unlabeled real images, while KD [2, 73, 98] typically studies better distillation strategies at the feature or logit level through labeled images." - This citation highlights the novelty of the authors' approach to knowledge distillation.
    - **[43, 69, 10]:** "Moreover, it is indeed non-trivial and risky to directly distill feature representations between two models with a tremendous scale gap [48]." - This citation highlights the challenges of traditional knowledge distillation methods.

**2.9 Conclusion**

- **Key Points:**
    - The authors summarize the key contributions of Depth Anything V2, including its ability to produce robust and fine-grained depth predictions, its support for various model scales, and its ease of fine-tuning for downstream tasks.
    - They emphasize the importance of their findings regarding the use of synthetic images and pseudo-labeled real images for MDE training.
    - They highlight the DA-2K benchmark as a valuable tool for evaluating MDE models.
- **Significant Citations:**
    - **[99, 39]:** "Considering the widespread application of MDE models in AIGC [99, 39], we provide additional non-real images, such as AI-generated images, cartoon images, etc.." - This citation highlights the importance of MDE in AIGC applications.

**3. Key Insights and Supporting Literature**

- **Insight 1:** Synthetic images are superior to real images for training MDE models due to their precise depth labels and ability to capture fine details.
    - **Supporting Citations:**
        - **[31, 20, 25]:** "Inspired by several recent SD-based studies [31, 20, 25], that exclusively utilize synthetic images with complete depth information for training, we extensively check the label quality of synthetic images and note their potential to mitigate the drawbacks discussed above." - This citation highlights the recent trend of using synthetic images for MDE training.
        - **[58, 63, 53]:** "In a word, the depth of synthetic images is truly "GT". In the right side of Figure 4c, we show the fine-grained prediction of a MDE model trained on synthetic images. Moreover, we can quickly enlarge synthetic training images by collecting from graphics engines [58, 63, 53], which would not cause any privacy or ethical concerns, as compared to real images." - This citation emphasizes the advantages of synthetic images in terms of data quality and ethical considerations.
- **Insight 2:** Unlabeled real images play a crucial role in bridging the domain gap between synthetic and real images, enhancing scene coverage, and facilitating knowledge transfer from a large teacher model to smaller student models.
    - **Supporting Citations:**
        - **[89]:** "Depth Anything V1 [89] has highlighted the importance of large-scale unlabeled real data." - This citation acknowledges the importance of unlabeled real data in previous work.
        - **[27]:** "But differently, our distillation is enforced at the label level via extra unlabeled real data, instead of at the feature or logit level with original labeled data." - This citation distinguishes the authors' approach from traditional knowledge distillation methods.
        - **[48]:** "Finally, as supported in Figure 16, unlabeled images boost the robustness of our smaller models tremendously." - This citation highlights the benefits of using unlabeled real images for improving model robustness.
- **Insight 3:** The DA-2K benchmark provides a more comprehensive and accurate evaluation of MDE models by addressing the limitations of existing benchmarks.
    - **Supporting Citations:**
        - **[70]:** "Here, we further argue that widely adopted test benchmarks are also noisy. Figure 8 illustrates incorrect annotations for mirrors and thin structures on NYU-D [70] despite using specialized depth sensors." - This citation highlights the issue of noisy depth labels in existing benchmarks.
        - **[70]:** "Most of them were originally proposed for a single scene. For example, NYU-D [70] focuses on a few indoor rooms, while KITTI [24] only contains several street scenes." - This citation highlights the limited diversity of existing benchmarks.
        - **[11]:** "Following DIW [11], we annotate sparse depth pairs for each image." - This citation acknowledges the inspiration for the authors' approach to annotating depth pairs.
        - **[33]:** "We use SAM [33] to automatically predict object masks." - This citation highlights the use of SAM for object segmentation in the annotation process.
        - **[41, 21, 3]:** "Lastly, we believe it is also a potential testbed for the 3D awareness of future multimodal LLMs [41, 21, 3]." - This citation suggests potential future applications of the DA-2K benchmark.

**4. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The authors use DPT [55] as the depth decoder and DINOv2 encoders for their model.
    - They train the model on a combination of synthetic and pseudo-labeled real images, using Adam optimizer and specific learning rates for the encoder and decoder.
    - They evaluate the model on conventional benchmarks (KITTI, NYU-D, Sintel, ETH3D, DIODE) and their proposed DA-2K benchmark.
- **Foundations:**
    - **[89]:** "Follow Depth Anything V1 [89], we use DPT [55] as our depth decoder, built on DINOv2 encoders." - This citation highlights the use of a similar approach to model architecture as in Depth Anything V1.
    - **[55]:** "Follow Depth Anything V1 [89], we use DPT [55] as our depth decoder, built on DINOv2 encoders." - This citation introduces the DPT model used as the depth decoder.
    - **[56]:** "But differently, we find Lgm is super beneficial to the depth sharpness when using synthetic images (Section B.7)." - This citation highlights the importance of the gradient matching loss for improving depth sharpness.
    - **[6]:** "To validate the generalization ability of our model, we transfer its encoder to the downstream metric depth estimation task. First, same as V1 [89], we follow the ZoeDepth [6] pipeline, but replace its MiDaS [7] encoder with our pre-trained encoder." - This citation highlights the approach used for fine-tuning the model for metric depth estimation.
- **Novel Aspects:**
    - The authors introduce the use of pseudo-labeled real images for training MDE models, which is a novel approach compared to previous methods that relied solely on synthetic or real labeled images.
    - They also introduce the DA-2K benchmark, which is a novel evaluation benchmark designed to address the limitations of existing benchmarks.
    - The authors cite no specific works to justify these novel approaches, but they build upon the existing literature on MDE, semi-supervised learning, and knowledge distillation.

**5. Results in Context**

- **Main Results:**
    - Depth Anything V2 outperforms Depth Anything V1 and other existing MDE models on the DA-2K benchmark, achieving higher accuracy and robustness.
    - The model demonstrates strong generalization ability when fine-tuned for metric depth estimation on various datasets.
    - The ablation study highlights the importance of pseudo-labeled real images for improving model performance.
- **Comparison with Existing Literature:**
    - **[89, 7]:** "For fairness, we compare with Depth Anything V1 [89] and MiDaS V3.1 [7] on five unseen test datasets." - This citation highlights the models used for comparison in zero-shot relative depth estimation.
    - **[31, 20, 25]:** "Our most capable model achieves 10.6% higher accuracy than Margold in terms of relative depth discrimination." - This citation highlights the improvement over Marigold in terms of relative depth discrimination.
    - **[6]:** "To validate the generalization ability of our model, we transfer its encoder to the downstream metric depth estimation task. First, same as V1 [89], we follow the ZoeDepth [6] pipeline, but replace its MiDaS [7] encoder with our pre-trained encoder." - This citation highlights the approach used for fine-tuning the model for metric depth estimation.
    - **[33]:** "This observation is indeed similar to SAM [33] that only releases its pseudo-labeled masks." - This citation highlights the similarity of the authors' approach to using pseudo-labeled data with SAM.
- **Confirmation, Contradiction, Extension:**
    - The authors' results confirm the importance of large-scale unlabeled data for improving MDE model performance, as previously highlighted in Depth Anything V1 [89].
    - Their results contradict the notion that real labeled images are always superior to synthetic images for MDE training, demonstrating the advantages of using synthetic images with pseudo-labeled real images.
    - The authors extend the existing literature by introducing a novel benchmark, DA-2K, which provides a more comprehensive and accurate evaluation of MDE models.

**6. Discussion and Related Work**

- **Situating the Work:**
    - The authors position their work within the context of existing research on monocular depth estimation, highlighting the limitations of previous approaches and the novelty of their proposed solution.
    - They discuss the importance of their findings regarding the use of synthetic images and pseudo-labeled real images for MDE training.
    - They emphasize the significance of their DA-2K benchmark for evaluating MDE models.
- **Key Papers Cited:**
    - **[89]:** "Depth Anything V1 [89] has highlighted the importance of large-scale unlabeled real data." - This citation acknowledges the importance of unlabeled real data in previous work.
    - **[31, 20, 25]:** "Inspired by several recent SD-based studies [31, 20, 25], that exclusively utilize synthetic images with complete depth information for training, we extensively check the label quality of synthetic images and note their potential to mitigate the drawbacks discussed above." - This citation highlights the recent trend of using synthetic images for MDE training.
    - **[56, 55, 7, 94]:** "For example, MiDaS [56, 55, 7] and Metric3D [94] collect 2M and 8M labeled images respectively." - This citation highlights the use of large-scale datasets for MDE training.
    - **[36, 86, 71, 90]:** "Learning from unlabeled real images is widely studied in the field of semi-supervised learning [36, 86, 71, 90]." - This citation provides a broad overview of related work in semi-supervised learning.
    - **[27]:** "But differently, our distillation is enforced at the label level via extra unlabeled real data, instead of at the feature or logit level with original labeled data." - This citation distinguishes the authors' approach from traditional knowledge distillation methods.
    - **[48]:** "Finally, as supported in Figure 16, unlabeled images boost the robustness of our smaller models tremendously." - This citation highlights the benefits of using unlabeled real images for improving model robustness.
- **Novelty and Importance:**
    - The authors highlight the novelty of their approach, which combines the use of synthetic images, pseudo-labeled real images, and knowledge distillation to address the limitations of existing methods.
    - They emphasize the significance of their DA-2K benchmark for evaluating MDE models.

**7. Future Work and Open Questions**

- **Future Work:**
    - The authors suggest exploring more efficient ways to leverage large-scale unlabeled data for training.
    - They also plan to collect synthetic images from a wider range of sources to improve the diversity of their training data.
- **Open Questions:**
    - The authors do not explicitly mention any open questions for future research. However, their work raises several questions, such as:
        - How can the distribution shift between synthetic and real images be further minimized?
        - Can the DA-2K benchmark be extended to include more diverse and challenging scenarios?
        - How can the computational burden of training on large-scale unlabeled data be reduced?

**8. Critical Analysis of Citation Usage**

- **Effectiveness:**
    - The authors effectively use citations to support their arguments and findings.
    - They provide a comprehensive overview of the existing literature on MDE, semi-supervised learning, and knowledge distillation.
    - They cite relevant works to support their claims and to contextualize their findings.
- **Areas for Improvement:**
    - While the authors provide a broad overview of the existing literature, they could have included more specific citations to support certain claims, such as the limitations of existing MDE benchmarks and the advantages of using synthetic images.
    - They could have also provided more citations to justify their novel approaches, such as the use of pseudo-labeled real images and the DA-2K benchmark.
- **Potential Biases:**
    - The authors primarily cite works from the field of computer vision, particularly those related to MDE and semi-supervised learning.
    - They could have included more citations from other related fields, such as natural language processing and robotics, to provide a broader perspective on the research.

**9. Final Summary**

- **Contribution:** Depth Anything V2 makes a significant contribution to the field of monocular depth estimation by introducing a novel approach that combines the use of synthetic images, pseudo-labeled real images, and knowledge distillation to address the limitations of existing methods. The authors also introduce a new benchmark, DA-2K, which provides a more comprehensive and accurate evaluation of MDE models.
- **Influential Works:**
    - **[89]:** Depth Anything V1, which serves as the foundation for the authors' work.
    - **[56, 7]:** MiDaS, which pioneered zero-shot MDE.
    - **[31, 20, 25]:** Recent works that have demonstrated the effectiveness of using synthetic images for MDE training.
    - **[27]:** Knowledge distillation, which provides a framework for transferring knowledge from a large teacher model to smaller student models.
- **Integration of Literature:**
    - The authors effectively integrate existing literature to support their claims and findings.
    - They provide a comprehensive overview of the existing literature on MDE, semi-supervised learning, and knowledge distillation.
    - They cite relevant works to support their claims and to contextualize their findings.

Overall, Depth Anything V2 is a significant contribution to the field of monocular depth estimation. The authors' novel approach and their introduction of the DA-2K benchmark provide valuable insights and tools for future research in this area. The paper effectively integrates existing literature to support its claims and findings, making it a valuable resource for researchers in the field.
