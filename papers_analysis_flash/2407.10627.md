## Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena

**1. Introduction**

- **Title:** Arena Learning: Build Data Flywheel for LLMs Post-training via Simulated Chatbot Arena
- **Authors:** Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Qingwei Lin, Jianguang Lou, Shifeng Chen, Yansong Tang, Weizhu Chen
- **Publication Date:** July 15, 2024
- **Objective:** To introduce Arena Learning, an offline strategy for simulating chatbot arena battles using AI-driven annotations, enabling continuous improvement of LLMs through supervised fine-tuning and reinforcement learning.
- **Total References:** 115

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - LLMs have shown remarkable progress in NLP, particularly in conversational AI.
    - The success of LLM-powered chatbots is attributed to high-quality instruction following data for effective post-training.
    - The LMSYS Chatbot Arena is a highly effective evaluation platform for chatbots, but it is limited by the costs and time required for human annotation.
    - Arena Learning aims to address these limitations by simulating arena battles using AI-driven annotations.
- **Significant Citations:**
    - **[1-3]:** "These models, trained on vast amounts of text data, have demonstrated an exceptional ability to understand, generate, and interact with human language in a wide range of tasks." This citation establishes the context of LLMs and their capabilities in NLP.
    - **[4-8]:** "One of the most exciting applications of LLMs has been in the realm of conversational AI [4–8], where they have been utilized to create powerful chatbots capable of engaging in naturalistic dialogues." This citation highlights the specific application of LLMs in conversational AI.
    - **[9-13]:** "One of the key factors contributing to the success of LLM-powered chatbots is the ability to leverage large-scale high-quality instruction following data for effective post-training [9-13]." This citation emphasizes the importance of post-training data for LLM performance.
    - **[14, 15]:** "The emergence of the LMSYS Chatbot Arena [14, 15] has been a significant development." This citation introduces the LMSYS Chatbot Arena as a key platform for evaluating chatbots.
    - **[16]:** "By leveraging a diverse set of human evaluators, the Chatbot Arena provides a more robust and comprehensive evaluation of chatbot performance, going beyond the limitations of traditional benchmarking approaches." This citation highlights the advantages of the LMSYS Chatbot Arena over traditional benchmarking methods.
    - **[17]:** "At the same time, it also opened up some real direct chat and battle preferences data [17], which have been proven to be valuable resources for model post-training and developmental guidance [18]." This citation emphasizes the value of the data generated by the LMSYS Chatbot Arena for post-training and development.
    - **[19]:** "Manually orchestrating and waiting the interactions between chatbots and human evaluators can be time-consuming and resource-intensive, limiting the scale and frequency of evaluation and training data opensource cycles." This citation highlights the limitations of the human-based evaluation process.
    - **[19]:** "Most models are unable to participate in arena evaluations, and the community can only obtain 10% of the chat data at most, making it hard to directly and efficiently guide the development of the target model based on this Arena." This citation further emphasizes the limitations of the LMSYS Chatbot Arena.

**2.2 Related Work**

- **Key Points:**
    - The paper discusses the advancements in LLMs, including open-source LLMs and their applications in conversational AI.
    - It highlights the challenges of evaluating LLM performance in real-world scenarios and the limitations of existing benchmarks.
    - The paper acknowledges the LMSYS Chatbot Arena as a valuable platform for evaluating chatbots but emphasizes its limitations in terms of cost and time.
- **Significant Citations:**
    - **[50-52]:** "LLMs have made significant strides in Natural Language Processing (NLP), serving as a versatile foundation for numerous applications [50-52]." This citation provides a general overview of LLMs and their applications.
    - **[53-73]:** This section lists various notable LLMs, including both closed-source and open-source models, highlighting the rapid advancements in the field.
    - **[19]:** "LMSYS has developed a chatbot arena [19] that utilizes anonymous battle and human judgment, but assessing all models is both time-consuming and costly." This citation highlights the limitations of the LMSYS Chatbot Arena.

**2.3 Approach**

- **Key Points:**
    - Arena Learning consists of three main components: Offline Pair-wise LLM Battle Arena, Iterative Post-training, and Model Evaluation.
    - The Chatbot Arena is a platform for evaluating chatbots by pitting them against each other in a series of conversational challenges.
    - Elo rankings are used to quantify the relative performance of chatbot models based on the outcomes of these battles.
    - Arena Learning uses a powerful LLM as a "judge model" to simulate human annotators in evaluating the quality, relevance, and appropriateness of model responses.
    - The paper describes the process of collecting large-scale instruction data and using it to train WizardLM-β through supervised fine-tuning (SFT), direct preference optimization (DPO), and proximal policy optimization (PPO).
    - Arena Learning employs an iterative process for training and improving WizardLM-β, where the model is continuously updated and re-evaluated against other models.
    - The paper introduces WizardArena, a carefully prepared offline test set designed to evaluate the performance of chatbot models and predict their Elo rankings.
- **Significant Citations:**
    - **[14, 15]:** "The Chatbot Arena is a pioneering platform that has revolutionized the way chatbot models are evaluated and compared." This citation highlights the importance of the Chatbot Arena.
    - **[16]:** "At the core of this Arena lies the concept of Elo rankings, a widely adopted rating system originally devised for chess players." This citation explains the concept of Elo rankings.
    - **[20]:** "These synthetic battle results are then used to enhance WizardLM-ẞ through some training strategies, including supervised fine-tuning (SFT), direct preference optimization (DPO) [20], and proximal policy optimization (PPO) [21], enabling it to learn from the strengths of other good models." This citation introduces DPO as a training strategy.
    - **[21]:** "This updated model is then re-introduced into the arena, where it battles against the other SOTA models once again." This citation highlights the iterative nature of Arena Learning.
    - **[11, 14, 24, 25]:** "To accurately evaluate the performance of chatbot models and predict their Elo rankings, Arena Learning relies on a carefully curated offline test set, which is designed to strike a balance between diversity and complexity [14, 24, 25], ensuring a comprehensive assessment of the models' capabilities across a wide range of conversational scenarios." This citation explains the design principles of WizardArena.
    - **[22]:** "The inputs are dialogue history, user instruction, and the responses of two LLMs. The outputs consist of scores for each LLM, along with explanations focused on various factors, such as coherence, factual accuracy, context-awareness, and overall quality, to determine whether one response is superior to the other." This citation describes the functionality of the "judge model".

**2.4 Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The authors trained an initial model WizardLM-β-I0 on 10k ShareGPT data.
    - They collected instructions from various datasets and optimized them using several steps, including filtering, cleaning, deduplication, and removing duplicates.
    - They constructed two offline test sets: Offline-Diverse WizardArena and Offline-Hard WizardArena.
    - They conducted pairwise battles between WizardLM-β and other models using Llama3-70B-Instruct as the "judge model".
    - They used the Bradley-Terry model to calculate the final ELO scores for each model.
    - They employed multiple bootstraps to ensure the reliability of the results.
- **Cited Works for Methodology:**
    - **[10, 11, 17, 27, 28]:** "We then collected some instructions from open available datasets [10, 11, 17, 27, 28], and optimized them using the following steps: first, we filtered out all illegal and toxic conversations; second, we removed conversations with instruction lengths of less than 10; third, we eliminated duplicate instructions with prefixes of 10; next, we employed the MinHashLSH technique [29] for data deduplication; subsequently, we used an embedding model gte-large [26] to exclude instructions from the top 5 matches in semantic similarity with benchmarks (i.e., WizardArena, Arena-Hard Auto [24], MT-Bench [14], AlpacaEval [25], OpenLLM Leaderboard [30–34]) to prevent test data leakage. Finally, we removed all non-English instructions." This citation lists the datasets used for collecting instructions and the methods used for data optimization.
    - **[24, 25]:** "Different from Arena-Hard-v1.0 [24], which mainly focuses on single-turn dialogue data, WizardArena-Mix incorporates multi-turn dialogue data." This citation highlights the difference between WizardArena-Mix and Arena-Hard-v1.0.
    - **[26]:** "We used an embedding model gte-large [26] to exclude instructions from the top 5 matches in semantic similarity with benchmarks." This citation explains the use of gte-large for data deduplication.
    - **[29]:** "We employed the MinHashLSH technique [29] for data deduplication." This citation explains the use of MinHashLSH for data deduplication.
    - **[35]:** "Following LMSYS Chatbot Arena, we adopt the Bradley-Terry model [35] to calculate the final ELO scores for each model." This citation explains the use of the Bradley-Terry model for calculating ELO scores.
    - **[23]:** "To mitigate potential position bias, we used a two-game setup, swapping the models between the first and second positions for each instance [23]." This citation explains the use of a two-game setup to mitigate position bias.
- **Novel Aspects of Methodology:**
    - The use of a powerful LLM as a "judge model" to simulate human annotators in evaluating model responses is a novel aspect of the methodology.
    - The authors do not cite any specific works to justify this novel approach, but they do mention that the judge model is specifically prompted and adjusted on a diverse range of conversational pair data.

**2.5 Results in Context**

- **Main Results:**
    - WizardArena achieves an average consistency of 98.79% with the LMSYS Chatbot Arena, outperforming Arena-Hard-v1.0 by 8.58% and MT-Bench by 35.23%.
    - Models trained on the extensive battle data generated by Arena Learning exhibit significant performance improvements during the SFT, DPO, and PPO stages.
    - Arena Learning can scale up to more training data, as evidenced by the continuous improvement in model performance across three iterative loops.
    - The authors conducted an ablation study to explore the impact of data size, threshold, and the number of battle models on model performance.
    - They found that Arena Learning can effectively build a data flywheel and enhance model performance through post-training.
    - They also explored the impact of using different battle modes and found that using multiple models pairwise battle with each other to build the simulated offline Chatbot Arena achieved the best performance.
    - They compared the performance of WizardLM-β across various benchmarks, including LMSYS Arena-Hard Auto, AlpacaEval 2.0 LC, and the OpenLLM Leaderboard.
    - They found that WizardLM-β significantly outperforms other models across these benchmarks.
- **Citations for Comparison with Existing Literature:**
    - **[14, 24, 25]:** "The experimental results demonstrate that the Elo rankings produced by WizardArena achieve an average consistency of 98.79% with the LMSYS Chatbot Arena, outperforming Arena-Hard-v1.0 by 8.58% and MT-Bench by 35.23%." This citation compares the performance of WizardArena with other benchmarks.
    - **[24]:** "This finding not only validates the effectiveness of WizardArena as a reliable and cost-effective alternative to human-based evaluation platforms, but also further proves the reliability of using the “judge” model to generate a large amount of battle training data in simulated arena." This citation highlights the significance of the results in validating the effectiveness of WizardArena.
    - **[14, 23, 24]:** "To mitigate potential position bias [14, 23, 24], we employ a two-game setup, alternating the positions of the two LLMs." This citation explains the use of a two-game setup to mitigate position bias.
    - **[24]:** "Different from Arena-Hard-v1.0 [24], which mainly focuses on single-turn dialogue data, WizardArena-Mix incorporates multi-turn dialogue data." This citation highlights the difference between WizardArena-Mix and Arena-Hard-v1.0.
    - **[30-34]:** "We used an embedding model gte-large [26] to exclude instructions from the top 5 matches in semantic similarity with benchmarks (i.e., WizardArena, Arena-Hard Auto [24], MT-Bench [14], AlpacaEval [25], OpenLLM Leaderboard [30–34]) to prevent test data leakage." This citation lists the benchmarks used for data deduplication.
    - **[24, 25]:** "The experimental results demonstrate the effectiveness of Arena Learning in producing large-scale synthetic data flywheel to continuously improve WizardLM-8, through various training strategies including SFT, DPO, and PPO." This citation highlights the effectiveness of Arena Learning in post-training.
    - **[24]:** "In contrast to MT-Bench and Arena-Hard-v1.0 which use proprietary models (i.e. GPT-4) as the judge model, our approach employs current SOTA open-source model Llama-3-70B-Chat, which not only has a significantly lower cost but also achieves strong consistency." This citation highlights the advantages of using Llama3-70B-Chat as the "judge model".
    - **[30]:** "The results of baselines are cited from Arena-Hard Auto [24], AlpacaEval 2.0 LC [25], and OpenLLM Leaderboard [30]." This citation lists the benchmarks used for comparing the performance of WizardLM-β with other models.

**2.6 Discussion and Related Work**

- **Key Points:**
    - The authors discuss the limitations of existing benchmarks and highlight the need for a more robust and reliable evaluation platform.
    - They emphasize the importance of Arena Learning in addressing these limitations by providing a cost-effective and scalable alternative to human-based evaluation systems.
    - They acknowledge the potential limitations of Arena Learning, such as the reliance on the "judge model" and the possibility of generating unethical or misleading information.
- **Significant Citations:**
    - **[14, 24, 25]:** "The experimental results demonstrate that the Elo rankings produced by WizardArena achieve an average consistency of 98.79% with the LMSYS Chatbot Arena, outperforming Arena-Hard-v1.0 by 8.58% and MT-Bench by 35.23%." This citation highlights the significance of the results in validating the effectiveness of WizardArena.
    - **[19]:** "This finding not only validates the effectiveness of WizardArena as a reliable and cost-effective alternative to human-based evaluation platforms, but also further proves the reliability of using the “judge” model to generate a large amount of battle training data in simulated arena." This citation highlights the significance of the results in validating the effectiveness of WizardArena.
    - **[14, 23, 24]:** "To mitigate potential position bias [14, 23, 24], we employ a two-game setup, alternating the positions of the two LLMs." This citation explains the use of a two-game setup to mitigate position bias.
    - **[24]:** "Different from Arena-Hard-v1.0 [24], which mainly focuses on single-turn dialogue data, WizardArena-Mix incorporates multi-turn dialogue data." This citation highlights the difference between WizardArena-Mix and Arena-Hard-v1.0.
    - **[30-34]:** "We used an embedding model gte-large [26] to exclude instructions from the top 5 matches in semantic similarity with benchmarks (i.e., WizardArena, Arena-Hard Auto [24], MT-Bench [14], AlpacaEval [25], OpenLLM Leaderboard [30–34]) to prevent test data leakage." This citation lists the benchmarks used for data deduplication.
    - **[24, 25]:** "The experimental results demonstrate the effectiveness of Arena Learning in producing large-scale synthetic data flywheel to continuously improve WizardLM-8, through various training strategies including SFT, DPO, and PPO." This citation highlights the effectiveness of Arena Learning in post-training.
    - **[24]:** "In contrast to MT-Bench and Arena-Hard-v1.0 which use proprietary models (i.e. GPT-4) as the judge model, our approach employs current SOTA open-source model Llama-3-70B-Chat, which not only has a significantly lower cost but also achieves strong consistency." This citation highlights the advantages of using Llama3-70B-Chat as the "judge model".
    - **[30]:** "The results of baselines are cited from Arena-Hard Auto [24], AlpacaEval 2.0 LC [25], and OpenLLM Leaderboard [30]." This citation lists the benchmarks used for comparing the performance of WizardLM-β with other models.

**2.7 Future Work and Open Questions**

- **Future Work:**
    - The authors suggest exploring the use of different judge models and investigating the impact of the judge model's performance on the overall results.
    - They also suggest exploring the use of different battle modes and investigating the impact of the battle mode on model performance.
    - They propose further research on the scalability of Arena Learning and its application to other tasks and domains.
- **Citations for Future Work:**
    - **[113, 114]:** "Therefore, we have developed the simulated offline WizardArena, which not only effectively differentiates model performance but also aligns closely with the online human-based LMSYS ChatBot Arena [19], which achieves an average consistency of 98% with LMSYS ChatBot Arena, simultaneously making it suitable for selecting the optimal models and predicting the performance of models while significantly enhancing model post-training through battle data." This citation highlights the potential of Arena Learning for future research.

**2.8 Critical Analysis of Citation Usage**

- **Effectiveness of Citation Usage:**
    - The authors effectively use citations to support their arguments and findings.
    - They provide a comprehensive overview of the relevant literature and clearly demonstrate how their work builds upon and extends existing research.
- **Areas for Additional Citations:**
    - The authors could have provided more citations to support their claims about the limitations of existing benchmarks and the need for a more robust and reliable evaluation platform.
    - They could have also provided more citations to support their claims about the scalability of Arena Learning and its potential for future research.
- **Potential Biases in Citation Selection:**
    - The authors primarily cite works from the field of deep learning and large language models.
    - They could have included more citations from other related fields, such as natural language processing, artificial intelligence, and human-computer interaction.

**2.9 Final Summary**

- **Contribution to the Field:**
    - Arena Learning is a novel and promising approach for simulating chatbot arena battles using AI-driven annotations.
    - It offers a cost-effective and scalable alternative to human-based evaluation systems, enabling continuous improvement of LLMs through supervised fine-tuning and reinforcement learning.
- **Influential or Frequently Cited Works:**
    - **[14, 19, 24, 25]:** These works are frequently cited throughout the paper, highlighting the importance of the LMSYS Chatbot Arena and other benchmarks for evaluating chatbot performance.
    - **[1-3, 9-13, 50-52, 74-76]:** These works provide a broad overview of the field of deep learning and large language models, establishing the context for the paper's research.
- **Integration of Existing Literature:**
    - The paper effectively integrates existing literature to support its claims and findings.
    - It provides a comprehensive overview of the relevant research and clearly demonstrates how its work builds upon and extends existing research.

**Overall, this paper makes a significant contribution to the field of deep learning and large language models by introducing Arena Learning, a novel and promising approach for simulating chatbot arena battles using AI-driven annotations. The paper provides a comprehensive overview of the relevant literature, clearly demonstrates the effectiveness of Arena Learning, and highlights its potential for future research.**