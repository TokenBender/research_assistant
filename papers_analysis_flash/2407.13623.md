## Analysis of "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies"

**1. Introduction:**

- **Title:** Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies
- **Authors:** Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, Ngai Wong
- **Publication Date:** July 26, 2024 (arXiv preprint)
- **Objective:** The paper investigates the impact of vocabulary size on LLM scaling laws, aiming to determine the optimal vocabulary size for LLMs of varying parameter counts and compute budgets.
- **References:** 83 references cited in the paper.

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:**
    - Existing scaling laws primarily focus on model parameters and training data size, neglecting vocabulary size.
    - The authors argue that vocabulary size significantly impacts LLM performance and scaling.
    - They highlight the variability in vocabulary sizes across current LLMs, raising the question of optimal vocabulary size.
- **Citations:**
    - **Claim:** "Extensive prior work on LLMs has focused on deriving so-called scaling laws: a set of empirical formulas to predict how model performance scales, mainly as computing floating-point operations (FLOPs), model parameters, and quantity of training data change."
    - **Citation:** [30, 26, 63, 2, 43, 57]
    - **Explanation:** This citation supports the authors' claim that previous research on scaling laws has primarily focused on FLOPs, model parameters, and training data size.
    - **Claim:** "These works show that power-law fits can effectively predict language modeling loss and by extension downstream performance."
    - **Citation:** [23, 54]
    - **Explanation:** This citation highlights the effectiveness of power-law fits in predicting language modeling loss and downstream performance, which is a key aspect of scaling laws.
    - **Claim:** "This negligence has resulted in substantial variability in the vocabulary size of current LLMs."
    - **Citation:** [30]
    - **Explanation:** This citation points to the fact that previous scaling laws often neglect vocabulary size, leading to inconsistencies in vocabulary size across different LLMs.
    - **Claim:** "For example, Llama2-7B employs a vocabulary size of 32K, while Gemma-7B adopts a much larger vocabulary size of 256K despite both having a similar number of total parameters."
    - **Citation:** [67, 64]
    - **Explanation:** This citation provides specific examples of LLMs with significantly different vocabulary sizes, highlighting the need for a more comprehensive understanding of vocabulary size's impact.

**2.2 Preliminary:**

- **Key Points:**
    - The authors introduce a modified scaling law that incorporates vocabulary size.
    - They define key attributes for scaling laws: non-vocabulary parameters (Nnv), vocabulary parameters (Nv), and training characters (H).
    - They propose a function f(V) to estimate the compression ratio of a tokenizer based on vocabulary size.
    - They introduce a vocabulary-insensitive loss function (Lu) to fairly compare models with different vocabulary sizes.
- **Citations:**
    - **Claim:** "Scaling laws commonly deal with the attributes, model parameters (N) and number of training tokens (D)."
    - **Citation:** [26, 43]
    - **Explanation:** This citation establishes the traditional attributes used in scaling laws, which the authors adapt to include vocabulary size.
    - **Claim:** "We use N₁ = Vd to represent both the vocabulary parameters in the output layer."
    - **Explanation:** This citation clarifies the authors' notation for vocabulary parameters, which is crucial for understanding their analysis.
    - **Claim:** "We measure data not in tokens (D) but in training characters (H)."
    - **Citation:** [30]
    - **Explanation:** This citation justifies the authors' choice to measure data in training characters instead of tokens, as it allows for a vocabulary-independent measure of data volume.
    - **Claim:** "By fitting several tokenizers with V ranging from 1K to 1024K, we obtain a = 0.0064, b = −0.1581 and c = 1.2047."
    - **Citation:** [58]
    - **Explanation:** This citation highlights the use of the BPE algorithm for tokenization, which is a common practice in NLP.
    - **Claim:** "We design the unigram-normalized language model loss as:"
    - **Citation:** [53]
    - **Explanation:** This citation introduces the unigram-normalized loss function (Lu), which is a key contribution of the paper.

**2.3 Analysis: Why the optimal vocabulary size is bounded by compute:**

- **Key Points:**
    - The authors analyze the relationship between FLOPs, vocabulary size, and normalized loss.
    - They argue that the optimal vocabulary size is constrained by the computational budget.
    - They present three perspectives on the optimal vocabulary size: fixed normalized loss, fixed FLOPs budget, and parameter growth.
- **Citations:**
    - **Claim:** "According to Kaplan et al. [30], the FLOPs (C) of a Transformer-based language model can be estimated as C ≈ 6ND, which can be re-written as:"
    - **Citation:** [30]
    - **Explanation:** This citation introduces the FLOPs formula used by Kaplan et al., which is a foundational aspect of scaling laws.
    - **Claim:** "Given a fixed FLOPs budget, we isolate the FLOPs and investigate how the vocabulary influences the loss."
    - **Citation:** [26]
    - **Explanation:** This citation highlights the importance of considering FLOPs budget when determining the optimal vocabulary size.
    - **Claim:** "Traditionally, scaling up model parameters in language models has been approached in two ways: increasing depth (i.e., the number of layers) or width (i.e., the hidden size)."
    - **Citation:** [63]
    - **Explanation:** This citation provides context for the authors' discussion of parameter growth, which is a key aspect of scaling up language models.

**2.4 Estimating the optimal vocabulary size:**

- **Key Points:**
    - The authors propose three approaches to estimate the optimal vocabulary size: IsoFLOPs analysis, derivative-based estimation, and parametric fit of the loss formula.
    - They validate their predictions empirically using models with 3B parameters.
- **Citations:**
    - **Claim:** "We pre-train models with non-vocabulary parameters ranging from 33M to 1.13B, with groups of models that share the same FLOPs (“IsoFLOPs") but varying vocabulary configurations."
    - **Citation:** [26]
    - **Explanation:** This citation highlights the use of IsoFLOPs analysis, which is a common technique for fitting scaling laws.
    - **Claim:** "We propose an alternative approach leveraging insights from the estimation of the FLOPs itself."
    - **Citation:** [26, 30]
    - **Explanation:** This citation introduces the derivative-based estimation approach, which is a novel contribution of the paper.
    - **Claim:** "Following a classical risk decomposition used in Hoffmann et al. [26], we design the vocabulary-dependent loss formula as:"
    - **Citation:** [26]
    - **Explanation:** This citation highlights the use of a parametric loss function, which is a common approach in scaling law research.

**2.5 Discussion:**

- **Key Points:**
    - The authors discuss the implications of their findings for predicting optimal vocabulary sizes for larger models.
    - They highlight the importance of considering data scarcity and overtraining scenarios.
    - They emphasize the need for further research on scaling laws in the context of vocabulary size.
- **Citations:**
    - **Claim:** "The community is starting to shift to larger vocabularies, such as with Llama3 [40] having a 128K vocabulary size up from 32K of Llama2 [67]."
    - **Citation:** [40, 67]
    - **Explanation:** This citation highlights the recent trend towards larger vocabulary sizes in LLMs.
    - **Claim:** "Our research underscores the overlooked importance of vocabulary and the need to jointly consider the vocabulary size, model parameters, and training data for effective scaling."
    - **Citation:** [26]
    - **Explanation:** This citation emphasizes the importance of considering vocabulary size alongside other scaling factors.

**2.6 Related Work:**

- **Key Points:**
    - The authors discuss the evolution of large language models and the importance of vocabulary size in their development.
    - They highlight the limitations of byte-level language models and the need for scaling vocabulary size.
    - They review existing research on scaling laws and the role of vocabulary in language models.
- **Citations:**
    - **Claim:** "The Transformer [68] has proven to be a very scalable architecture with consistent performance gains which has led to a series of large language models (LLMs)."
    - **Citation:** [68]
    - **Explanation:** This citation introduces the Transformer architecture, which is a foundational element of modern LLMs.
    - **Claim:** "Our scaling laws suggest that the limited vocabulary (i.e., 256 in byte-level language models) may constrain their performance, especially for larger models."
    - **Citation:** [77, 73]
    - **Explanation:** This citation highlights the limitations of byte-level language models, which have a fixed vocabulary size.
    - **Claim:** "Kaplan et al. [30] show that model performance improves as a power law with more compute allocated to both parameters or data."
    - **Citation:** [30]
    - **Explanation:** This citation summarizes the key findings of Kaplan et al.'s work on scaling laws.
    - **Claim:** "Takahashi and Tanaka-Ishii [62] find that larger vocabularies are better at capturing the true statistical distribution of language."
    - **Citation:** [62]
    - **Explanation:** This citation highlights the importance of vocabulary size for capturing the statistical properties of language.

**2.7 Conclusion:**

- **Key Points:**
    - The authors conclude that vocabulary size significantly impacts LLM scaling laws.
    - They propose three approaches for predicting optimal vocabulary size.
    - They emphasize the need for further research on scaling laws in the context of vocabulary size.
- **Citations:**
    - **Claim:** "We investigate the impact of the vocabulary size when scaling language models."
    - **Citation:** [26, 30]
    - **Explanation:** This citation reiterates the main focus of the paper.
    - **Claim:** "Our results show that models trained with an optimal vocabulary size as predicted by our approaches outperform models with a conventional vocabulary size under the same FLOPs budget."
    - **Citation:** [26]
    - **Explanation:** This citation summarizes the key findings of the paper.

**3. Key Insights and Supporting Literature:**

- **Insight:** Vocabulary size significantly impacts LLM scaling laws and performance.
    - **Citations:** [30, 26, 63, 2, 43, 57, 23, 54, 67, 64]
    - **Explanation:** These citations highlight the importance of vocabulary size in scaling laws and the variability in vocabulary sizes across current LLMs.
- **Insight:** The optimal vocabulary size is constrained by the computational budget.
    - **Citations:** [30, 26]
    - **Explanation:** These citations provide the theoretical foundation for the authors' analysis of the relationship between FLOPs, vocabulary size, and optimal performance.
- **Insight:** The authors propose three approaches for predicting the optimal vocabulary size: IsoFLOPs analysis, derivative-based estimation, and parametric fit of the loss formula.
    - **Citations:** [26, 30, 53]
    - **Explanation:** These citations highlight the novel contributions of the paper in terms of proposing new methods for determining optimal vocabulary size.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:** The authors train a series of models with varying non-vocabulary parameters (Nnv) and vocabulary sizes (V) under the same FLOPs budget. They evaluate the models on a held-out validation dataset using a vocabulary-insensitive loss function (Lu).
- **Foundations:**
    - **IsoFLOPs analysis:** [26]
    - **Derivative-based estimation:** [26, 30]
    - **Parametric fit of the loss formula:** [26]
- **Novel Aspects:**
    - The authors introduce a novel derivative-based estimation approach for predicting optimal vocabulary size.
    - They modify the parametric loss function to incorporate vocabulary size.
    - They use a vocabulary-insensitive loss function (Lu) to fairly compare models with different vocabulary sizes.
    - **Citations:** [26, 30, 53]

**5. Results in Context:**

- **Main Results:**
    - The authors find that the optimal vocabulary size scales slower than non-vocabulary parameters with respect to the computational budget.
    - They empirically verify their predictions using models with 3B parameters, showing that models trained with the predicted optimal vocabulary size consistently outperform models with commonly used vocabulary sizes.
- **Comparison with Existing Literature:**
    - The authors compare their findings with existing scaling laws, highlighting the importance of considering vocabulary size.
    - They note that most existing LLMs use suboptimal vocabulary sizes.
    - **Citations:** [30, 26, 63, 2, 43, 57, 23, 54, 67, 64]
- **Confirmation, Contradiction, or Extension:**
    - The authors' findings extend existing scaling laws by incorporating vocabulary size.
    - Their results highlight the need for a more comprehensive understanding of scaling laws in the context of vocabulary size.

**6. Discussion and Related Work:**

- **Situating the Work:** The authors situate their work within the broader context of LLM research, highlighting the importance of scaling laws and the need for a more comprehensive understanding of vocabulary size's impact.
- **Key Papers Cited:**
    - [30, 26, 63, 2, 43, 57, 23, 54, 67, 64, 68, 77, 73, 62, 71, 15, 80, 34, 35, 53, 18, 19, 21, 24, 25, 27, 28, 29, 31, 32, 33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 55, 56, 57, 58, 59, 60, 61, 65, 66, 69, 70, 72, 74, 75, 76, 78, 79, 80, 81, 82, 83]
- **Novelty and Importance:** The authors highlight the novelty of their work in incorporating vocabulary size into scaling laws and proposing new methods for predicting optimal vocabulary size. They argue that their findings have significant implications for the development of more efficient and effective LLMs.

**7. Future Work and Open Questions:**

- **Areas for Further Research:**
    - Extending the proposed approaches to multilingual and multimodal scenarios.
    - Investigating the impact of vocabulary size on larger models (e.g., 400-billion parameter LLMs).
    - Exploring the use of parametric loss functions that incorporate vocabulary size.
    - Conducting empirical studies on different datasets to understand how vocabulary size impacts loss under varied data conditions.
- **Citations:**
    - [40, 48, 49, 26, 43, 1, 65, 22]

**8. Critical Analysis of Citation Usage:**

- **Effectiveness:** The authors effectively use citations to support their arguments and findings. They provide a comprehensive overview of existing literature on scaling laws and the role of vocabulary in language models.
- **Areas for Improvement:**
    - The authors could have provided more citations to support their claims about the limitations of byte-level language models.
    - They could have included more citations to recent work on multilingual and multimodal LLMs.
- **Potential Biases:**
    - The authors primarily cite works from major research labs and conferences, which may reflect a bias towards mainstream research.
    - They could have included more citations to work from independent researchers and smaller labs.

**9. Final Summary:**

- **Contribution:** The paper makes a significant contribution to the field of LLM research by highlighting the importance of vocabulary size in scaling laws and proposing new methods for predicting optimal vocabulary size.
- **Influential Works:** [30, 26, 63, 2, 43, 57, 23, 54, 67, 64, 68]
- **Integration of Literature:** The paper effectively integrates existing literature to support its claims and findings. It provides a comprehensive overview of scaling laws and the role of vocabulary in language models, highlighting the limitations of previous work and the need for a more comprehensive understanding of vocabulary size's impact.

Overall, this paper is a valuable contribution to the field of LLM research. It provides a comprehensive analysis of the impact of vocabulary size on scaling laws and proposes new methods for predicting optimal vocabulary size. The authors' findings have significant implications for the development of more efficient and effective LLMs.