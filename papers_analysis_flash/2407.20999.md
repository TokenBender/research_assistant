Okay, here's a comprehensive analysis of the paper "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning" in Markdown format, following the structure you provided:


# MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning

## 1. Introduction

- **Title:** MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning
- **Authors:** Yupeng Chen, Senmiao Wang, Zhihang Lin, Zeyu Qin, Yushun Zhang, Tian Ding, and Ruoyu Sun
- **Publication Date:** July 31, 2024 (arXiv preprint)
- **Main Objective:** This research aims to propose a novel fine-tuning algorithm called Momentum-Filtered Optimizer (MoFO) to mitigate the issue of catastrophic forgetting, where LLMs lose knowledge acquired during pre-training when fine-tuned on new tasks.
- **Total Number of References:** 102


## 2. Section-by-Section Analysis with Citation Extraction

### 2.1 Introduction

- **Key Points:** Introduces the concept of LLMs, their pre-training and fine-tuning stages, and the problem of catastrophic forgetting during fine-tuning. Highlights the two common approaches to mitigate forgetting: replay-based and regularization-based methods.
- **Significant Citations:**

    a. "Typically, LLMs are initially pre-trained on extensive corpora to acquire general capabilities, and subsequently, they are fine-tuned on smaller, task-specific datasets to adapt to particular tasks or domains [Dai and Le, 2015, Kenton and Toutanova, 2019, Radford et al., 2018]."
    b. **Citation:** Dai, A. M., & Le, Q. V. (2015). Semi-supervised sequence learning. *Advances in neural information processing systems*, *28*.
    c. **Relevance:** This citation establishes the standard LLM training pipeline, which involves pre-training on large datasets and subsequent fine-tuning for specific tasks.

    a. "However, it has been observed that during the fine-tuning process, LLMs may forget the knowledge acquired in pre-training, leading to a decline in general capabilities [Lin et al., 2023, Chen et al., 2020, Dong et al., 2021, Korbak et al., 2022, Luo et al., 2023]."
    b. **Citation:** Lin, Y., Tan, H., Lin, Z., Zheng, R., Pi, J., Zhang, S., ... & Yao, Y. (2023). Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models. *arXiv preprint arXiv:2309.06256*.
    c. **Relevance:** This citation highlights the problem of catastrophic forgetting, which is the central focus of the paper. It lists several recent works that have investigated this issue in LLMs.

    a. "In the literature, two classes of methods are commonly adopted to mitigate the forgetting: replay-based methods, and regularization-based methods."
    b. **Citation:** Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T., & Wayne, G. (2019). Experience replay for continual learning. *Advances in neural information processing systems*, *32*.
    c. **Relevance:** This citation introduces the two main categories of methods used to address catastrophic forgetting, providing context for the authors' proposed approach.


### 2.2 Motivation

- **Key Points:** Discusses the observation that fine-tuning can lead to convergence to different minima with varying distances from the pre-trained model. Argues that minima closer to the pre-trained model are less prone to forgetting. Provides an illustrative example using Pythia-160m and different optimizers (Adam and Lion) to demonstrate this phenomenon.
- **Significant Citations:**

    a. "During fine-tuning, different training methods usually converge to different minima. We observe that these minima share similar fine-tuning loss but can vary significantly in their distances to the pre-trained model. Furthermore, minima closer are less likely to forget pre-training knowledge."
    b. **Citation:** Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., ... & Zou, A. (2020). The Pile: An 800GB dataset of diverse text for language modeling. *arXiv preprint arXiv:2101.00027*.
    c. **Relevance:** This claim is supported by the observation that different optimization methods lead to different minima in the loss landscape. The authors connect this to the concept of forgetting, suggesting that proximity to the pre-trained model is crucial.

    a. "We conduct an experiment using the Pythia-160m model to illustrate this observation. We fine-tune this model on a subset of the FLAN dataset using two different optimizers: the Adam optimizer and the Lion optimizer."
    b. **Citation:** Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*.
    c. **Relevance:** This citation introduces the Adam optimizer, a widely used optimization algorithm, which serves as a baseline for comparison in the authors' experiments.

    a. "Table 1 shows that Adam suffers from less accuracy degradation on average, indicating better preservation of the pre-training knowledge."
    b. **Citation:** Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., & Choi, Y. (2019). HellaSwag: Can a machine really finish your sentence? In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics* (pp. 4791–4800).
    c. **Relevance:** This citation introduces the HellaSwag dataset, which is used to evaluate the models' ability to retain common sense reasoning capabilities after fine-tuning. The results show that Adam, compared to Lion, retains more of this knowledge, supporting the authors' hypothesis.


### 2.3 Algorithm Formulation

- **Key Points:** Introduces the MoFO algorithm, which is a momentum-filtered optimizer that selectively updates parameters based on their momentum magnitudes. Explains how MoFO partitions the parameters into blocks and updates only a subset of parameters with the largest momentum in each block.
- **Significant Citations:**

    a. "We formally introduce the Momentum-Filtered Optimizer (MoFO) in Algorithm 1. MoFO partitions all the parameters into B fixed parts as shown in Line 4. At each iteration, MoFO selects the parameter entries with the largest a% momentum magnitudes in each part as shown in Lines 10-13 of Algorithm 1, where the update fraction a% is the predetermined hyperparameter."
    b. **Citation:** Tseng, P. (2001). Convergence of a block coordinate descent method for nondifferentiable minimization. *Journal of optimization theory and applications*, *109*(2), 475–494.
    c. **Relevance:** This citation introduces the concept of Block Coordinate Descent (BCD), which is a key inspiration for the MoFO algorithm. MoFO leverages the idea of updating only a subset of parameters at each iteration, similar to BCD.

    a. "MoFO efficiently selects and updates the most influential parameters, as dictated by the momentum's magnitude, thus enhancing the fine-tuning process while alleviating the catastrophic forgetting of pre-training knowledge."
    b. **Citation:** Zhang, Y., Chen, C., Shi, N., Sun, R., & Luo, Z.-Q. (2022). Adam can converge without any modification on update rules. *Advances in neural information processing systems*, *35*, 28386–28399.
    c. **Relevance:** This claim highlights the core idea of MoFO, which is to prioritize updating parameters that have the largest impact on the loss function, as indicated by their momentum. This is a novel approach to mitigating forgetting.


### 2.4 Convergence Result

- **Key Points:** Presents a simplified version of the MoFO algorithm (based on gradient descent) and provides a theoretical convergence analysis.
- **Significant Citations:**

    a. "In summary, we demonstrate the convergence of a GD version of MoFO, providing theoretical support for the strong performance of MoFO in fine-tuning tasks."
    b. **Citation:** Zhang, Y., Chen, C., Shi, N., Sun, R., & Luo, Z.-Q. (2022). Adam can converge without any modification on update rules. *Advances in neural information processing systems*, *35*, 28386–28399.
    c. **Relevance:** This citation acknowledges the challenge of proving convergence for the full MoFO algorithm due to its complex structure involving both first and second-order momentum. The authors provide a simplified version for analysis.


### 3. Experiments

- **Key Points:** Describes the experimental setup, including the base models (Llama-2-7B and TinyLlama-1.1B), datasets (MetaMathQA, Code-Alpaca, and TRACE), and evaluation metrics (MMLU, Commonsense, GSM8K, HumanEval, OP, and BWT).
- **Significant Citations:**

    a. "Now we verify the effectiveness of MoFO on instruction fine-tuning and continual fine-tuning. We use Llama-2-7B and TinyLlama-1.1B as the base models for our experiments."
    b. **Citation:** Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., ... & Bhargava, S. (2023). Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.
    c. **Relevance:** This citation introduces the Llama-2-7B model, which is one of the base models used in the experiments.

    a. "Datasets for instruction fine-tuning. This group of datasets covers question-answer pairs from different domains like mathematical reasoning and code generation."
    b. **Citation:** Yu, L., Jiang, W., Shi, H., Jincheng, Z., Liu, Y., Zhang, Y., ... & Liu, W. (2023). Metamath: Bootstrap your own mathematical questions for large language models. In *The Twelfth International Conference on Learning Representations*.
    c. **Relevance:** This citation introduces the MetaMathQA dataset, which is used for instruction fine-tuning on mathematical reasoning tasks.

    a. "Datasets for continual fine-tuning. We investigate the performance of MoFO in the continual fine-tuning scenario by implementing our approach on the TRACE benchmark dataset."
    b. **Citation:** Wang, X., Zhang, Y., Chen, T., Gao, S., Jin, S., Yang, X., ... & Gui, T. (2023). Trace: A comprehensive benchmark for continual learning in large language models. *arXiv preprint arXiv:2310.06762*.
    c. **Relevance:** This citation introduces the TRACE benchmark dataset, which is used to evaluate the models' performance in continual learning scenarios.


### 3.1 Experimental Settings

- **Key Points:** Provides details about the datasets used for instruction and continual fine-tuning, including the specific tasks and metrics used for evaluation.
- **Significant Citations:**

    a. "Metrics for instruction fine-tuning. We introduce a set of widely used benchmarks to assess the performance and catastrophic forgetting effects on the general capabilities of LLMs after instruction fine-tuning."
    b. **Citation:** Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, D., Song, L., & Steinhardt, J. (2021). Measuring massive multitask language understanding. In *International Conference on Learning Representations*.
    c. **Relevance:** This citation introduces the MMLU benchmark, which is used to evaluate the models' general knowledge and ability to perform across a wide range of tasks.

    a. "Metrics for continual fine-tuning. To evaluate the LLM's performance in continual learning, we consider two key metrics in this scenario: Overall Performance (OP) and BackWard Transfer (BWT)."
    b. **Citation:** Chaudhry, A., Dokania, P. K., Ajanthan, T., & Torr, P. H. (2018). Riemannian walk for incremental learning: Understanding forgetting and intransigence. In *European Conference on Computer Vision* (pp. 556–572).
    c. **Relevance:** This citation introduces the OP and BWT metrics, which are used to evaluate the models' ability to learn new tasks while retaining knowledge from previously learned tasks in a continual learning setting.


### 3.2 Instruction Fine-Tuning

- **Key Points:** Presents the results of instruction fine-tuning on MetaMathQA and Code-Alpaca datasets. Compares MoFO's performance with various baseline methods (Full FT, HFT, L1/L2 regularization).
- **Significant Citations:**

    a. "Results of fine-tuning on MetaMathQA. We fine-tune Llama-2-7B on MetaMathQA using various baseline methods and present the experimental results on mathematical reasoning (GSM8K) and general capabilities in Table 3."
    b. **Citation:** Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Ramalho, T. (2017). Overcoming catastrophic forgetting in neural networks. *Proceedings of the national academy of sciences*, *114*(13), 3521–3526.
    c. **Relevance:** This citation introduces L2 regularization, one of the baseline methods used for comparison. The authors compare MoFO's performance with L2 regularization to demonstrate its effectiveness in mitigating forgetting.

    a. "MoFO is compatible to the performance of Full FT and HFT on the math task, yet significantly outperforms these methods in preserving general capability."
    b. **Citation:** Hui, T., Zhang, Z., Wang, S., Xu, W., Sun, Y., & Wu, H. (2024). Hft: Half fine-tuning for large language models. *arXiv preprint arXiv:2404.18466*.
    c. **Relevance:** This citation introduces Half Fine-tuning (HFT), another baseline method used for comparison. The authors compare MoFO's performance with HFT to highlight its ability to maintain general capabilities while achieving comparable performance on the specific task.


### 3.3 Continual Fine-Tuning

- **Key Points:** Presents the results of continual fine-tuning on the TRACE benchmark dataset. Compares MoFO's performance with Full FT, HFT, and other continual learning methods (GEM, Replay).
- **Significant Citations:**

    a. "In this section, we explore the performance of our proposed MoFO in continual fine-tuning on the TRACE benchmark."
    b. **Citation:** Wang, X., Zhang, Y., Chen, T., Gao, S., Jin, S., Yang, X., ... & Gui, T. (2023). Trace: A comprehensive benchmark for continual learning in large language models. *arXiv preprint arXiv:2310.06762*.
    c. **Relevance:** This citation reiterates the use of the TRACE benchmark for evaluating continual learning performance.

    a. "MoFO outperforms Full FT and HFT by at least 1.4% on the OP score and by at least 4.7% on the BWT score."
    b. **Citation:** Lopez-Paz, D., & Ranzato, M. (2017). Gradient episodic memory for continual learning. *Advances in neural information processing systems*, *30*.
    c. **Relevance:** This citation introduces the concept of Gradient Episodic Memory (GEM), a method for continual learning that is used as a baseline for comparison. The authors compare MoFO's performance with GEM to demonstrate its effectiveness in continual learning.


### 3.4 Further Analysis

- **Key Points:** Investigates the impact of the parameter update fraction and different update strategies within MoFO.
- **Significant Citations:**

    a. "In this section, we first investigate the impact of the update fraction of parameters in the MoFO algorithm at each iteration, and then explore the effects of different update strategies within MoFO."
    b. **Citation:** Nesterov, Y. (2012). Efficiency of coordinate descent methods on huge-scale optimization problems. *SIAM Journal on Optimization*, *22*(2), 341–362.
    c. **Relevance:** This section explores the impact of hyperparameters on MoFO's performance, which is a common practice in evaluating optimization algorithms. The authors connect this to the concept of BCD, which is a key component of MoFO.


### 4. Why MoFO Converges to a Closer Point

- **Key Points:** Explores the reasons why MoFO converges to a minimum closer to the pre-trained model compared to Adam. Uses a toy example to illustrate this phenomenon.
- **Significant Citations:**

    a. "We attempt to answer this question by the following toy example. We denote Θ = (θ₁, θ₂) ∈ ℝ² to be the trainable parameters of our model and make the following assumptions:"
    b. **Citation:** Zenke, F., Poole, B., & Ganguli, S. (2017). Continual learning through synaptic intelligence. In *International conference on machine learning* (pp. 3987–3995).
    c. **Relevance:** This section uses a simplified toy example to illustrate the core idea of MoFO. The authors connect this to the concept of attractors in the loss landscape, which can influence the convergence of optimization algorithms.


### 5. Related Works

- **Key Points:** Provides a comprehensive overview of existing research on catastrophic forgetting, including replay-based, regularization-based, and architecture-based methods. Discusses the relevance of MoFO within this broader context.
- **Significant Citations:**

    a. "Catastrophic forgetting, a significant issue where models forget previously learned information upon learning new data, has received considerable attention in machine learning."
    b. **Citation:** McCloskey, M., & Cohen, N. J. (1989). Catastrophic interference in connectionist networks: The sequential learning problem. In *Psychology of learning and motivation*, *24*, 109–165.
    c. **Relevance:** This citation introduces the concept of catastrophic forgetting, which is the central problem addressed by the paper.

    a. "Researchers have proposed numerous methods to alleviate forgetting in continual learning, which involves learning a sequence of tasks."
    b. **Citation:** Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., & Tuytelaars, T. (2018). Memory aware synapses: Learning what (not) to forget. In *Proceedings of the European conference on computer vision* (ECCV) (pp. 139–154).
    c. **Relevance:** This citation introduces the concept of continual learning, which is a related area of research that often deals with catastrophic forgetting.

    a. "LoRA modifies the model architecture by freezing the pre-training weights and introducing low-rank trainable matrices."
    b. **Citation:** Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., ... & Chen, W. (2022). LoRA: Low-rank adaptation of large language models. In *International Conference on Learning Representations*.
    c. **Relevance:** This citation introduces LoRA, a popular parameter-efficient fine-tuning (PEFT) method that has been shown to be effective in mitigating forgetting. The authors compare MoFO with LoRA to highlight its advantages.


### 6. Conclusion

- **Key Points:** Summarizes the main contributions of the paper, including the introduction of MoFO, its effectiveness in mitigating forgetting, and its potential applications in multimodal LLMs.
- **Significant Citations:** None in this section, but the paper's findings are supported by the citations throughout the previous sections.


## 3. Key Insights and Supporting Literature

- **Insight 1:** Fine-tuning LLMs can lead to catastrophic forgetting, where the models lose knowledge acquired during pre-training.
    - **Supporting Citations:**
        - Lin et al. (2023) - Speciality vs generality: An empirical study on catastrophic forgetting in fine-tuning foundation models.
        - Chen et al. (2020) - Recall and learn: Fine-tuning deep pretrained language models with less forgetting.
        - Dong et al. (2021) - How should pre-trained language models be fine-tuned towards adversarial robustness?
        - Korbak et al. (2022) - Controlling conditional language models without catastrophic forgetting.
        - Luo et al. (2023) - An empirical study of catastrophic forgetting in large language models during continual fine-tuning.
    - **Contribution:** These works establish the problem of catastrophic forgetting in LLMs, providing the context for the need for mitigation techniques.

- **Insight 2:** Minima closer to the pre-trained model in the loss landscape are less prone to forgetting.
    - **Supporting Citations:**
        - Gao et al. (2020) - The Pile: An 800GB dataset of diverse text for language modeling.
        - Zellers et al. (2019) - HellaSwag: Can a machine really finish your sentence?
        - Kingma & Ba (2014) - Adam: A method for stochastic optimization.
    - **Contribution:** This insight motivates the design of MoFO, which aims to guide the fine-tuning process towards minima that preserve pre-training knowledge.

- **Insight 3:** MoFO, by selectively updating parameters based on momentum, effectively mitigates catastrophic forgetting while maintaining fine-tuning performance.
    - **Supporting Citations:**
        - Tseng (2001) - Convergence of a block coordinate descent method for nondifferentiable minimization.
        - Zhang et al. (2022) - Adam can converge without any modification on update rules.
        - Nutini et al. (2015) - Coordinate descent converges faster with the Gauss-Southwell rule than random selection.
    - **Contribution:** This is the core contribution of the paper, demonstrating the effectiveness of MoFO in addressing the catastrophic forgetting problem.


## 4. Experimental Methodology and Its Foundations

- **Experimental Setup:** The authors use Llama-2-7B and TinyLlama-1.1B as base models. They evaluate MoFO on instruction fine-tuning tasks using MetaMathQA and Code-Alpaca datasets and on continual fine-tuning tasks using the TRACE benchmark. They compare MoFO's performance with various baseline methods, including Full FT, HFT, L1/L2 regularization, GEM, and Replay.
- **Foundations in Cited Works:**
    - The authors draw inspiration from Block Coordinate Descent (BCD) [Tseng, 2001] for the idea of selectively updating parameters.
    - They use Adam optimizer [Kingma & Ba, 2014] as a baseline for comparison.
    - They leverage the concept of continual learning [Rolnick et al., 2019] and its associated metrics (OP and BWT) for evaluating MoFO in continual fine-tuning scenarios.
- **Novel Aspects:**
    - The core novelty lies in the **momentum-filtered update strategy** within MoFO. The authors justify this approach by arguing that momentum provides a better indicator of parameter influence than gradients in Adam-like optimizers.
    - The authors also provide a **theoretical convergence analysis** of a simplified version of MoFO, which is a novel contribution to the understanding of the algorithm's behavior.


## 5. Results in Context

- **Main Results:**
    - MoFO achieves comparable performance to Full FT and HFT on instruction fine-tuning tasks (e.g., GSM8K for MetaMathQA, HumanEval for Code-Alpaca) while significantly mitigating catastrophic forgetting of general capabilities (e.g., MMLU, Commonsense).
    - MoFO outperforms L1/L2 regularization in both task performance and forgetting mitigation.
    - In continual fine-tuning on the TRACE benchmark, MoFO outperforms Full FT and HFT in terms of both OP and BWT scores.
    - MoFO combines well with traditional continual learning methods like GEM and Replay, further improving performance.
- **Comparison with Existing Literature:**
    - The results confirm that catastrophic forgetting is a significant issue in LLM fine-tuning, as reported in previous works [Lin et al., 2023, Chen et al., 2020, etc.].
    - MoFO's performance surpasses that of many existing methods for mitigating forgetting, including L1/L2 regularization, HFT, and GEM, demonstrating its effectiveness.
    - The results extend the findings of previous works on BCD [Tseng, 2001] by showing that a momentum-based parameter selection strategy can be more effective than gradient-based or random selection in the context of LLM fine-tuning.


## 6. Discussion and Related Work

- **Situating the Work:** The authors position MoFO as a novel optimization method that addresses the limitations of existing approaches to catastrophic forgetting in LLMs. They highlight that MoFO is orthogonal to replay-based methods and does not modify the original loss function, making it a more practical and versatile solution.
- **Key Papers Cited:**
    - Rolnick et al. (2019) - Experience replay for continual learning.
    - Kirkpatrick et al. (2017) - Overcoming catastrophic forgetting in neural networks.
    - Hu et al. (2022) - LoRA: Low-rank adaptation of large language models.
    - Tseng (2001) - Convergence of a block coordinate descent method for nondifferentiable minimization.
    - Chaudhry et al. (2018) - Riemannian walk for incremental learning: Understanding forgetting and intransigence.
- **Highlighting Novelty:**
    - The authors emphasize that MoFO's replay-free and regularization-free nature makes it more practical than many existing methods.
    - They contrast MoFO with LoRA, highlighting that MoFO allows for full-rank updates, potentially leading to better fine-tuning performance.
    - They discuss how MoFO's momentum-based parameter selection is a more effective greedy variant of BCD compared to gradient-based or random selection.


## 7. Future Work and Open Questions

- **Areas for Further Research:**
    - Exploring further optimizations and potential applications of MoFO in multimodal LLMs.
    - Investigating the convergence properties of the original MoFO algorithm (with 1st and 2nd-order momentum).
- **Supporting Citations:**
    - Zhu et al. (2024) - Model tailor: Mitigating catastrophic forgetting in multi-modal large language models.
    - Zhang et al. (2022) - Adam can converge without any modification on update rules.


## 8. Critical Analysis of Citation Usage

- **Effectiveness of Citation Usage:** The authors effectively use citations to support their claims and findings. They provide a strong foundation for their work by referencing relevant literature on catastrophic forgetting, continual learning, and optimization methods.
- **Areas for Improvement:**
    - While the paper covers a wide range of related work, it could benefit from a more in-depth discussion of specific methods for model merging, which is a related approach to mitigating forgetting.
    - The authors could have included more citations related to the specific challenges of applying BCD to LLMs, as this is a relatively unexplored area.
- **Potential Biases:** The authors primarily cite works from the deep learning and NLP communities, which is appropriate given the focus of the paper. However, there might be relevant work in other fields (e.g., neuroscience, cognitive science) that could provide additional insights into the phenomenon of catastrophic forgetting.


## 9. Final Summary

- **Contribution:** The paper makes a significant contribution to the field of LLM fine-tuning by introducing MoFO, a novel optimization method that effectively mitigates catastrophic forgetting. MoFO achieves comparable performance to standard fine-tuning methods while preserving pre-training knowledge.
- **Influential Cited Works:**
    - Tseng (2001) - Convergence of a block coordinate descent method for nondifferentiable minimization.
    - Kingma & Ba (2014) - Adam: A method for stochastic optimization.
    - Rolnick et al. (2019) - Experience replay for continual learning.
    - Kirkpatrick et al. (2017) - Overcoming catastrophic forgetting in neural networks.
    - Hu et al. (2022) - LoRA: Low-rank adaptation of large language models.
- **Assessment of Literature Integration:** The paper demonstrates a strong understanding of the existing literature on catastrophic forgetting and LLM fine-tuning. It effectively integrates this literature to support its claims and findings, providing a clear and compelling argument for the novelty and importance of MoFO.


I hope this comprehensive analysis is helpful in understanding the paper and its place within the broader research context. Feel free to ask if you have any further questions. I'm ready to provide more details or insights as needed.