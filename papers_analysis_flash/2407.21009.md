## AI-Assisted Generation of Difficult Math Questions: A Citation-Focused Analysis

This paper, titled "AI-Assisted Generation of Difficult Math Questions" by Vedant Shah et al. (2024), addresses the growing need for diverse and challenging mathematics questions to evaluate the mathematical reasoning capabilities of large language models (LLMs). The paper proposes a novel framework that combines the strengths of LLMs with human-in-the-loop approaches to generate a diverse array of challenging math questions. The paper cites a total of **44 references**.

### 1. Introduction

The paper highlights the limitations of existing publicly available math datasets and the challenges of relying solely on human experts for question generation. The authors argue that LLM-generated questions often lack the requisite diversity and difficulty. The main objective of the research is to develop a framework that combines the strengths of LLMs with human feedback to generate a diverse array of challenging math questions.

### 2. Section-by-Section Analysis with Citation Extraction

**2.1 Introduction**

- **Claim:** Current LLM training positions mathematical reasoning as a core capability.
    - **Citation:** [Chowdhery et al., 2023; Anil et al., 2023; Team, 2023; Team et al., 2023; Abdin et al., 2024; Achiam et al., 2023; Touvron et al., 2023]
    - **Relevance:** This citation highlights the recent advancements in LLMs' ability to understand and generate complex mathematical content, emphasizing the importance of evaluating their mathematical reasoning skills.
- **Claim:** Publicly available sources of high-quality, varied, and difficult mathematical questions are drying up.
    - **Citation:** [Hendrycks et al., 2021]
    - **Relevance:** This citation introduces the MATH dataset, a benchmark for evaluating LLMs' mathematical reasoning abilities, and highlights the need for new, diverse, and challenging questions to overcome the limitations of existing datasets.
- **Claim:** LLM-generated questions often lack the necessary difficulty.
    - **Citation:** [Huang et al., 2024; Chan et al., 2024; Yu et al., 2024]
    - **Relevance:** This citation highlights the limitations of existing LLM-based question generation methods, emphasizing the need for a more robust approach that ensures the diversity and difficulty of generated questions.

**2.2 Evaluation Saturation Phenomenon**

- **Claim:** LLM evaluations are getting saturated due to across-the-board improvements and evaluation-specific enhancements.
    - **Citation:** [Yue et al., 2023; Yu et al., 2023; Li et al., 2024; Zhang et al., 2024]
    - **Relevance:** This section discusses the phenomenon of evaluation saturation in LLM research, highlighting the need for more challenging and diverse evaluation datasets to assess genuine mathematical understanding.

**2.3 Proposed Framework: AI-assisted Generation of Difficult Math Questions**

- **Claim:** LLMs possess a robust understanding of mathematical skills and can identify the skills required to solve given questions.
    - **Citation:** [Arora and Goyal, 2023; Didolkar et al., 2024; Reid et al., 2024; Achiam et al., 2023]
    - **Relevance:** This citation introduces the concept of LLM metacognition, highlighting their ability to extract and identify core mathematical skills, which forms the basis for the proposed framework.
- **Claim:** LLMs can generate creative math questions when provided with a list of skills, but often exhibit shortcomings such as generating questions too similar to existing datasets, containing errors or nonsensical elements, or being too tedious or mechanical.
    - **Citation:** [Trinh et al., 2024; Li et al., 2024; Gunasekar et al., 2023; Patel et al., 2024; Toshniwal et al., 2024; Gupta et al., 2023; Lu et al., 2024; Honovich et al., 2022]
    - **Relevance:** This citation highlights the limitations of existing LLM-based question generation methods and motivates the need for a more robust approach that addresses these shortcomings.

**2.4 Pipeline for AI-Assisted Question Generation**

- **Claim:** The proposed pipeline involves five steps: skill pair validation, question generation, attempted solution, question validation, and final solution.
    - **Citation:** [Didolkar et al., 2024]
    - **Relevance:** This citation introduces the concept of skill extraction, a crucial step in the proposed framework, and provides a foundation for the subsequent steps in the pipeline.

**2.5 Experiments and Findings**

- **Claim:** The authors evaluate the performance of various models on MATH2 and compare it to their performance on MATH.
    - **Citation:** [Hendrycks et al., 2021]
    - **Relevance:** This citation introduces the MATH dataset as the baseline for comparison and highlights the importance of evaluating models on a new, more challenging dataset.
- **Claim:** The authors observe a striking relationship between models' performance on MATH2 and MATH: the success rate on MATH2 is approximately the square of the success rate on MATH.
    - **Relevance:** This finding suggests that successfully solving a question in MATH2 requires a nontrivial combination of two distinct math skills, highlighting the effectiveness of the proposed framework in generating more challenging questions.

**2.6 Observations from the Question Generation Process**

- **Claim:** The authors identify several failure modes of the question generation pipeline, including insufficient involvement of skills, insufficient information, unsolvable or computationally intractable questions, nonsensical questions, deceitful solutions, and finding a needle in the haystack.
    - **Relevance:** This section provides insights into the challenges and limitations of the proposed framework, highlighting the importance of human oversight and validation in ensuring the quality and difficulty of generated questions.

**2.7 Conclusions**

- **Claim:** The authors conclude that the proposed framework effectively leverages the complementary strengths of humans and AI to generate new, challenging mathematics questions.
    - **Relevance:** This section summarizes the key contributions of the paper, highlighting the effectiveness of the proposed framework in generating high-quality, challenging math questions that can be used to evaluate the mathematical reasoning capabilities of LLMs.

### 3. Key Insights and Supporting Literature

- **Key Insight:** The proposed framework effectively combines the strengths of LLMs and human feedback to generate a diverse array of challenging math questions.
    - **Supporting Citations:** [Arora and Goyal, 2023; Didolkar et al., 2024; Reid et al., 2024; Achiam et al., 2023; Hendrycks et al., 2021; Huang et al., 2024; Chan et al., 2024; Yu et al., 2024]
    - **Explanation:** These citations highlight the limitations of existing LLM-based question generation methods and the importance of incorporating human feedback to ensure the quality and difficulty of generated questions.
- **Key Insight:** The new dataset, MATH2, is significantly more challenging than MATH for all models, suggesting that it effectively assesses the ability of models to combine multiple skills.
    - **Supporting Citations:** [Hendrycks et al., 2021; Yue et al., 2023; Wei et al., 2022; Yu et al., 2023; Didolkar et al., 2024]
    - **Explanation:** These citations highlight the importance of evaluating models on diverse and challenging datasets to assess their true mathematical reasoning abilities.
- **Key Insight:** The authors observe a striking relationship between models' performance on MATH2 and MATH: the success rate on MATH2 is approximately the square of the success rate on MATH.
    - **Supporting Citations:** [Hendrycks et al., 2021]
    - **Explanation:** This finding suggests that successfully solving a question in MATH2 requires a nontrivial combination of two distinct math skills, highlighting the effectiveness of the proposed framework in generating more challenging questions.

### 4. Experimental Methodology and Its Foundations

The paper uses a five-step pipeline for generating challenging math questions: skill pair validation, question generation, attempted solution, question validation, and final solution. The authors leverage the MATH dataset [Hendrycks et al., 2021] as the source for skill extraction and in-context exemplars. The authors use GPT-4 and Claude as the primary LLMs for question generation and validation. The authors also employ human annotators to verify and refine the generated questions, ensuring their quality and difficulty.

- **Novel Aspects of Methodology:** The authors introduce the concept of combining two distinct skills in each question, which significantly increases the difficulty and challenges the models' ability to generalize.
- **Citations for Novel Approaches:** The authors do not explicitly cite any works to justify this novel approach, but it builds upon the existing research on LLM metacognition and the need for more challenging evaluation datasets.

### 5. Results in Context

- **Main Result:** The authors demonstrate that MATH2 is significantly more challenging than MATH for all models, highlighting the effectiveness of the proposed framework in generating more difficult questions.
    - **Comparison with Existing Literature:** The authors compare the performance of various models on MATH2 with their performance on MATH [Hendrycks et al., 2021], demonstrating a significant drop in performance across the board.
    - **Confirmation, Contradiction, or Extension:** The results confirm the need for more challenging evaluation datasets and highlight the limitations of existing LLM-based question generation methods.
- **Main Result:** The authors observe a striking relationship between models' performance on MATH2 and MATH: the success rate on MATH2 is approximately the square of the success rate on MATH.
    - **Comparison with Existing Literature:** The authors do not explicitly compare this finding with existing literature, but it suggests that successfully solving a question in MATH2 requires a nontrivial combination of two distinct math skills.
    - **Confirmation, Contradiction, or Extension:** This finding extends the existing research on LLM metacognition and highlights the importance of generating questions that require the combination of multiple skills.

### 6. Discussion and Related Work

The authors discuss the limitations of the proposed framework, including the high cost of human verification and the need for further research to improve the efficiency of the pipeline. They also highlight the potential applications of the framework in other domains beyond mathematics.

- **Key Papers Cited:** [Arora and Goyal, 2023; Didolkar et al., 2024; Reid et al., 2024; Achiam et al., 2023; Hendrycks et al., 2021; Huang et al., 2024; Chan et al., 2024; Yu et al., 2024; Bowman and etal, 2022]
- **Explanation:** The authors use these citations to highlight the novelty and importance of their work, emphasizing the need for more challenging evaluation datasets and the potential of human-AI collaboration in generating high-quality data for evaluating LLMs.

### 7. Future Work and Open Questions

The authors suggest several areas for future research, including:

- **Reducing the cost of human verification:** The authors propose using open weights models and optimizing prompting strategies to reduce the need for extensive human verification.
- **Developing automated validation tools:** The authors suggest developing automated tools to evaluate the quality and difficulty of generated questions, further reducing the reliance on human annotators.
- **Integrating a training-based feedback loop:** The authors propose incorporating a training-based feedback loop to progressively improve the quality of generated questions.
- **Extending the framework to other domains:** The authors suggest exploring the potential applications of the framework in other domains beyond mathematics.

- **Citations for Future Work:** The authors do not explicitly cite any works to support these suggestions for future work, but they build upon the existing research on LLM metacognition, human-AI collaboration, and the need for more challenging evaluation datasets.

### 8. Critical Analysis of Citation Usage

The authors effectively use citations to support their arguments and findings. They provide a comprehensive overview of the existing literature on LLM metacognition, question generation, and evaluation datasets. However, there are a few areas where additional citations might have been beneficial:

- **Novelty of the proposed approach:** The authors do not explicitly cite any works to justify the novelty of their approach, which combines two distinct skills in each question. While this approach builds upon existing research on LLM metacognition and the need for more challenging evaluation datasets, it would be beneficial to explicitly cite relevant works to highlight the novelty of their contribution.
- **Potential biases in citation selection:** The authors primarily cite works from the field of deep learning and large language models, with a limited focus on other relevant fields such as education and cognitive science. This bias might limit the broader context of the work and its potential impact on other disciplines.

### 9. Final Summary

This paper makes a significant contribution to the field of LLM evaluation by proposing a novel framework for generating challenging and diverse math questions. The authors effectively combine the strengths of LLMs with human feedback to create a new dataset, MATH2, which is significantly more challenging than MATH for all models. The paper also highlights the importance of human oversight and validation in ensuring the quality and difficulty of generated questions. The authors' findings suggest that successfully solving a question in MATH2 requires a nontrivial combination of two distinct math skills, highlighting the effectiveness of the proposed framework in generating more challenging questions. The paper effectively integrates existing literature to support its claims and findings, providing a comprehensive overview of the current state of research in LLM metacognition, question generation, and evaluation datasets.

The most influential or frequently cited works used throughout the paper include:

- **MATH dataset:** [Hendrycks et al., 2021]
- **LLM metacognition:** [Arora and Goyal, 2023; Didolkar et al., 2024; Reid et al., 2024; Achiam et al., 2023]
- **Evaluation saturation:** [Yue et al., 2023; Yu et al., 2023; Li et al., 2024; Zhang et al., 2024]

Overall, this paper provides a valuable contribution to the field of LLM evaluation by addressing the growing need for more challenging and diverse math questions. The authors' proposed framework and the new dataset, MATH2, offer a promising approach for evaluating the mathematical reasoning capabilities of LLMs and advancing the field of AI-assisted education.
