## Analysis of "A Spitting Image: Modular Superpixel Tokenization in Vision Transformers"

**1. Introduction**

- **Title:** A Spitting Image: Modular Superpixel Tokenization in Vision Transformers
- **Authors:** Marius Aasan, Odd Kolbjørnsen, Anne Schistad Solberg, and Adín Ramirez Rivera
- **Publication Date:** 15 August 2024 (v2)
- **Objective:** The paper proposes a modular superpixel tokenization strategy for Vision Transformers (ViTs) that decouples tokenization and feature extraction, aiming to improve the faithfulness of attributions, provide pixel-level granularity for dense prediction tasks, and maintain predictive performance in classification tasks.
- **Number of References:** 56

**2. Section-by-Section Analysis with Citation Extraction**

**2.1 Introduction**

- **Key Points:**
    - ViTs traditionally use grid-based tokenization, ignoring semantic content.
    - Patch-based tokenization has limitations in terms of scale and redundancy.
    - Superpixels offer adaptability in scale and shape, aligning better with semantic structures.
- **Significant Citations:**
    - **[14] (Dosovitskiy et al., 2021):** "Vision Transformers [14] (ViTs) have become the cynosure of vision tasks in the wake of convolutional architectures." This citation establishes the context of ViTs as a dominant approach in vision tasks.
    - **[12, 42] (Vaswani et al., 2017; Brown et al., 2020):** "In the original transformer for language [12, 42], tokenization serves as a crucial preprocessing step, with the aim of optimally partitioning data based on a predetermined entropic measure [20, 34]." This citation highlights the importance of tokenization in the original transformer architecture and its connection to entropy-based partitioning.
    - **[7, 25, 38, 39, 40, 41] (Carion et al., 2020; Liu et al., 2021; Touvron et al., 2021):** "As models were adapted to vision, tokenization was simplified to partitioning images into square patches. This approach proved effective [7, 25, 38, 39, 40, 41], and soon became canonical; an integral part of the architecture." This citation acknowledges the widespread adoption of patch-based tokenization in ViTs and its perceived effectiveness.
    - **[37] (Stutz et al., 2018):** "Superpixels offer an opportunity to mitigate the shortcomings of patch-based tokenization by allowing for adaptability in scale and shape while leveraging inherent redundancies in visual data. Superpixels have been shown to align better with semantic structures within images [37], providing a rationale for their potential utility in vision transformer architectures." This citation introduces the concept of superpixels and their potential advantages over patch-based tokenization.

**2.2 Methodology**

- **Key Points:**
    - The paper proposes a modular framework for ViTs that decouples tokenization, feature extraction, and embedding.
    - The authors introduce a novel hierarchical superpixel tokenization method.
    - The method uses a parallel edge contraction approach with regularization for size and compactness.
    - The authors provide a detailed description of the superpixel graph construction and weight function.
    - The paper emphasizes the importance of fair comparison with established baselines without architectural optimizations.
- **Significant Citations:**
    - **[36] (Steiner et al., 2022):** "Hence, we design experiments to establish a fair comparison against well-known baselines without architectural optimizations. This controlled comparison is crucial for attributing observed disparities specifically to the tokenization strategy, and eliminates confounding factors from specialized architectures or training regimes." This citation highlights the importance of controlled experiments for isolating the impact of tokenization on model performance.
    - **[48, 53] (Wei et al., 2018; Yan et al., 2022):** "Hierarchical superpixels [48, 53] are highly parallelizable graph-based approaches suitable for on-line tokenization." This citation acknowledges the existing work on hierarchical superpixels and their suitability for online tokenization.
    - **[37] (Stutz et al., 2018):** "Tokenization in language tasks involves partitioning text into optimally informative tokens, analogous to how superpixels [37] partition spatial data into discrete connected regions." This citation draws a parallel between tokenization in language and superpixel segmentation in vision.

**2.3 Feature Extraction with Irregular Patches**

- **Key Points:**
    - Irregular patches pose challenges for embedding due to their unaligned nature, varying shapes, and non-convexity.
    - The authors propose a minimal set of properties for feature extraction: color, texture, shape, scale, and position.
    - The paper introduces a kernelized approach for positional encoding that handles complex shapes, scales, and positions.
    - The authors describe the use of bilinear interpolation for color features and gradient operators for texture features.
- **Significant Citations:**
    - **[10, 24] (Dalal & Triggs, 2005; Leung & Malik, 2001):** "Gradient operators provides a simple robust method of extracting texture information [10, 24]." This citation acknowledges the use of gradient operators for texture extraction in computer vision.
    - **[33] (Scharr, 2007):** "We use the gradient operator proposed by Scharr [33] due to improved rotational symmetry and discretization errors." This citation highlights the specific gradient operator used in the paper and its advantages.

**2.4 Generalization of Canonical ViT**

- **Key Points:**
    - The proposed framework generalizes the canonical ViT tokenization.
    - The authors prove that the framework is equivalent to applying a canonical patch embedder with a fixed patch size and gradient excluding feature extraction.
- **Significant Citations:**
    - **None:** The authors do not explicitly cite any specific works to support their claim of generalization. However, the proposition and proof presented in this section demonstrate the equivalence of the proposed framework to the canonical ViT, establishing its generality.

**3. Experiments and Results**

**3.1 Classification**

- **Key Points:**
    - The authors evaluate the performance of ViTs with different tokenization strategies (ViT, RVIT, SPIT) on ImageNet, CIFAR100, and CALTECH256.
    - The results show that SPiT with gradient features performs comparably to ViT with square patches.
    - SPiT with gradient excluding features underperforms, suggesting the importance of gradient features for irregular patches.
    - SPiT demonstrates better robustness to label noise and generalizes better in real-world scenarios.
    - SPiT performs better with kNN classification for higher resolution images, indicating its ability to capture finer-grained information.
- **Significant Citations:**
    - **[11] (Deng et al., 2009):** "We train ViTs with different tokenization strategies (ViT, RVIT, SPIT) using base (B) and small (S) capacities on a general purpose classification task on ImageNet [11] (IN1K)." This citation identifies the dataset used for training and evaluation.
    - **[16, 22] (Griffin et al., 2022; Krizhevsky et al., 2009):** "We evaluate the models by fine-tuning on CIFAR100 [22] and CALTECH256 [16], in addition to validation using the INREAL labels [4], ablating the effect of gradient features." This citation identifies the datasets used for fine-tuning and evaluation.
    - **[4, 8, 28] (Beyer et al., 2020; Caron et al., 2021; Oquab et al., 2028):** "We also evaluate our models by replacing the linear classifier head with a k-nearest neighbours (kNN) classifier over the representation space of different models, focusing solely on the clustering quality of the class tokens in the embedded space [8, 28]." This citation highlights the use of kNN classification for evaluating the clustering quality of class tokens.

**3.2 Evaluating Tokenized Representations**

- **Key Points:**
    - The authors evaluate the faithfulness of attributions and the model's performance on zero-shot unsupervised segmentation.
    - SPiT with gradient features demonstrates better comprehensiveness and sufficiency scores for attributions compared to ViT and RViT.
    - SPiT shows promising results in unsupervised segmentation, demonstrating its ability to extract salient regions without a separate decoder.
- **Significant Citations:**
    - **[8, 14, 28] (Caron et al., 2021; Dosovitskiy et al., 2021; Oquab et al., 2028):** "Techniques such as attention rollout [8, 14], attention flow [1], and PCA projections [28] have been leveraged to visualize the reasoning behind the model's decisions." This citation highlights existing techniques for visualizing attention in transformers.
    - **[3, 30] (Adebayo et al., 2018; Ribeiro et al., 2016):** "Unlike gradient-based attributions, which often lack clear causal links to model predictions [3], attention based attributions are intrinsically connected to the flow of information in the model, and provide direct insight into the decision-making process in an interpretable manner." This citation contrasts gradient-based and attention-based attributions and emphasizes the interpretability of attention.
    - **[13] (Chan et al., 2022):** "To quantify the faithfulness of interpretations under different tokenization strategies, we compute the attention flow of the model in addition to PCA projected features and contrast this with attributions from LIME with independently computed SLIC superpixels, and measure faithfulness using comprehensiveness (COMP) and sufficiency (SUFF) [13]." This citation introduces the metrics used for evaluating the faithfulness of attributions.
    - **[23, 51] (Ladický et al., 2009; Yan et al., 2015):** "Superpixels have historically been applied in dense prediction tasks such as segmentation and object detection [23, 51] as a lower-dimensional prior for dense prediction tasks." This citation highlights the historical use of superpixels in dense prediction tasks.
    - **[47] (Wang et al., 2022):** "To evaluate our tokens, we are particularly interested in tasks for which the outputs of the pre-trained model can be leveraged directly, without the addition of a downstream decoder. Wang et al. [47] propose an unsupervised methodology for extracting salient segmentation maps for any transformer model using normalized graph cut [35]." This citation introduces the TokenCut framework for unsupervised segmentation and its reliance on normalized graph cut.

**3.3 Ablations**

- **Key Points:**
    - The authors conduct ablation studies to evaluate the impact of different tokenization strategies on model performance.
    - The results show that ViTs with square tokenization perform poorly when evaluated on irregular patches.
    - RViT models show improved accuracy when evaluated on square patches.
    - SPiT models generalize well to both square and Voronoi tokens, but are highly dependent on gradient features.
- **Significant Citations:**
    - **None:** The authors do not explicitly cite any specific works to support their ablation study findings. However, the results presented in this section provide valuable insights into the impact of different tokenization strategies on model performance and highlight the importance of gradient features for handling irregular patches.

**4. Discussion and Related Work**

- **Key Points:**
    - The authors discuss the growing interest in adaptive tokenization for ViTs.
    - They present a taxonomy of adaptive tokenization based on coupling/decoupling with the transformer architecture and token granularity.
    - The authors highlight the limitations of their proposed framework, including its lack of gradient-based optimization and dependence on a predefined number of tokens.
    - They suggest areas for future work, such as exploring GNNs for tokenization, hierarchical properties in self-supervised frameworks, and dynamic interactions between ViTs and tokenization.
- **Significant Citations:**
    - **[5, 19, 26] (Bolya et al., 2023; Huang et al., 2022; Ma et al., 2023):** "Several approaches [5, 19, 26] are inherently coupled to the architecture, while others adopt a decoupled approach [18, 31] which more closely aligns with our framework." This citation highlights the different approaches to adaptive tokenization based on coupling/decoupling with the transformer architecture.
    - **[18, 31] (Havtorn et al., 2023; Ronen et al., 2023):** "A significant body of current research is primarily designed to improve scaling and overall compute for attention [5, 32, 55] by leveraging token merging strategies in the transformer layers with square patches, and can as such be considered low-granularity coupled approaches." This citation discusses the focus on improving scaling and computational efficiency in existing adaptive tokenization approaches.
    - **[5, 32, 55] (Bolya et al., 2023; Ryoo et al., 2021; Yuan et al., 2021):** "Distinctively, SuperToken [19] applies a coupled approach to extract a non-uniform token representation. The approach is fundamentally patch based, and does not aim for pixel-level granularity." This citation highlights the limitations of SuperToken in terms of pixel-level granularity.
    - **[18, 31] (Havtorn et al., 2023; Ronen et al., 2023):** "In contrast, multi-scale tokenization [18, 31] apply a decoupled approach where the tokenizer is independent of the transformer architecture. These are commensurable with any transformer backbone, and improve computational overhead." This citation highlights the advantages of decoupled multi-scale tokenization in terms of flexibility and computational efficiency.
    - **[26] (Ma et al., 2023):** "On the periphery, Ma et al. [26] propose a pixel-level clustering method with a coupled high granularity approach." This citation mentions a different approach to adaptive tokenization with high granularity.
    - **[6] (Brown et al., 2020):** "To contextualize vision models () with LLMs (■), GPT-3 [6] is included for reference." This citation provides a broader context for adaptive tokenization by referencing GPT-3, a large language model with a different approach to tokenization.

**5. Conclusion**

- **Key Points:**
    - The paper proposes a modular superpixel tokenization framework for ViTs that generalizes the canonical ViT architecture.
    - The authors demonstrate that superpixel tokenization with gradient features performs comparably to ViT with square patches in classification tasks.
    - SPiT shows promising results in unsupervised segmentation and attribution faithfulness.
    - The authors acknowledge the limitations of their framework and suggest areas for future work.
- **Significant Citations:**
    - **None:** The authors do not explicitly cite any specific works to support their conclusions. However, the findings presented throughout the paper, particularly in the experimental results and discussion sections, support their overall conclusion that superpixel tokenization offers a promising approach for improving ViT performance and interpretability.

**6. Experimental Methodology and Its Foundations**

- **Experimental Setup:**
    - The authors train ViTs with different tokenization strategies (ViT, RVIT, SPIT) on ImageNet, CIFAR100, and CALTECH256.
    - They use base (B) and small (S) capacities for the models.
    - The training process involves standard techniques like ADAMW optimizer, cosine annealing learning rate scheduler, weight decay, stochastic depth dropout, and data augmentation.
    - The authors conduct ablation studies to evaluate the impact of different tokenization strategies and gradient features.
    - They also evaluate the faithfulness of attributions and the model's performance on zero-shot unsupervised segmentation.
- **Cited Works for Methodology:**
    - **[36] (Steiner et al., 2022):** The authors follow the recommendations provided by Steiner et al. for training ViTs.
    - **[39] (Touvron et al., 2021):** The authors use the AuG3 framework by Touvron et al. for data augmentation.
    - **[56] (Yun et al., 2019):** The authors use CUTMIX for data augmentation.
    - **[13] (Chan et al., 2022):** The authors use the comprehensiveness and sufficiency metrics by Chan et al. for evaluating the faithfulness of attributions.
    - **[47] (Wang et al., 2022):** The authors use the TokenCut framework by Wang et al. for unsupervised segmentation.
- **Novel Aspects of Methodology:**
    - The paper introduces a novel hierarchical superpixel tokenization method that uses a parallel edge contraction approach with regularization for size and compactness.
    - The authors propose a modular framework for ViTs that decouples tokenization, feature extraction, and embedding.
- **Cited Works for Novel Approaches:**
    - **[48, 53] (Wei et al., 2018; Yan et al., 2022):** The authors cite existing work on hierarchical superpixels for their tokenization method.
    - **[37] (Stutz et al., 2018):** The authors draw inspiration from the concept of superpixels for their tokenization approach.

**7. Results in Context**

- **Main Results:**
    - SPiT with gradient features performs comparably to ViT with square patches in classification tasks.
    - SPiT with gradient excluding features underperforms, suggesting the importance of gradient features for irregular patches.
    - SPiT demonstrates better robustness to label noise and generalizes better in real-world scenarios.
    - SPiT performs better with kNN classification for higher resolution images, indicating its ability to capture finer-grained information.
    - SPiT shows promising results in unsupervised segmentation, demonstrating its ability to extract salient regions without a separate decoder.
    - SPiT demonstrates better comprehensiveness and sufficiency scores for attributions compared to ViT and RViT.
- **Comparison with Existing Literature:**
    - The authors compare their results with existing work on ViTs, particularly focusing on the performance of patch-based tokenization and the use of attention-based techniques for interpretability.
    - They also compare their results with existing work on superpixel segmentation and attribution faithfulness.
- **Confirmation, Contradiction, or Extension of Cited Works:**
    - The authors' results confirm the effectiveness of patch-based tokenization for classification tasks, but also highlight its limitations in terms of scale and redundancy.
    - Their findings extend existing work on attention-based interpretability by demonstrating the potential of superpixel tokenization for improving the faithfulness of attributions.
    - The authors' results confirm the potential of superpixels for unsupervised segmentation, but also demonstrate the effectiveness of their proposed framework for extracting salient regions without a separate decoder.

**8. Discussion and Related Work**

- **Situating Work within Existing Literature:**
    - The authors position their work within the growing field of adaptive tokenization for ViTs.
    - They highlight the limitations of existing approaches, particularly those that are coupled to the transformer architecture or focus on low-granularity tokenization.
    - They emphasize the novelty of their modular framework and its ability to decouple tokenization, feature extraction, and embedding.
- **Key Papers Cited in Discussion/Related Work:**
    - **[5, 19, 26] (Bolya et al., 2023; Huang et al., 2022; Ma et al., 2023):** These papers discuss coupled approaches to adaptive tokenization.
    - **[18, 31] (Havtorn et al., 2023; Ronen et al., 2023):** These papers discuss decoupled approaches to adaptive tokenization.
    - **[5, 32, 55] (Bolya et al., 2023; Ryoo et al., 2021; Yuan et al., 2021):** These papers discuss approaches that focus on improving scaling and computational efficiency.
    - **[19] (Huang et al., 2022):** This paper discusses SuperToken, a coupled approach to adaptive tokenization.
    - **[26] (Ma et al., 2023):** This paper discusses a pixel-level clustering method with a coupled high granularity approach.
    - **[6] (Brown et al., 2020):** This paper provides a broader context for adaptive tokenization by referencing GPT-3.
- **Highlighting Novelty/Importance of Work:**
    - The authors highlight the novelty of their modular framework and its ability to decouple tokenization, feature extraction, and embedding.
    - They emphasize the importance of their work in extending the space of ViTs to a larger class of semantically-rich models.

**9. Future Work and Open Questions**

- **Areas for Further Research:**
    - Exploring graph neural networks (GNNs) for tokenization.
    - Leveraging hierarchical properties in self-supervised frameworks.
    - Studying the dynamic interactions between ViTs and tokenization.
    - Investigating the effects of irregularity in feature extraction.
    - Developing learnable frameworks for adaptive tokenization.
- **Citations for Future Work:**
    - **[8] (Caron et al., 2021):** The authors suggest exploring GNNs for tokenization in self-supervised frameworks.
    - **[45, 46] (Wang et al., 2021; Wang et al., 2022):** The authors suggest leveraging hierarchical properties in pyramid models.
    - **[18, 5] (Havtorn et al., 2023; Bolya et al., 2023):** The authors suggest studying the dynamic interactions between ViTs and tokenization, particularly in the context of gating and merging.

**10. Critical Analysis of Citation Usage**

- **Effectiveness of Citation Usage:**
    - The authors generally use citations effectively to support their arguments and findings.
    - They provide relevant citations to establish the context of their work, introduce key concepts, and acknowledge previous research.
- **Areas for Additional Citations:**
    - The authors could have provided more citations to support their claims about the limitations of patch-based tokenization and the advantages of superpixels.
    - They could have also cited more work on the use of attention-based techniques for interpretability in transformers.
- **Potential Biases in Citation Selection:**
    - The authors primarily cite works from the field of computer vision, with a limited number of citations from other related fields like natural language processing.
    - They also tend to cite more recent works, potentially overlooking relevant older research.

**11. Final Summary**

- **Contribution to the Field:**
    - The paper makes a significant contribution to the field of adaptive tokenization for ViTs by proposing a modular superpixel tokenization framework that decouples tokenization, feature extraction, and embedding.
    - The authors demonstrate the effectiveness of their approach in improving the faithfulness of attributions, providing pixel-level granularity for dense prediction tasks, and maintaining predictive performance in classification tasks.
- **Influential/Frequently Cited Works:**
    - **[14] (Dosovitskiy et al., 2021):** This paper establishes the context of ViTs as a dominant approach in vision tasks.
    - **[12, 42] (Vaswani et al., 2017; Brown et al., 2020):** These papers highlight the importance of tokenization in the original transformer architecture.
    - **[37] (Stutz et al., 2018):** This paper introduces the concept of superpixels and their potential advantages over patch-based tokenization.
    - **[36] (Steiner et al., 2022):** This paper provides recommendations for training ViTs.
    - **[13] (Chan et al., 2022):** This paper introduces the metrics used for evaluating the faithfulness of attributions.
    - **[47] (Wang et al., 2022):** This paper introduces the TokenCut framework for unsupervised segmentation.
- **Assessment of Literature Integration:**
    - The paper effectively integrates existing literature to support its claims and findings.
    - The authors provide a clear and concise overview of the relevant research, highlighting the key concepts and challenges in the field.
    - They also effectively position their work within the broader context of adaptive tokenization for ViTs.

This analysis provides a comprehensive overview of the paper's content, its relationship to existing literature, and its potential impact on the field. By extracting and presenting the citations used by the authors, this analysis helps readers understand the factual basis of the research, its relationship to existing literature, and the broader context of the work.