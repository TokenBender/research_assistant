## Analysis of "Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning"

**1. Introduction:**

- **Title:** Fire-Flyer AI-HPC: A Cost-Effective Software-Hardware Co-Design for Deep Learning
- **Authors:** Wei An, Xiao Bi, Guanting Chen, Shanhuang Chen, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Wenjun Gao, Kang Guan, Jianzhong Guo, Yongqiang Guo, Zhe Fu, Ying He, Panpan Huang, Jiashi Li, Wenfeng Liang, Xiaodong Liu, Xin Liu, Yiyuan Liu, Yuxuan Liu, Shanghao Lu, Xuan Lu, Xiaotao Nie, Tian Pei, Junjie Qiu, Hui Qu, Zehui Ren, Zhangli Sha, Xuecheng Su, Xiaowen Sun, Yixuan Tan, Minghui Tang, Shiyu Wang, Yaohui Wang, Yongji Wang, Ziwei Xie, Yiliang Xiong, Yanhong Xu, Shengfeng Ye, Shuiping Yu, Yukun Zha, Liyue Zhang, Haowei Zhang, Mingchuan Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Yuheng Zou
- **Publication Date:** 31 August 2024 (v2)
- **Objective:** The paper aims to address the challenges of high computational power and bandwidth demands in deep learning and large language models (LLMs) by introducing a cost-effective hardware-software co-design framework called Fire-Flyer AI-HPC.
- **Number of References:** 101

**2. Section-by-Section Analysis with Citation Extraction:**

**a. Introduction:**

- **Key Points:** The paper highlights the rapid growth of deep learning and LLMs, leading to exponentially increasing demands for computational power and bandwidth. This has significantly inflated the cost of high-performance computing (HPC) infrastructure. The authors propose Fire-Flyer AI-HPC as a solution to address these challenges.
- **Significant Citations:**
    - **Claim:** "The rapid progress in Deep Learning (DL) and Large Language Models (LLMs) has exponentially increased demands of computational power and bandwidth."
    - **Citation:** [1] Y. LeCun, Y. Bengio, and G. Hinton, "Deep learning," nature, vol. 521, no. 7553, pp. 436-444, 2015.
    - **Explanation:** This citation introduces the concept of deep learning and its rapid development, setting the context for the paper's focus on the computational demands of this field.
    - **Claim:** "Since then, researchers have gone down the path of making models bigger and never looked back."
    - **Citation:** [3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
    - **Explanation:** This citation refers to the trend of increasing model size in LLMs, highlighting the growing computational demands associated with this approach.

**b. Related Work:**

- **Key Points:** This section provides a historical overview of the evolution of deep learning, highlighting key milestones and the increasing computational demands associated with each advancement. It also discusses the challenges and solutions in model training, including various parallelism strategies and the limitations of traditional HPC systems for deep learning workloads.
- **Significant Citations:**
    - **Claim:** "The revolution in Machine Learning and Deep Learning began in 2012 with AlexNet [21], which outperformed traditional methods in image classification, marking the onset of big data utilization and increased computational demands."
    - **Citation:** [21] A. Krizhevsky, I. Sutskever, and G. E. Hinton, "Imagenet classification with deep convolutional neural networks," Advances in neural information processing systems, vol. 25, 2012.
    - **Explanation:** This citation introduces AlexNet, a landmark model that marked the beginning of the deep learning revolution and highlighted the increasing importance of big data and computational resources.
    - **Claim:** "The shift towards the Mixture-of-Experts (MoE) Models [28]-[30] architecture starting from GPT-4 [7], and the recent AI Generated Content (AIGC) multi-modal (Sora [31]) has amplified the demand for memory and computational resources."
    - **Citation:** [7] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., "Gpt-4 technical report," arXiv preprint arXiv:2303.08774, 2023.
    - **Explanation:** This citation introduces GPT-4, a recent LLM that utilizes the Mixture-of-Experts architecture, further increasing the demand for computational resources and highlighting the need for cost-effective solutions.

**c. Fire-Flyer 2: Our Approach for Deep Learning and Early LLM Training:**

- **Key Points:** This section details the Fire-Flyer 2 AI-HPC architecture, which is composed of 10,000 PCIe A100 GPUs. The authors compare their architecture to the NVIDIA DGX-A100, highlighting its cost-effectiveness and lower CO2 emissions. They also discuss the key technical topics in their architecture, including network co-design and the HFReduce library for efficient allreduce communication.
- **Significant Citations:**
    - **Claim:** "We specifically engineered HFReduce to accelerate allreduce communication and implemented numerous measures to keep our Computation-Storage Integrated Network congestion-free."
    - **Citation:** [10] NVIDIA, "Nvidia collective communications library (nccl): Optimized primitives for collective multi-gpu communication," 2017. [Online]. Available: https://github.com/NVIDIA/nccl
    - **Explanation:** This citation introduces NCCL, a widely used library for allreduce communication, which the authors compare their HFReduce library to.

**d. Network Topology: Two-Layer Fat-Tree with Storage and Computation Integrated:**

- **Key Points:** The authors explain their choice of the Fat-Tree topology for their network architecture, highlighting its advantages in terms of bisection bandwidth. They also discuss the use of a two-zone network configuration to reduce costs and the integration of storage and computation networks.
- **Significant Citations:**
    - **Claim:** "We selected the Fat-Tree [9] topology as our primary network architecture due to its exceptionally high bisection bandwidth, making it the preferred choice for AI-HPC and high-throughput storage environments."
    - **Citation:** [9] C. E. Leiserson, "Fat-trees: Universal networks for hardware-efficient supercomputing,‚Äù IEEE Transactions on Computers, vol. C-34, no. 10, pp. 892-901, Oct 1985.
    - **Explanation:** This citation introduces the Fat-Tree topology and its advantages for high-performance computing, justifying the authors' choice for their network architecture.

**e. Cost Performance of Our Architecture:**

- **Key Points:** The authors compare the cost-performance of their Fire-Flyer 2 AI-HPC architecture to the NVIDIA DGX-A100, demonstrating significant cost savings while achieving 83% of the performance.
- **Significant Citations:**
    - **Claim:** "Compared to the NVIDIA DGX-A100 [8] architecture, our approach using PCIe A100 achieves approximately 83% of the performance in TF32 and FP16 General Matrix Multiply (GEMM) benchmarks."
    - **Citation:** [8] NVIDIA, "Nvidia dgx platform the best of nvidia ai-all in one place." 2022. [Online]. Available: https://www.nvidia.com/en-us/data-center/dgx-platform/
    - **Explanation:** This citation introduces the NVIDIA DGX-A100, a widely used AI-HPC system, which the authors use as a benchmark for comparing the cost-performance of their Fire-Flyer 2 architecture.

**f. HFReduce: Hardware Software Co-Design in Network:**

- **Key Points:** This section introduces HFReduce, a library specifically designed for efficient allreduce communication on PCIe GPUs. The authors highlight the advantages of HFReduce over NCCL, including reduced PCIe bandwidth consumption and no GPU kernel overhead. They also discuss the performance improvements achieved with NVLink and the limitations of the EPYC Rome CPU.
- **Significant Citations:**
    - **Claim:** "In contrast, NCCL's allreduce operation requires GPU kernel execution, which can affect other computational kernels on the GPU."
    - **Citation:** [10] NVIDIA, "Nvidia collective communications library (nccl): Optimized primitives for collective multi-gpu communication," 2017. [Online]. Available: https://github.com/NVIDIA/nccl
    - **Explanation:** This citation again refers to NCCL, highlighting the GPU kernel overhead associated with its allreduce operation, which HFReduce aims to avoid.

**g. HaiScale: Special Optimization for Deep Learning Models Training:**

- **Key Points:** This section introduces HaiScale, a training tool that utilizes HFReduce for efficient communication and overlaps allreduce operations with backpropagation. The authors highlight the advantages of HaiScale over PyTorch's DDP, including improved parallel scalability and performance. They also discuss the use of HaiScale for optimizing LLM training with various parallelism strategies.
- **Significant Citations:**
    - **Claim:** "HaiScale Distributed Data Parallel (DDP) is a training tool that utilizes HFReduce as its communication backend, in contrast to PyTorch's DDP [67] which employs NCCL as its backend."
    - **Citation:** [67] P. Foundation, "Tensors and dynamic neural networks in python with strong gpu acceleration," 2016. [Online]. Available: https://github.com/pytorch/pytorch
    - **Explanation:** This citation introduces PyTorch's DDP, a widely used training tool that utilizes NCCL for communication, which the authors compare HaiScale to.

**h. Ensuring Minimal Congestion in Our Computation-Storage Integrated Network:**

- **Key Points:** This section discusses the importance of isolating different types of traffic and controlling network congestion in the Fire-Flyer 2 AI-HPC architecture. The authors describe their strategies for achieving this, including traffic divergence, topology adjustment, route optimization, and NCCL optimization.
- **Significant Citations:**
    - **Claim:** "By using InfiniBand's Service Level (SL) technology [74] [75], we assign different value of SL when establishing connections between nodes and map SL to IB physical queues Virtual Lanes (VLs) [74] [75]."
    - **Citation:** [74] S.-A. Reinemo, T. Skeie, T. Sodring, O. Lysne, and O. Trudbakken, "An overview of qos capabilities in infiniband, advanced switching interconnect, and ethernet," IEEE Communications Magazine, vol. 44, no. 7, pp. 32-38, 2006.
    - **Explanation:** This citation introduces the concept of Service Level (SL) in InfiniBand, a mechanism used by the authors to isolate different types of traffic and prevent congestion.

**i. High-Throughput Distributed File System: 3FS:**

- **Key Points:** This section introduces 3FS, a high-performance distributed file system developed by the authors. They discuss the key technical points of 3FS, including its hardware architecture, key features, and the use of Chain Replication with Apportioned Queries (CRAQ) for strong consistency.
- **Significant Citations:**
    - **Claim:** "3FS is our in-house developed high performance distributed file system, akin to WekaFS [78], DAOS [79], [80], and BeeGFS [81]."
    - **Citation:** [78] Œñ. Liran, H. David, and Œú. Barbara, "Wekafs architecture white paper," 2021. [Online]. Available: https://www.weka.io/wp-content/uploads/files/2017/12/Architectural_WhitePaper-W02R6WP201812-1.pdf
    - **Explanation:** This citation introduces WekaFS, a widely used distributed file system, which the authors compare their 3FS system to.

**j. HAI Platform: a Time-Sharing Scheduling Platform:**

- **Key Points:** This section describes the HAI Platform, a time-sharing scheduling platform that manages cluster resources and facilitates efficient task execution. The authors highlight the key features of the HAI Platform, including checkpoint management, stability and robustness, and the use of a validator for detecting hardware failures.
- **Significant Citations:**
    - **Claim:** "The cluster deploying HAI Platform does not pool GPU resources, but classifies and marks them based on computing nodes as basic units, according to resource types, network areas, etc."
    - **Citation:** [85] K. Liu, Z. Jiang, J. Zhang, H. Wei, X. Zhong, L. Tan, T. Pan, and T. Huang, "Hostping: Diagnosing intra-host network bottlenecks in RDMA servers," in 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23). Boston, MA: USENIX Association, Apr. 2023, pp. 15-29. [Online]. Available: https://www.usenix.org/conference/nsdi23/presentation/liu-kefei
    - **Explanation:** This citation introduces hostping, a tool used by the authors for diagnosing hardware failures, highlighting the importance of robust monitoring and maintenance in large-scale HPC systems.

**k. Hardware Failures Characterization in Fire-Flyer 2 AI-HPC:**

- **Key Points:** This section discusses the challenges of hardware failures in large-scale HPC systems, including GPU Xid errors, network flash cuts, and the importance of error correction codes (ECC). The authors highlight the need for prompt identification and categorization of hardware failures to improve cluster utilization.
- **Significant Citations:**
    - **Claim:** "In supercomputers and data centers, hardware failures and chip errors can lead to floating-point overflow, non-convergence, or slow convergence during model training [86]."
    - **Citation:** [86] Y. He, M. Hutton, S. Chan, R. De Gruijl, R. Govindaraju, N. Patil, and Y. Li, "Understanding and Mitigating Hardware Failures in Deep Learning Training Systems," in Proceedings of the 50th Annual International Symposium on Computer Architecture. Orlando FL USA: ACM, Jun. 2023, pp. 1-16. [Online]. Available: https://dl.acm.org/doi/10.1145/3579371.3589105
    - **Explanation:** This citation highlights the potential impact of hardware failures on deep learning training, emphasizing the need for robust error detection and recovery mechanisms.

**l. Discussion on Congestion Control in RDMA Networks:**

- **Key Points:** This section discusses the challenges of congestion control in RDMA networks, highlighting the limitations of DCQCN and the advantages of static routing algorithms. The authors explain their choice to disable DCQCN and rely on network tuning methods to prevent congestion.
- **Significant Citations:**
    - **Claim:** "Lossless RDMA networks offer several flow-control mechanisms, such as Priority Flow Control (PFC) [89] for RoCE networks and credit-based flow control [90] for IB networks."
    - **Citation:** [89] "Priority flow control: Build reliable layer 2 infrastructure," 2015. [Online]. Available: https://api.semanticscholar.org/CorpusID:42645413
    - **Explanation:** This citation introduces Priority Flow Control (PFC), a mechanism used for congestion control in RoCE networks, providing context for the authors' discussion of congestion control in RDMA networks.

**m. Discussion about NVLink Technology Choices:**

- **Key Points:** The authors discuss their decision to initially avoid using NVLink due to cost and stability concerns. They explain that the need for NVLink arose with the advent of LLMs and that the decision to install NVLink should be based on specific needs.

**n. Maintaince Cost Overview:**

- **Key Points:** This section provides an overview of the maintenance costs associated with the Fire-Flyer 2 AI-HPC system, including construction costs, power consumption, and operational costs.

**o. Stability Compared with Other Architectures:**

- **Key Points:** The authors compare the stability of their Fire-Flyer 2 AI-HPC architecture to other architectures, highlighting the prevalence of NVLink-related failures in other systems and the lower occurrence of such failures in their own system.
- **Significant Citations:**
    - **Claim:** "A recent paper [96] reportsthat NVLink-related failures account for approximately 52.42% (54 out of 103) of total failures, with raw data indicating 54 NVLink Errors, 21 CUDA Errors, 16 Node Failures, 12 ECC Errors, and 12 Network Errors."
    - **Citation:** [96] Q. Hu, Z. Ye, Z. Wang, G. Wang, M. Zhang, Q. Chen, P. Sun, D. Lin, X. Wang, Y. Luo et al., "Characterization of large language model development in the datacenter," in 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24), 2024, pp. 709-729.
    - **Explanation:** This citation provides data on the prevalence of NVLink-related failures in other systems, highlighting the importance of the authors' approach in minimizing such failures.

**p. Future Arch and Integration with New GPU Models:**

- **Key Points:** The authors discuss their plans for a next-generation PCIe architecture designed for MoE (Mixture of Experts) LLM training. They highlight the importance of a 1:1 GPU to NIC ratio and the use of a multi-plane network to reduce costs.
- **Significant Citations:**
    - **Claim:** "Our next-generation PCIe architecture is designed for MoE (Mixture of Experts) LLM training, where all-to-all performance is crucial. Therefore, the next-gen nodes feature a 1:1 GPU to NIC ratio, comparable to DGX-H100/B100 systems, as illustrated in Figure 12."
    - **Citation:** [97] DeepSeek-AI, X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, H. Gao, K. Gao, W. Gao, R. Ge, K. Guan, D. Guo, J. Guo, G. Hao, Z. Hao, Y. He, W. Hu, P. Huang, E. Li, G. Li, J. Li, Y. Li, Y. K. Li, W. Liang, F. Lin, A. X. Liu, B. Liu, W. Liu, X. Liu, X. Liu, Y. Liu, H. Lu, S. Lu, F. Luo, S. Ma, X. Nie, T. Pei, Y. Piao, J. Qiu, H. Qu, T. Ren, Z. Ren, C. Ruan, Z. Sha, Z. Shao, J. Song, X. Su, J. Sun, Y. Sun, M. Tang, B. Wang, P. Wang, S. Wang, Y. Wang, Y. Wang, T. Wu, Y. Wu, X. Xie, Z. Xie, Z. Xie, Y. Xiong, H. Xu, R. X. Xu, Y. Xu, D. Yang, Y. You, S. Yu, X. Yu, B. Zhang, H. Zhang, L. Zhang, L. Zhang, M. Zhang, M. Zhang, W. Zhang, Y. Zhang, C. Zhao, Y. Zhao, S. Zhou, S. Zhou, Q. Zhu, and Y. Zou, "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism," Jan. 2024, arXiv:2401.02954 [cs]. [Online]. Available: http://arxiv.org/abs/2401.02954
    - **Explanation:** This citation introduces DeepSeek LLM, a large language model developed by the authors, highlighting the need for a next-generation architecture optimized for MoE training.

**3. Key Insights and Supporting Literature:**

- **Insight:** Fire-Flyer 2 AI-HPC architecture achieves significant cost savings compared to NVIDIA DGX-A100 while maintaining 80% of the performance.
    - **Supporting Citations:** [8] NVIDIA, "Nvidia dgx platform the best of nvidia ai-all in one place." 2022. [Online]. Available: https://www.nvidia.com/en-us/data-center/dgx-platform/
    - **Explanation:** This citation provides a benchmark for comparing the cost-performance of Fire-Flyer 2 to a widely used AI-HPC system.
- **Insight:** HFReduce library offers significant advantages over NCCL for allreduce communication on PCIe GPUs, including reduced PCIe bandwidth consumption and no GPU kernel overhead.
    - **Supporting Citations:** [10] NVIDIA, "Nvidia collective communications library (nccl): Optimized primitives for collective multi-gpu communication," 2017. [Online]. Available: https://github.com/NVIDIA/nccl
    - **Explanation:** This citation introduces NCCL, a widely used library for allreduce communication, which the authors compare HFReduce to, highlighting the advantages of HFReduce.
- **Insight:** HaiScale training tool effectively overlaps allreduce operations with backpropagation, improving parallel scalability and performance compared to PyTorch's DDP.
    - **Supporting Citations:** [67] P. Foundation, "Tensors and dynamic neural networks in python with strong gpu acceleration," 2016. [Online]. Available: https://github.com/pytorch/pytorch
    - **Explanation:** This citation introduces PyTorch's DDP, a widely used training tool that utilizes NCCL for communication, which the authors compare HaiScale to, highlighting the advantages of HaiScale.

**4. Experimental Methodology and Its Foundations:**

- **Experimental Setup:** The paper describes the deployment of a cluster composed of 10,000 PCIe A100 GPUs for deep learning training. The authors compare their architecture to the NVIDIA DGX-A100 in terms of cost-effectiveness, performance, and energy consumption.
- **Methodology Foundations:** The authors do not explicitly cite any specific works as the basis for their methodology. However, they draw upon common practices in deep learning training, such as the use of various parallelism strategies and the importance of efficient allreduce communication.
- **Novel Aspects:** The authors highlight the novel aspects of their architecture, including the two-zone network configuration, the integration of storage and computation networks, and the development of HFReduce and HaiScale libraries. They do not explicitly cite any works to justify these novel approaches.

**5. Results in Context:**

- **Main Results:**
    - Fire-Flyer 2 AI-HPC architecture achieves 83% of the performance of NVIDIA DGX-A100 while reducing costs by half and energy consumption by 40%.
    - HFReduce library outperforms NCCL in terms of inter-node bandwidth, achieving 6.3-8.1GB/s compared to 1.6-4.8GB/s for NCCL.
    - HaiScale training tool demonstrates excellent parallel scalability, achieving 91% efficiency for LLaMa-13B training and 92.92% efficiency for DeepSeekMoE-16B training.
- **Comparison with Existing Literature:** The authors compare their results to the NVIDIA DGX-A100, a widely used AI-HPC system, highlighting the cost-effectiveness and performance advantages of their Fire-Flyer 2 architecture. They also compare HFReduce to NCCL, demonstrating its superior performance.
- **Confirmation, Contradiction, or Extension:** The authors' results demonstrate the effectiveness of their cost-effective hardware-software co-design approach, confirming the potential for achieving high performance at lower costs. Their findings also extend existing research on allreduce communication by highlighting the advantages of HFReduce over NCCL.

**6. Discussion and Related Work:**

- **Situating Work within Literature:** The authors discuss the limitations of traditional HPC systems for deep learning workloads and highlight the need for specialized architectures and software solutions. They also discuss the challenges of congestion control in RDMA networks and the importance of robust hardware and software fault tolerance.
- **Key Papers Cited:**
    - [8] NVIDIA, "Nvidia dgx platform the best of nvidia ai-all in one place." 2022. [Online]. Available: https://www.nvidia.com/en-us/data-center/dgx-platform/
    - [10] NVIDIA, "Nvidia collective communications library (nccl): Optimized primitives for collective multi-gpu communication," 2017. [Online]. Available: https://github.com/NVIDIA/nccl
    - [67] P. Foundation, "Tensors and dynamic neural networks in python with strong gpu acceleration," 2016. [Online]. Available: https://github.com/pytorch/pytorch
    - [85] K. Liu, Z. Jiang, J. Zhang, H. Wei, X. Zhong, L. Tan, T. Pan, and T. Huang, "Hostping: Diagnosing intra-host network bottlenecks in RDMA servers," in 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23). Boston, MA: USENIX Association, Apr. 2023, pp. 15-29. [Online]. Available: https://www.usenix.org/conference/nsdi23/presentation/liu-kefei
    - [86] Y. He, M. Hutton, S. Chan, R. De Gruijl, R. Govindaraju, N. Patil, and Y. Li, "Understanding and Mitigating Hardware Failures in Deep Learning Training Systems," in Proceedings of the 50th Annual International Symposium on Computer Architecture. Orlando FL USA: ACM, Jun. 2023, pp. 1-16. [Online]. Available: https://dl.acm.org/doi/10.1145/3579371.3589105
    - [89] "Priority flow control: Build reliable layer 2 infrastructure," 2015. [Online]. Available: https://api.semanticscholar.org/CorpusID:42645413
    - [96] Q. Hu, Z. Ye, Z. Wang, G. Wang, M. Zhang, Q. Chen, P. Sun, D. Lin, X. Wang, Y. Luo et al., "Characterization of large language model development in the datacenter," in 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24), 2024, pp. 709-729.
    - [97] DeepSeek-AI, X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, H. Gao, K. Gao, W. Gao, R. Ge, K. Guan, D. Guo, J. Guo, G. Hao, Z. Hao, Y. He, W. Hu, P. Huang, E. Li, G. Li, J. Li, Y. Li, Y. K. Li, W. Liang, F. Lin, A. X. Liu, B. Liu, W. Liu, X. Liu, X. Liu, Y. Liu, H. Lu, S. Lu, F. Luo, S. Ma, X. Nie, T. Pei, Y. Piao, J. Qiu, H. Qu, T. Ren, Z. Ren, C. Ruan, Z. Sha, Z. Shao, J. Song, X. Su, J. Sun, Y. Sun, M. Tang, B. Wang, P. Wang, S. Wang, Y. Wang, Y. Wang, T. Wu, Y. Wu, X. Xie, Z. Xie, Z. Xie, Y. Xiong, H. Xu, R. X. Xu, Y. Xu, D. Yang, Y. You, S. Yu, X. Yu, B. Zhang, H. Zhang, L. Zhang, L. Zhang, M. Zhang, M. Zhang, W. Zhang, Y. Zhang, C. Zhao, Y. Zhao, S. Zhou, S. Zhou, Q. Zhu, and Y. Zou, "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism," Jan. 2024, arXiv:2401.02954 [cs]. [Online]. Available: http://arxiv.org/abs/2401.02954
- **Highlighting Novelty:** The authors use these citations to highlight the novelty of their Fire-Flyer 2 AI-HPC architecture, HFReduce library, and HaiScale training tool, emphasizing their contributions to the field of cost-effective deep learning training.

**7. Future Work and Open Questions:**

- **Areas for Further Research:**
    - The authors suggest exploring the use of RoCE switches instead of IB switches to further reduce costs.
    - They also propose a next-generation PCIe architecture designed for MoE (Mixture of Experts) LLM training, highlighting the need for a 1:1 GPU to NIC ratio and the use of a multi-plane network.
- **Citations:**
    - [97] DeepSeek-AI, X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, H. Gao, K. Gao, W. Gao, R. Ge, K. Guan, D. Guo, J. Guo, G. Hao, Z. Hao, Y. He, W. Hu, P. Huang, E. Li, G. Li, J. Li, Y. Li, Y. K. Li, W. Liang, F. Lin, A. X. Liu, B. Liu, W. Liu, X. Liu, X. Liu, Y. Liu, H. Lu, S. Lu, F. Luo, S. Ma, X. Nie, T. Pei, Y. Piao, J. Qiu, H. Qu, T. Ren, Z. Ren, C. Ruan, Z. Sha, Z. Shao, J. Song, X. Su, J. Sun, Y. Sun, M. Tang, B. Wang, P. Wang, S. Wang, Y. Wang, Y. Wang, T. Wu, Y. Wu, X. Xie, Z. Xie, Z. Xie, Y. Xiong, H. Xu, R. X. Xu, Y. Xu, D. Yang, Y. You, S. Yu, X. Yu, B. Zhang, H. Zhang, L. Zhang, L. Zhang, M. Zhang, M. Zhang, W. Zhang, Y. Zhang, C. Zhao, Y. Zhao, S. Zhou, S. Zhou, Q. Zhu, and Y. Zou, "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism," Jan. 2024, arXiv:2401.02954 [cs]. [Online]. Available: http://arxiv.org/abs/2401.02954

**8. Critical Analysis of Citation Usage:**

- **Effectiveness:** The authors generally use citations effectively to support their arguments and findings. They cite relevant works to introduce key concepts, provide historical context, and compare their results to existing research.
- **Areas for Improvement:** While the authors cite a wide range of works, they could have provided more specific citations to justify their novel approaches, such as the two-zone network configuration and the development of HFReduce and HaiScale libraries.
- **Potential Biases:** The authors primarily cite works from the deep learning and HPC communities, potentially overlooking relevant research from other fields, such as networking and distributed systems.

**9. Final Summary:**

- **Contribution:** The paper makes a significant contribution to the field of cost-effective deep learning training by introducing the Fire-Flyer 2 AI-HPC architecture, a hardware-software co-design framework that achieves high performance at lower costs. The authors also introduce novel software solutions, including HFReduce and HaiScale, which address the challenges of efficient allreduce communication and parallel scalability in PCIe-based systems.
- **Influential Works:** The paper frequently cites works related to deep learning, LLMs, HPC, and RDMA networks, highlighting the importance of these fields in the development of cost-effective AI infrastructure.
- **Integration of Literature:** The authors effectively integrate existing literature to support their claims and findings, providing a comprehensive overview of the challenges and solutions in deep learning training and highlighting the novelty of their approach.

Overall, the paper provides a valuable contribution to the field of cost-effective deep learning training, offering insights into the design and implementation of a large-scale AI-HPC system. The authors effectively integrate existing literature to support their arguments and findings, demonstrating a strong understanding of the challenges and opportunities in this rapidly evolving field. However, the paper could benefit from more specific citations to justify their novel approaches and a broader exploration of relevant research from other fields.
