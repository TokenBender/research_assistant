## Analysis of "OMOE: Open Mixture-of-Experts Language Models"

**1. Introduction:**

- **Title:** OMOE: Open Mixture-of-Experts Language Models
- **Authors:** Niklas Muennighoff, Weijia Shi, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi
- **Publication Date:** September 3, 2024
- **Objective:** The paper introduces OLMOE, a fully open Mixture-of-Experts (MoE) language model that aims to achieve state-of-the-art performance while being cost-efficient and accessible for research and development.
- **Number of References:** 222

**2. Section-by-Section Analysis with Citation Extraction:**

**2.1 Introduction:**

- **Key Points:**
    - Highlights the trade-off between performance and cost in large language models (LLMs).
    - Introduces Mixture-of-Experts (MoEs) as a way to improve cost-efficiency by activating only a subset of experts for each input.
    - Mentions the lack of open-source MoE models and the need for more research on their design and training.
    - Introduces OLMOE as a fully open MoE model with state-of-the-art performance.
    - Outlines the key contributions of OLMOE, including its pretraining on 5.1 trillion tokens and its outperformance of other models with similar active parameters.
- **Significant Citations:**
    - **Claim:** "Industry frontier models use MoEs including Gemini-1.5 [173] and reportedly GPT-4 [29]."
    - **Citation:** [173]  "Gemini: A Family of Highly Capable Multimodal Models" by Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, et al. 2023.
    - **Explanation:** This citation supports the claim that MoEs are used in industry-leading models, highlighting the importance of MoEs for achieving high performance.
    - **Claim:** "Most MoE models, however, are closed-source: while some have publicly released model weights [43, 78, 156, 176, 178], they offer limited to no information about their training data, code, or recipes (see Figure 1)."
    - **Citation:** [43] "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism" by DeepSeek-AI, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. 2024.
    - **Explanation:** This citation provides examples of MoE models that have released weights but lack information about their training data, code, and recipes, highlighting the need for more open-source MoE models.
    - **Claim:** "Our comprehensive set of controlled experiments highlights key design choices for MoEs (see Table 1) and LMs in general."
    - **Citation:** [42] "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism" by DeepSeek-AI, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. 2024.
    - **Explanation:** This citation highlights the importance of controlled experiments for understanding the design choices involved in MoE models.

**2.2 Pretraining and Adaptation:**

- **Key Points:**
    - Describes the architecture of OLMOE, which uses a decoder-only transformer with an MoE module replacing the feedforward network.
    - Explains the MoE module's operation, including the selection of experts based on routing probabilities and the aggregation of expert outputs.
    - Discusses the key design choices for MoE models, including the number of active and total parameters, expert granularity, expert sharing, routing algorithm, sparse upcycling, load balancing loss, and router z-loss.
    - Outlines the pretraining data used for OLMOE-1B-7B, which includes a mix of data from DCLM and Dolma 1.7.
    - Describes the adaptation process for creating OLMOE-1B-7B-INSTRUCT, which involves instruction tuning and preference tuning.
- **Significant Citations:**
    - **Claim:** "Pretraining architecture OLMOE is a decoder-only LM consisting of NL transformer [183] layers. The feedforward network (FFN) in dense models like OLMo [64], is replaced with an MoE module consisting of Ne smaller FFN modules called experts, of which a subset of k experts are activated for each processed input token x (also see Figure 2)."
    - **Citation:** [183] "Attention Is All You Need" by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.
    - **Explanation:** This citation introduces the transformer architecture, which is the basis for OLMOE's architecture.
    - **Citation:** [64] "OLMO: Accelerating the Science of Language Models" by Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024.
    - **Explanation:** This citation introduces OLMo, a dense language model that serves as a baseline for comparison with OLMOE.
    - **Claim:** "Key decisions in designing an MoE model include determining the number of activated and total parameters, the design of the experts (e.g., granularity, whether or not to include shared experts), and the choice of the routing algorithm. Moreover, training an MoE model can involve initializing from a dense model (sparse upcycling) and changing the training objective, such as including auxiliary load balancing and router z-losses."
    - **Citation:** [152] "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer" by Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017.
    - **Explanation:** This citation provides a comprehensive overview of the key design choices involved in MoE models, highlighting the importance of these choices for achieving optimal performance.
    - **Claim:** "We use a mix of data from DCLM [89] and Dolma 1.7 [161], which includes the following: (1) a quality-filtered subset of Common Crawl, referred to as DCLM-Baseline, (2) StarCoder, Algebraic Stack and arXiv, used in both DCLM and Dolma 1.7, and (3) peS2o and Wikipedia from Dolma 1.7. We refer to our pretraining dataset as OLMOE-MIX."
    - **Citation:** [89] "DataComp-LM: In search of the next generation of training sets for language models" by Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. 2024.
    - **Explanation:** This citation introduces DCLM, a dataset that is used as a basis for the pretraining data in OLMOE-1B-7B.
    - **Citation:** [161] "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research" by Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 2024.
    - **Explanation:** This citation introduces Dolma 1.7, another dataset that is used in the pretraining data for OLMOE-1B-7B.
    - **Claim:** "We create OLMOE-1B-7B-INSTRUCT by following a standard adaptation recipe split into instruction tuning [117, 189, 147, 154, 205] followed by preference tuning [31, 15, 136, 54] building on prior open models [182, 75, 186]."
    - **Citation:** [117] "Cross-Task Generalization via Natural Language Crowdsourcing Instructions" by Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.
    - **Explanation:** This citation introduces instruction tuning, a common technique for adapting language models to specific tasks.
    - **Citation:** [189] "Finetuned Language Models Are Zero-Shot Learners" by Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022.
    - **Explanation:** This citation highlights the importance of instruction tuning for enabling zero-shot learning in language models.
    - **Citation:** [31] "Deep reinforcement learning from human preferences" by Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2023.
    - **Explanation:** This citation introduces preference tuning, another technique for adapting language models to specific tasks.
    - **Citation:** [186] "How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources" by Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023.
    - **Explanation:** This citation highlights the importance of preference tuning for improving the performance of language models on specific tasks.

**2.3 Results:**

- **Key Points:**
    - Presents the results of evaluating OLMOE-1B-7B during pretraining, after pretraining, and after adaptation.
    - Shows that OLMOE-1B-7B outperforms other open-source models with similar active parameters and achieves competitive performance with larger dense models.
    - Highlights the effectiveness of OLMOE-1B-7B-INSTRUCT on various downstream tasks, including MMLU, GSM8k, HumanEval, AlpacaEval, and XSTest.
- **Significant Citations:**
    - **Claim:** "In Figure 3 we benchmark the performance of OLMOE-1B-7B during pretraining with the current best OLMo models [64] on commonly used downstream tasks. We find that across all tasks OLMOE-1B-7B reaches better performance with less compute (FLOPs) than the dense OLMo models."
    - **Citation:** [64] "OLMO: Accelerating the Science of Language Models" by Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024.
    - **Explanation:** This citation provides a baseline for comparison with OLMOE-1B-7B, highlighting the model's performance advantage in terms of compute efficiency.
    - **Claim:** "We find that OLMOE-1B-7B performs best among models that use less than 2B active parameters, making it the most economical option for many use cases of LMs."
    - **Citation:** [181] "Llama 2: Open Foundation and Fine-Tuned Chat Models" by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023.
    - **Explanation:** This citation provides a context for understanding the cost-effectiveness of OLMOE-1B-7B, highlighting its performance advantage compared to other models with similar active parameters.
    - **Claim:** "We find that despite requiring ~6–7× less compute per forward pass, OLMOE-1B-7B outperforms some dense LMs with 7B parameters such as Llama2-7B [181], but falls short of others like Llama3.1-8B [50]."
    - **Citation:** [181] "Llama 2: Open Foundation and Fine-Tuned Chat Models" by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023.
    - **Explanation:** This citation provides a context for understanding the performance of OLMOE-1B-7B compared to other models with similar total parameters, highlighting its strengths and limitations.
    - **Claim:** "In Table 5, we benchmark our instruction (SFT) and preference (DPO) tuning of OLMOE-1B-7B. SFT improves our model on all tasks measured. We observe a >10× gain on GSM8k, likely due to our inclusion of additional math data to account for the relatively small amounts of math data during pretraining (§2). DPO helps on most tasks, especially AlpacaEval which aligns with findings from prior work [186, 75, 121]. Our DPO model, which we refer to as OLMOE-1B-7B-INSTRUCT, has the highest average among all models benchmarked. We find it to outperform the chat version of Qwen1.5-3B-14B despite Qwen having >2× more parameters and its pretrained model outperforming OLMOE-1B-7B in Table 4. The 84% score on AlpacaEval also outperforms much larger dense models on the leaderboard, such as Llama2-13B-Chat [181]."
    - **Citation:** [181] "Llama 2: Open Foundation and Fine-Tuned Chat Models" by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023.
    - **Explanation:** This citation provides a context for understanding the performance of OLMOE-1B-7B-INSTRUCT compared to other models, highlighting its strengths and limitations.

**2.4 Experimenting with Alternative Design Choices:**

- **Key Points:**
    - Presents a series of experiments that explore different design choices for MoE models and LMs in general.
    - Discusses the impact of various MoE-specific pretraining settings, including mixture-of-experts vs. dense, expert granularity, shared experts, expert choice vs. token choice, sparse upcycling, load balancing loss, and router z-loss.
    - Explores the impact of general pretraining settings, including dataset experiments, initialization, RMSNorm, decaying embedding parameters, and QK-Norm.
    - Examines the impact of adaptation settings, including the use of auxiliary losses, annealing checkpoint, and preference algorithm.
- **Significant Citations:**
    - **Claim:** "Prior work reports various speed-ups of MoEs over dense models: Artetxe et al. [10] report that MoEs require 2-4× less compute to match dense models, MoMa [99] exhibits 2.6× FLOP savings for language tasks, Arctic [159] yields 4× FLOP savings but for very different dense and MoE configurations, and Switch Transformers [56] train 2-7× faster with MoEs but for encoder-decoder models while the other works study decoder-only LMs [135]."
    - **Citation:** [10] "Efficient Large Scale Language Modeling with Mixtures of Experts" by Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Ves Stoyanov. 2022.
    - **Explanation:** This citation provides evidence from prior work that supports the claim that MoEs can achieve significant speed-ups over dense models.
    - **Citation:** [99] "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts" by Xi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Gosh, Luke Zettlemoyer, and Armen Aghajanyan. 2024.
    - **Explanation:** This citation provides further evidence from prior work that supports the claim that MoEs can achieve significant speed-ups over dense models.
    - **Citation:** [159] "StarCoder: may the source be with you!" by Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023.
    - **Explanation:** This citation provides further evidence from prior work that supports the claim that MoEs can achieve significant speed-ups over dense models.
    - **Citation:** [56] "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity" by William Fedus, Barret Zoph, and Noam Shazeer. 2022.
    - **Explanation:** This citation provides further evidence from prior work that supports the claim that MoEs can achieve significant speed-ups over dense models.
    - **Citation:** [135] "Language models are unsupervised multitask learners" by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.
    - **Explanation:** This citation provides a context for understanding the use of decoder-only models in MoE research.
    - **Claim:** "Dai et al. [39] propose to use small fine-grained experts to allow more combinations of experts and thus make the model more flexible. For example, the Mixtral model [78] uses the common configuration of 8 experts per layer, 2 of which are activated. This allows for (8) = 28 combinations per layer. By halving the size of each expert and therefore doubling the number of experts to maintain the same compute and parameter budget, we can increase the possible combinations to (16) = 1,820. Krajewski et al. [85] investigate compute-optimal granularity configurations finding that higher compute budgets warrant more granular experts."
    - **Citation:** [39] "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models" by Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. 2024.
    - **Explanation:** This citation introduces the concept of expert granularity and its impact on model flexibility and performance.
    - **Citation:** [78] "Mixtral of Experts" by Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024.
    - **Explanation:** This citation provides an example of a model that uses fine-grained experts to achieve high flexibility.
    - **Citation:** [85] "Scaling Laws for Fine-Grained Mixture of Experts" by Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pióro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Król, Tomasz Odrzygóźdź, Piotr Sankowski, Marek Cygan, and Sebastian Jaszczur. 2024.
    - **Explanation:** This citation provides further evidence that supports the claim that higher compute budgets warrant more granular experts.
    - **Claim:** "Dai et al. [39] propose training with a shared/fixed expert that is always used in addition to the routed experts. The intuition is to encourage the shared expert to learn common information and allow the other routed experts to learn more specialized knowledge. This should reduce redundancy among experts and thus lead to a better model as it can store more total information."
    - **Citation:** [39] "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models" by Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. 2024.
    - **Explanation:** This citation introduces the concept of shared experts and its potential benefits for improving model performance.
    - **Claim:** "Komatsuzaki et al. [84] propose turning a dense model into a Mixture-of-Experts model via sparse upcycling: (1) The dense MLP is cloned for each desired expert to constitute MoE layers. (2) A newly initialized router is added in front of each MoE layer. (3) Pretraining continues with the new model so that the cloned MLPs can gradually specialize in different things and the router can be learned. They find that the upcycling approach maintains a performance advantage over a language model trained from scratch for up to 120% of the compute budget of the original dense checkpoint that the sparse model was upcycled from. For example, if sparsely upcycling a 1.3B parameter model at 2 trillion tokens then only at 2.4 trillion tokens should an MoE trained from scratch catch up with the upcycled model. That is, the sparsely upcycled model would have been trained for another 400 billion tokens, thereby saving the equivalent of up to 2T tokens of compute. Other works such as MiniCPM [73], Qwen2 [200] and reportedly Mixtral [25, 78] have adopted sparse upcycling but only share limited information about their configuration."
    - **Citation:** [84] "Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints" by Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. 2023.
    - **Explanation:** This citation introduces the concept of sparse upcycling and its potential benefits for training MoE models.
    - **Claim:** "Shazeer et al. [152] propose the load balancing loss to penalize the model if it is unbalanced, i.e., if it routes all tokens to only a few experts. This is based on the observation that without such penalty, models tend to update only a select few experts in each layer [52, 17]. To compute the load balancing loss (LLB) we multiply the fraction of tokens fi routed to one expert E₁ with the total routing probability Pi allocated to E¿ for one batch and sum it across the number of experts NE: LLB = NE∑fi Pi"
    - **Citation:** [152] "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer" by Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017.
    - **Explanation:** This citation introduces the load balancing loss, a technique for addressing the issue of unbalanced expert activation in MoE models.
    - **Claim:** "Zoph et al. [220] propose the router z-loss to improve both the stability and quality of MoE models. This auxiliary loss penalizes large logits coming into the gating network. Such large logits can lead to numeric overflows in the large matrix multiplications happening in the MoE layer. It is computed by exponentiating the logits x; right before the router layer summed across the number of experts NE and averaged across the batch B, thereby making larger logits lead to a larger loss: LRZ(x) = (log exp())"
    - **Citation:** [220] "ST-MoE: Designing Stable and Transferable Sparse Expert Models" by Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. 2022.
    - **Explanation:** This citation introduces the router z-loss, a technique for improving the stability and quality of MoE models.
    - **Claim:** "Li et al. [89] release the DCLM-Baseline dataset and establish that it leads to better language models than Dolma 1.7 and other datasets as measured on common benchmarks like MMLU [69]. This motivates us to mix their DCLM dataset with some components from Dolma 1.7 that we deem to be high-quality; see §2. In Figure 12, we compare our mix, OLMOE-MIX, with Dolma 1.7 in a controlled setup. We find that OLMOE-MIX leads to clear gains on all three downstream metrics, especially MMLU. DCLM-Baseline has been created through a series of dataset ablations targeting MMLU and other downstream metrics, which explains these results. We also compare adding Reddit and FLAN to our mix as detailed in Appendix F, but do not find consistent performance gains. We do not have a strong intuition for why adding these datasets does not help and a more automatic approach to dataset mixing may be desirable for future iterations [100, 4]. We pretrain using our mix of DCLM-Baseline and Dolma 1.7 dubbed OLMOE-MIX."
    - **Citation:** [89] "DataComp-LM: In search of the next generation of training sets for language models" by Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash